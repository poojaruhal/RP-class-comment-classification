filename,comment_amount,code_amount,ratio,classComment,methodComment,methodBody,constructorComment,constructorBody,fieldComment,fieldBody,classCommentLineLength,classCommentSymbolLength
SessionManager.java,48,272,0.17647058823529413,"  ========================================================================|  Copyright (c) 1995-2018 Mort Bay Consulting Pty. Ltd.|  ------------------------------------------------------------------------|  All rights reserved. This program and the accompanying materials|  are made available under the terms of the Eclipse Public License v1.0|  and Apache License v2.0 which accompanies this distribution.||      The Eclipse Public License is available at|      http://www.eclipse.org/legal/epl-v10.html||      The Apache License v2.0 is available at|      http://www.opensource.org/licenses/apache2.0.php||  You may elect to redistribute this code under either of these licenses.|  ========================================================================||
 * Adapted from https://github.com/eclipse/jetty.project/blob/jetty-9.3.25.v20180904/
 *   jetty-server/src/main/java/org/eclipse/jetty/server/SessionManager.java
 ","/**
 * Returns the <code>HttpSession</code> with the given session id
 *
 * @param id the session id
 * @return the <code>HttpSession</code> with the corresponding id
 *         or null if no session with the given id exists
 */
 
/**
 * Creates a new <code>HttpSession</code>.
 *
 * @param request the HttpServletRequest containing the requested session id
 * @return the new <code>HttpSession</code>
 */
 
/**
 * @return true if session cookies should be HTTP-only (Microsoft extension)
 * @see HttpCookie#isHttpOnly()
 */
 
/**
 * @return the max period of inactivity, after which the session is invalidated, in seconds.
 * @see #setMaxInactiveInterval(int)
 */
 
/**
 * Sets the max period of inactivity, after which the session is invalidated, in seconds.
 *
 * @param seconds the max inactivity period, in seconds.
 * @see #getMaxInactiveInterval()
 */
 
/**
 * Sets the {@link SessionHandler}.
 *
 * @param handler the <code>SessionHandler</code> object
 */
 
/**
 * Adds an event listener for session-related events.
 *
 * @param listener the session event listener to add
 *                 Individual SessionManagers implementations may accept arbitrary listener types,
 *                 but they are expected to at least handle HttpSessionActivationListener,
 *                 HttpSessionAttributeListener,
 *                 HttpSessionBindingListener and HttpSessionListener.
 * @see #removeEventListener(EventListener)
 */
 
/**
 * Removes an event listener for for session-related events.
 *
 * @param listener the session event listener to remove
 * @see #addEventListener(EventListener)
 */
 
/**
 * Removes all event listeners for session-related events.
 *
 * @see #removeEventListener(EventListener)
 */
 
/**
 * Gets a Cookie for a session.
 *
 * @param session         the session to which the cookie should refer.
 * @param contextPath     the context to which the cookie should be linked.
 *                        The client will only send the cookie value when
 *                        requesting resources under this path.
 * @param requestIsSecure whether the client is accessing the server over
 *                        a secure protocol (i.e. HTTPS).
 * @return if this <code>SessionManager</code> uses cookies, then this method will return a new
 *         {@link Cookie cookie object} that should be set on the client
 *         in order to link future HTTP requests
 *         with the <code>session</code>. If cookies are not in use,
 *         this method returns <code>null</code>.
 */
 
/**
 * @return the cross context session id manager.
 * @see #setSessionIdManager(SessionIdManager)
 */
 
/**
 * @return the cross context session id manager.
 * @deprecated use {@link #getSessionIdManager()}
 */
 
/**
 * Sets the cross context session id manager
 *
 * @param idManager the cross context session id manager.
 * @see #getSessionIdManager()
 */
 
/**
 * @param session the session to test for validity
 * @return whether the given session is valid, that is, it has not been invalidated.
 */
 
/**
 * @param session the session object
 * @return the unique id of the session within the cluster, extended with an optional node id.
 * @see #getClusterId(HttpSession)
 */
 
/**
 * @param session the session object
 * @return the unique id of the session within the cluster (without a node id extension)
 * @see #getNodeId(HttpSession)
 */
 
/**
 * Called by the {@link SessionHandler} when a session is first accessed by a request.
 *
 * @param session the session object
 * @param secure  whether the request is secure or not
 * @return the session cookie. If not null,
 *         this cookie should be set on the response to either migrate
 *         the session or to refresh a session cookie that may expire.
 * @see #complete(HttpSession)
 */
 
/**
 * Called by the {@link SessionHandler} when a session is last accessed by a request.
 *
 * @param session the session object
 * @see #access(HttpSession, boolean)
 */
 
/**
 * Sets the session id URL path parameter name.
 *
 * @param parameterName the URL path parameter name
 *                      for session id URL rewriting (null or ""none"" for no rewriting).
 * @see #getSessionIdPathParameterName()
 * @see #getSessionIdPathParameterNamePrefix()
 */
 
/**
 * @return the URL path parameter name for session id URL rewriting, by default ""jsessionid"".
 * @see #setSessionIdPathParameterName(String)
 */
 
/**
 * @return a formatted version of {@link #getSessionIdPathParameterName()}, by default
 *         "";"" + sessionIdParameterName + ""="", for easier lookup in URL strings.
 * @see #getSessionIdPathParameterName()
 */
 
/**
 * @return whether the session management is handled via cookies.
 */
 
/**
 * @return whether the session management is handled via URLs.
 */
 
/**
 * @return True if absolute URLs are check for remoteness before being session encoded.
 */
 
/**
 * @param remote True if absolute URLs are check for remoteness before being session encoded.
 */
 
/**
 * Change the existing session id.
 *
 * @param oldClusterId the old cluster id
 * @param oldNodeId the old node id
 * @param newClusterId the new cluster id
 * @param newNodeId the new node id
 */
 
","getHttpSession 
newHttpSession 
getHttpOnly 
getMaxInactiveInterval 
setMaxInactiveInterval 
setSessionHandler 
addEventListener 
removeEventListener 
clearEventListeners 
getSessionCookie 
getSessionIdManager 
getMetaManager 
setSessionIdManager 
isValid 
getNodeId 
getClusterId 
access 
complete 
setSessionIdPathParameterName 
getSessionIdPathParameterName 
getSessionIdPathParameterNamePrefix 
isUsingCookies 
isUsingURLs 
isCheckingRemoteSessionIdEncoding 
setCheckingRemoteSessionIdEncoding 
renewSessionId 
",,,"/**
 * Session cookie name.
 * Defaults to <code>JSESSIONID</code>, but can be set with the
 * <code>org.eclipse.jetty.servlet.SessionCookie</code> context init parameter.
 */
 
/**
 * Session id path parameter name.
 * Defaults to <code>jsessionid</code>, but can be set with the
 * <code>org.eclipse.jetty.servlet.SessionIdPathParameterName</code> context init parameter.
 * If set to null or ""none"" no URL rewriting will be done.
 */
 
/**
 * Session Domain.
 * If this property is set as a ServletContext InitParam, then it is
 * used as the domain for session cookies. If it is not set, then
 * no domain is specified for the session cookie.
 */
 
/**
 * Session Path.
 * If this property is set as a ServletContext InitParam, then it is
 * used as the path for the session cookie.  If it is not set, then
 * the context path is used as the path for the cookie.
 */
 
/**
 * Session Max Age.
 * If this property is set as a ServletContext InitParam, then it is
 * used as the max age for the session cookie.  If it is not set, then
 * a max age of -1 is used.
 */
 
","Field __SessionCookieProperty
Field __SessionIdPathParameterNameProperty
Field __SessionDomainProperty
Field __SessionPathProperty
Field __MaxAgeProperty
",3,919
SessionHandler.java,38,42,0.9047619047619048,"  ========================================================================|  Copyright (c) 1995-2018 Mort Bay Consulting Pty. Ltd.|  ------------------------------------------------------------------------|  All rights reserved. This program and the accompanying materials|  are made available under the terms of the Eclipse Public License v1.0|  and Apache License v2.0 which accompanies this distribution.||      The Eclipse Public License is available at|      http://www.eclipse.org/legal/epl-v10.html||      The Apache License v2.0 is available at|      http://www.opensource.org/licenses/apache2.0.php||  You may elect to redistribute this code under either of these licenses.|  ========================================================================||
 * Adapted from https://github.com/eclipse/jetty.project/blob/jetty-9.3.25.v20180904/
 *   jetty-server/src/main/java/org/eclipse/jetty/server/session/SessionHandler.java
 ","/**
 * @return Returns the sessionManager.
 */
 
/**
 * @param sessionManager
 *            The sessionManager to set.
 */
 
","{
    return _sessionManager;
} 
{
    if (isStarted()) {
        throw new IllegalStateException();
    }
    if (sessionManager != null) {
        updateBean(_sessionManager, sessionManager);
        _sessionManager = sessionManager;
    }
} 
","/**
 * @param manager
 *            The session manager
 */
 
","{
    setSessionManager(manager);
} 
",,,3,927
ServerSocketUtil.java,53,70,0.7571428571428571,"
 * Copied from
 * hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/ServerSocketUtil.java
 * for Hadoop-3.x testing
 ","/**
 * Port scan & allocate is how most other apps find ports
 *
 * @param port given port
 * @param retries number of retries
 * @return
 * @throws IOException
 */
 
/**
 * Check whether port is available or not.
 *
 * @param port given port
 * @return
 */
 
/**
 * Wait till the port available.
 *
 * @param port given port
 * @param retries number of retries for given port
 * @return
 * @throws InterruptedException
 * @throws IOException
 */
 
/**
 * Find the specified number of unique ports available.
 * The ports are all closed afterwards,
 * so other network services started may grab those same ports.
 *
 * @param numPorts number of required port nubmers
 * @return array of available port numbers
 * @throws IOException
 */
 
","{
    int tryPort = port;
    int tries = 0;
    while (true) {
        if (tries > 0 || tryPort == 0) {
            tryPort = port + rand.nextInt(65535 - port);
        }
        if (tryPort == 0) {
            continue;
        }
        try (ServerSocket s = new ServerSocket(tryPort)) {
            LOG.info(""Using port "" + tryPort);
            return tryPort;
        } catch (IOException e) {
            tries++;
            if (tries >= retries) {
                LOG.info(""Port is already in use; giving up"");
                throw e;
            } else {
                LOG.info(""Port is already in use; trying again"");
            }
        }
    }
} 
{
    try (ServerSocket s = new ServerSocket(port)) {
        return true;
    } catch (IOException e) {
        return false;
    }
} 
{
    int tries = 0;
    while (true) {
        if (isPortAvailable(port)) {
            return port;
        } else {
            tries++;
            if (tries >= retries) {
                throw new IOException(""Port is already in use; giving up after "" + tries + "" times."");
            }
            Thread.sleep(1000);
        }
    }
} 
{
    ServerSocket[] sockets = new ServerSocket[numPorts];
    int[] ports = new int[numPorts];
    for (int i = 0; i < numPorts; i++) {
        ServerSocket sock = new ServerSocket(0);
        sockets[i] = sock;
        ports[i] = sock.getLocalPort();
    }
    for (ServerSocket sock : sockets) {
        sock.close();
    }
    return ports;
} 
",,,,,3,133
MesosExternalBlockStoreClient.java,29,77,0.37662337662337664,"
 * A client for talking to the external shuffle service in Mesos coarse-grained mode.
 *
 * This is used by the Spark driver to register with each external shuffle service on the cluster.
 * The reason why the driver has to talk to the service is for cleaning up shuffle files reliably
 * after the application exits. Mesos does not provide a great alternative to do this, so Spark
 * has to detect this itself.
 ",,,"/**
 * Creates an Mesos external shuffle client that wraps the {@link ExternalBlockStoreClient}.
 * Please refer to docs on {@link ExternalBlockStoreClient} for more information.
 */
 
","{
    super(conf, secretKeyHolder, authEnabled, registrationTimeoutMs);
} 
",,,6,401
SparkSubmitCommandBuilderSuite.java,23,353,0.06515580736543909,,,,,,,,1,0
InProcessLauncherSuite.java,25,113,0.22123893805309736,,,,,,,,1,0
BaseSuite.java,27,55,0.4909090909090909,"
 * Handles configuring the JUL -> SLF4J bridge, and provides some utility methods for tests.
 ","/**
 * Call a closure that performs a check every ""period"" until it succeeds, or the timeout
 * elapses.
 */
 
","{
    assertTrue(""Timeout needs to be larger than period."", timeout.compareTo(period) > 0);
    long deadline = System.nanoTime() + timeout.toNanos();
    int count = 0;
    while (true) {
        try {
            count++;
            check.run();
            return;
        } catch (Throwable t) {
            if (System.nanoTime() >= deadline) {
                String msg = String.format(""Failed check after %d tries: %s."", count, t.getMessage());
                throw new IllegalStateException(msg, t);
            }
            Thread.sleep(period.toMillis());
        }
    }
} 
",,,,,1,92
ChildProcAppHandleSuite.java,20,193,0.10362694300518134,"
   * A log4j appender used by child apps of this test. It records all messages logged through it in
   * memory so the test can check them.
   ",,,,,,,2,139
SparkSubmitOptionParserSuite.java,16,75,0.21333333333333335,,,,,,,,1,0
CommandBuilderUtilsSuite.java,18,84,0.21428571428571427,,,,,,,,1,0
LauncherServerSuite.java,33,208,0.15865384615384615,,"/**
 * Try a few times to get a client-side error, since the client-side socket may not reflect the
 * server-side close immediately.
 */
 
","{
    final AtomicBoolean helloSent = new AtomicBoolean();
    eventually(Duration.ofSeconds(1), Duration.ofMillis(10), () -> {
        try {
            if (!helloSent.get()) {
                client.send(new Hello(secret, ""1.4.0""));
                helloSent.set(true);
            } else {
                client.send(new SetAppId(""appId""));
            }
            fail(""Expected error but message went through."");
        } catch (IllegalStateException | IOException e) {
        // Expected.
        }
    });
} 
",,,,,1,0
SparkLauncher.java,152,260,0.5846153846153846,"
 * Launcher for Spark applications.
 * <p>
 * Use this class to start Spark applications programmatically. The class uses a builder pattern
 * to allow clients to configure the Spark application and launch it as a child process.
 * </p>
 ","/**
 * Set a configuration value for the launcher library. These config values do not affect the
 * launched application, but rather the behavior of the launcher library itself when managing
 * applications.
 *
 * @since 1.6.0
 * @param name Config name.
 * @param value Config value.
 */
 
/**
 * Set a custom JAVA_HOME for launching the Spark application.
 *
 * @param javaHome Path to the JAVA_HOME to use.
 * @return This launcher.
 */
 
/**
 * Set a custom Spark installation location for the application.
 *
 * @param sparkHome Path to the Spark installation to use.
 * @return This launcher.
 */
 
/**
 * Sets the working directory of spark-submit.
 *
 * @param dir The directory to set as spark-submit's working directory.
 * @return This launcher.
 */
 
/**
 * Specifies that stderr in spark-submit should be redirected to stdout.
 *
 * @return This launcher.
 */
 
/**
 * Redirects error output to the specified Redirect.
 *
 * @param to The method of redirection.
 * @return This launcher.
 */
 
/**
 * Redirects standard output to the specified Redirect.
 *
 * @param to The method of redirection.
 * @return This launcher.
 */
 
/**
 * Redirects error output to the specified File.
 *
 * @param errFile The file to which stderr is written.
 * @return This launcher.
 */
 
/**
 * Redirects error output to the specified File.
 *
 * @param outFile The file to which stdout is written.
 * @return This launcher.
 */
 
/**
 * Sets all output to be logged and redirected to a logger with the specified name.
 *
 * @param loggerName The name of the logger to log stdout and stderr.
 * @return This launcher.
 */
 
/**
 * Launches a sub-process that will start the configured Spark application.
 * <p>
 * The {@link #startApplication(SparkAppHandle.Listener...)} method is preferred when launching
 * Spark, since it provides better control of the child application.
 *
 * @return A process handle for the Spark app.
 */
 
/**
 * Starts a Spark application.
 *
 * <p>
 * Applications launched by this launcher run as child processes. The child's stdout and stderr
 * are merged and written to a logger (see <code>java.util.logging</code>) only if redirection
 * has not otherwise been configured on this <code>SparkLauncher</code>. The logger's name can be
 * defined by setting {@link #CHILD_PROCESS_LOGGER_NAME} in the app's configuration. If that
 * option is not set, the code will try to derive a name from the application's name or main
 * class / script file. If those cannot be determined, an internal, unique name will be used.
 * In all cases, the logger name will start with ""org.apache.spark.launcher.app"", to fit more
 * easily into the configuration of commonly-used logging systems.
 *
 * @since 1.6.0
 * @see AbstractLauncher#startApplication(SparkAppHandle.Listener...)
 * @param listeners Listeners to add to the handle before the app is launched.
 * @return A handle for the launched application.
 */
 
","{
    launcherConfig.put(name, value);
} 
{
    checkNotNull(javaHome, ""javaHome"");
    builder.javaHome = javaHome;
    return this;
} 
{
    checkNotNull(sparkHome, ""sparkHome"");
    builder.childEnv.put(ENV_SPARK_HOME, sparkHome);
    return this;
} 
{
    workingDir = dir;
    return this;
} 
{
    redirectErrorStream = true;
    return this;
} 
{
    errorStream = to;
    return this;
} 
{
    outputStream = to;
    return this;
} 
{
    errorStream = ProcessBuilder.Redirect.to(errFile);
    return this;
} 
{
    outputStream = ProcessBuilder.Redirect.to(outFile);
    return this;
} 
{
    setConf(CHILD_PROCESS_LOGGER_NAME, loggerName);
    return this;
} 
{
    ProcessBuilder pb = createBuilder();
    boolean outputToLog = outputStream == null;
    boolean errorToLog = !redirectErrorStream && errorStream == null;
    String loggerName = getLoggerName();
    if (loggerName != null && outputToLog && errorToLog) {
        pb.redirectErrorStream(true);
    }
    Process childProc = pb.start();
    if (loggerName != null) {
        InputStream logStream = outputToLog ? childProc.getInputStream() : childProc.getErrorStream();
        new OutputRedirector(logStream, loggerName, REDIRECTOR_FACTORY);
    }
    return childProc;
} 
{
    LauncherServer server = LauncherServer.getOrCreateServer();
    ChildProcAppHandle handle = new ChildProcAppHandle(server);
    for (SparkAppHandle.Listener l : listeners) {
        handle.addListener(l);
    }
    String secret = server.registerHandle(handle);
    String loggerName = getLoggerName();
    ProcessBuilder pb = createBuilder();
    if (LOG.isLoggable(Level.FINE)) {
        LOG.fine(String.format(""Launching Spark application:%n%s"", join("" "", pb.command())));
    }
    boolean outputToLog = outputStream == null;
    boolean errorToLog = !redirectErrorStream && errorStream == null;
    // Only setup stderr + stdout to logger redirection if user has not otherwise configured output
    // redirection.
    if (loggerName == null && (outputToLog || errorToLog)) {
        String appName;
        if (builder.appName != null) {
            appName = builder.appName;
        } else if (builder.mainClass != null) {
            int dot = builder.mainClass.lastIndexOf(""."");
            if (dot >= 0 && dot < builder.mainClass.length() - 1) {
                appName = builder.mainClass.substring(dot + 1, builder.mainClass.length());
            } else {
                appName = builder.mainClass;
            }
        } else if (builder.appResource != null) {
            appName = new File(builder.appResource).getName();
        } else {
            appName = String.valueOf(COUNTER.incrementAndGet());
        }
        String loggerPrefix = getClass().getPackage().getName();
        loggerName = String.format(""%s.app.%s"", loggerPrefix, appName);
    }
    if (outputToLog && errorToLog) {
        pb.redirectErrorStream(true);
    }
    pb.environment().put(LauncherProtocol.ENV_LAUNCHER_PORT, String.valueOf(server.getPort()));
    pb.environment().put(LauncherProtocol.ENV_LAUNCHER_SECRET, secret);
    try {
        Process child = pb.start();
        InputStream logStream = null;
        if (loggerName != null) {
            logStream = outputToLog ? child.getInputStream() : child.getErrorStream();
        }
        handle.setChildProc(child, loggerName, logStream);
    } catch (IOException ioe) {
        handle.kill();
        throw ioe;
    }
    return handle;
} 
","/**
 * Creates a launcher that will set the given environment variables in the child.
 *
 * @param env Environment variables to set.
 */
 
","{
    if (env != null) {
        this.builder.childEnv.putAll(env);
    }
} 
","/**
 * The Spark master.
 */
 
/**
 * The Spark deploy mode.
 */
 
/**
 * Configuration key for the driver memory.
 */
 
/**
 * Configuration key for the driver class path.
 */
 
/**
 * Configuration key for the default driver VM options.
 */
 
/**
 * Configuration key for the driver VM options.
 */
 
/**
 * Configuration key for the driver native library path.
 */
 
/**
 * Configuration key for the executor memory.
 */
 
/**
 * Configuration key for the executor class path.
 */
 
/**
 * Configuration key for the default executor VM options.
 */
 
/**
 * Configuration key for the executor VM options.
 */
 
/**
 * Configuration key for the executor native library path.
 */
 
/**
 * Configuration key for the number of executor CPU cores.
 */
 
/**
 * Logger name to use when launching a child process.
 */
 
/**
 * A special value for the resource that tells Spark to not try to process the app resource as a
 * file. This is useful when the class being executed is added to the application using other
 * means - for example, by adding jars using the package download feature.
 */
 
/**
 * Maximum time (in ms) to wait for a child process to connect back to the launcher server
 * when using @link{#start()}.
 */
 
/**
 * Used internally to create unique logger names.
 */
 
/**
 * Factory for creating OutputRedirector threads. *
 */
 
","Field SPARK_MASTER
Field DEPLOY_MODE
Field DRIVER_MEMORY
Field DRIVER_EXTRA_CLASSPATH
Field DRIVER_DEFAULT_JAVA_OPTIONS
Field DRIVER_EXTRA_JAVA_OPTIONS
Field DRIVER_EXTRA_LIBRARY_PATH
Field EXECUTOR_MEMORY
Field EXECUTOR_EXTRA_CLASSPATH
Field EXECUTOR_DEFAULT_JAVA_OPTIONS
Field EXECUTOR_EXTRA_JAVA_OPTIONS
Field EXECUTOR_EXTRA_LIBRARY_PATH
Field EXECUTOR_CORES
Field CHILD_PROCESS_LOGGER_NAME
Field NO_RESOURCE
Field CHILD_CONNECTION_TIMEOUT
Field COUNTER
Field REDIRECTOR_FACTORY
",5,228
SparkClassCommandBuilder.java,24,81,0.2962962962962963,"
 * Command builder for internal Spark classes.
 * <p>
 * This class handles building the command to launch all internal Spark classes except for
 * SparkSubmit (which is handled by {@link SparkSubmitCommandBuilder} class.
 ",,,,,,,4,215
InProcessLauncher.java,43,60,0.7166666666666667,"
 * In-process launcher for Spark applications.
 * <p>
 * Use this class to start Spark applications programmatically. Applications launched using this
 * class will run in the same process as the caller.
 * <p>
 * Because Spark only supports a single active instance of <code>SparkContext</code> per JVM, code
 * that uses this class should be careful about which applications are launched. It's recommended
 * that this launcher only be used to launch applications in cluster mode.
 * <p>
 * Also note that, when running applications in client mode, JVM-related configurations (like
 * driver memory or configs which modify the driver's class path) do not take effect. Logging
 * configuration is also inherited from the parent application.
 *
 * @since Spark 2.3.0
 ","/**
 * Starts a Spark application.
 *
 * @see AbstractLauncher#startApplication(SparkAppHandle.Listener...)
 * @param listeners Listeners to add to the handle before the app is launched.
 * @return A handle for the launched application.
 */
 
","{
    if (builder.isClientMode(builder.getEffectiveConfig())) {
        LOG.warning(""It's not recommended to run client-mode applications using InProcessLauncher."");
    }
    Method main = findSparkSubmit();
    LauncherServer server = LauncherServer.getOrCreateServer();
    InProcessAppHandle handle = new InProcessAppHandle(server);
    for (SparkAppHandle.Listener l : listeners) {
        handle.addListener(l);
    }
    String secret = server.registerHandle(handle);
    setConf(LauncherProtocol.CONF_LAUNCHER_PORT, String.valueOf(server.getPort()));
    setConf(LauncherProtocol.CONF_LAUNCHER_SECRET, secret);
    List<String> sparkArgs = builder.buildSparkSubmitArgs();
    String[] argv = sparkArgs.toArray(new String[sparkArgs.size()]);
    String appName = CommandBuilderUtils.firstNonEmpty(builder.appName, builder.mainClass, ""<unknown>"");
    handle.start(appName, main, argv);
    return handle;
} 
",,,,,14,740
AbstractCommandBuilder.java,72,216,0.3333333333333333,"
 * Abstract Spark command builder that defines common functionality.
 ","/**
 * Builds the command to execute.
 *
 * @param env A map containing environment variables for the child process. It may already contain
 *            entries defined by the user (such as SPARK_HOME, or those defined by the
 *            SparkLauncher constructor that takes an environment), and may be modified to
 *            include other variables needed by the process to be executed.
 */
 
/**
 * Builds a list of arguments to run java.
 *
 * This method finds the java executable to use and appends JVM-specific options for running a
 * class with Spark in the classpath. It also loads options from the ""java-opts"" file in the
 * configuration directory being used.
 *
 * Callers should still add at least the class to run, as well as any arguments to pass to the
 * class.
 */
 
/**
 * Builds the classpath for the application. Returns a list with one classpath entry per element;
 * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more
 * specifically, with trailing slashes for directories).
 */
 
/**
 * Adds entries to the classpath.
 *
 * @param cp List to which the new entries are appended.
 * @param entries New classpath entries (separated by File.pathSeparator).
 */
 
/**
 * Loads the configuration file for the application, if it exists. This is either the
 * user-specified properties file, or the spark-defaults.conf file under the Spark configuration
 * directory.
 */
 
","buildCommand 
{
    List<String> cmd = new ArrayList<>();
    String[] candidateJavaHomes = new String[] { javaHome, childEnv.get(""JAVA_HOME""), System.getenv(""JAVA_HOME""), System.getProperty(""java.home"") };
    for (String javaHome : candidateJavaHomes) {
        if (javaHome != null) {
            cmd.add(join(File.separator, javaHome, ""bin"", ""java""));
            break;
        }
    }
    // Load extra JAVA_OPTS from conf/java-opts, if it exists.
    File javaOpts = new File(join(File.separator, getConfDir(), ""java-opts""));
    if (javaOpts.isFile()) {
        try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(javaOpts), StandardCharsets.UTF_8))) {
            String line;
            while ((line = br.readLine()) != null) {
                addOptionString(cmd, line);
            }
        }
    }
    cmd.add(""-cp"");
    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));
    return cmd;
} 
{
    String sparkHome = getSparkHome();
    Set<String> cp = new LinkedHashSet<>();
    addToClassPath(cp, appClassPath);
    addToClassPath(cp, getConfDir());
    boolean prependClasses = !isEmpty(getenv(""SPARK_PREPEND_CLASSES""));
    boolean isTesting = ""1"".equals(getenv(""SPARK_TESTING""));
    if (prependClasses || isTesting) {
        String scala = getScalaVersion();
        List<String> projects = Arrays.asList(""common/kvstore"", ""common/network-common"", ""common/network-shuffle"", ""common/network-yarn"", ""common/sketch"", ""common/tags"", ""common/unsafe"", ""core"", ""examples"", ""graphx"", ""launcher"", ""mllib"", ""repl"", ""resource-managers/mesos"", ""resource-managers/yarn"", ""sql/catalyst"", ""sql/core"", ""sql/hive"", ""sql/hive-thriftserver"", ""streaming"");
        if (prependClasses) {
            if (!isTesting) {
                System.err.println(""NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of "" + ""assembly."");
            }
            for (String project : projects) {
                addToClassPath(cp, String.format(""%s/%s/target/scala-%s/classes"", sparkHome, project, scala));
            }
        }
        if (isTesting) {
            for (String project : projects) {
                addToClassPath(cp, String.format(""%s/%s/target/scala-%s/test-classes"", sparkHome, project, scala));
            }
        }
        // Add this path to include jars that are shaded in the final deliverable created during
        // the maven build. These jars are copied to this directory during the build.
        addToClassPath(cp, String.format(""%s/core/target/jars/*"", sparkHome));
        addToClassPath(cp, String.format(""%s/mllib/target/jars/*"", sparkHome));
    }
    // Add Spark jars to the classpath. For the testing case, we rely on the test code to set and
    // propagate the test classpath appropriately. For normal invocation, look for the jars
    // directory under SPARK_HOME.
    boolean isTestingSql = ""1"".equals(getenv(""SPARK_SQL_TESTING""));
    String jarsDir = findJarsDir(getSparkHome(), getScalaVersion(), !isTesting && !isTestingSql);
    if (jarsDir != null) {
        addToClassPath(cp, join(File.separator, jarsDir, ""*""));
    }
    addToClassPath(cp, getenv(""HADOOP_CONF_DIR""));
    addToClassPath(cp, getenv(""YARN_CONF_DIR""));
    addToClassPath(cp, getenv(""SPARK_DIST_CLASSPATH""));
    return new ArrayList<>(cp);
} 
{
    if (isEmpty(entries)) {
        return;
    }
    String[] split = entries.split(Pattern.quote(File.pathSeparator));
    for (String entry : split) {
        if (!isEmpty(entry)) {
            if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {
                entry += File.separator;
            }
            cp.add(entry);
        }
    }
} 
{
    Properties props = new Properties();
    File propsFile;
    if (propertiesFile != null) {
        propsFile = new File(propertiesFile);
        checkArgument(propsFile.isFile(), ""Invalid properties file '%s'."", propertiesFile);
    } else {
        propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);
    }
    if (propsFile.isFile()) {
        try (InputStreamReader isr = new InputStreamReader(new FileInputStream(propsFile), StandardCharsets.UTF_8)) {
            props.load(isr);
            for (Map.Entry<Object, Object> e : props.entrySet()) {
                e.setValue(e.getValue().toString().trim());
            }
        }
    }
    return props;
} 
",,,,,1,68
LauncherProtocol.java,37,32,1.15625,"
 * Message definitions for the launcher communication protocol. These messages must remain
 * backwards-compatible, so that the launcher can talk to older versions of Spark that support
 * the protocol.
 |
   * Hello message, sent from client to server.
   |
   * SetAppId message, sent from client to server.
   |
   * SetState message, sent from client to server.
   |
   * Stop message, send from server to client to stop the application.
   ",,,,,"/**
 * Environment variable where the server port is stored.
 */
 
/**
 * Environment variable where the secret for connecting back to the server is stored.
 */
 
/**
 * Spark conf key used to propagate the server port for in-process launches.
 */
 
/**
 * Spark conf key used to propagate the app secret for in-process launches.
 */
 
","Field ENV_LAUNCHER_PORT
Field ENV_LAUNCHER_SECRET
Field CONF_LAUNCHER_PORT
Field CONF_LAUNCHER_SECRET
",11,427
NamedThreadFactory.java,16,17,0.9411764705882353,,,,,,,,1,0
LauncherServer.java,84,262,0.32061068702290074,"
 * A server that listens locally for connections from client launched by the library. Each client
 * has a secret that it needs to send to the server to identify itself and establish the session.
 *
 * I/O is currently blocking (one thread per client). Clients have a limited time to connect back
 * to the server, otherwise the server will ignore the connection.
 *
 * === Architecture Overview ===
 *
 * The launcher server is used when Spark apps are launched as separate processes than the calling
 * app. It looks more or less like the following:
 *
 *         -----------------------                       -----------------------
 *         |      User App       |     spark-submit      |      Spark App      |
 *         |                     |  -------------------> |                     |
 *         |         ------------|                       |-------------        |
 *         |         |           |        hello          |            |        |
 *         |         | L. Server |<----------------------| L. Backend |        |
 *         |         |           |                       |            |        |
 *         |         -------------                       -----------------------
 *         |               |     |                              ^
 *         |               v     |                              |
 *         |        -------------|                              |
 *         |        |            |      <per-app channel>       |
 *         |        | App Handle |<------------------------------
 *         |        |            |
 *         -----------------------
 *
 * The server is started on demand and remains active while there are active or outstanding clients,
 * to avoid opening too many ports when multiple clients are launched. Each client is given a unique
 * secret, and have a limited amount of time to connect back
 * ({@link SparkLauncher#CHILD_CONNECTION_TIMEOUT}), at which point the server will throw away
 * that client's state. A client is only allowed to connect back to the server once.
 *
 * The launcher server listens on the localhost only, so it doesn't need access controls (aside from
 * the per-app secret) nor encryption. It thus requires that the launched app has a local process
 * that communicates with the server. In cluster mode, this means that the client that launches the
 * application must remain alive for the duration of the application (or until the app handle is
 * disconnected).
 ","/**
 * Registers a handle with the server, and returns the secret the child app needs to connect
 * back.
 */
 
/**
 * Removes the client handle from the pending list (in case it's still there), and unrefs
 * the server.
 */
 
/**
 * Wait for the remote side to close the connection so that any pending data is processed.
 * This ensures any changes reported by the child application take effect.
 *
 * This method allows a short period for the above to happen (same amount of time as the
 * connection timeout, which is configurable). This should be fine for well-behaved
 * applications, where they close the connection arond the same time the app handle detects the
 * app has finished.
 *
 * In case the connection is not closed within the grace period, this method forcefully closes
 * it and any subsequent data that may arrive will be ignored.
 */
 
","{
    String secret = createSecret();
    secretToPendingApps.put(secret, handle);
    return secret;
} 
{
    for (Map.Entry<String, AbstractAppHandle> e : secretToPendingApps.entrySet()) {
        if (e.getValue().equals(handle)) {
            String secret = e.getKey();
            secretToPendingApps.remove(secret);
            break;
        }
    }
    unref();
} 
{
    Thread connThread = this.connectionThread;
    if (Thread.currentThread() != connThread) {
        try {
            connThread.join(getConnectionTimeout());
        } catch (InterruptedException ie) {
        // Ignore.
        }
        if (connThread.isAlive()) {
            LOG.log(Level.WARNING, ""Timed out waiting for child connection to close."");
            close();
        }
    }
} 
",,,"/**
 * For creating secrets used for communication with child processes.
 */
 
","Field RND
",38,2391
SparkSubmitOptionParser.java,74,137,0.5401459854014599,"
 * Parser for spark-submit command line options.
 * <p>
 * This class encapsulates the parsing code for spark-submit command line options, so that there
 * is a single list of options that needs to be maintained (well, sort of, but it makes it harder
 * to break things).
 ","/**
 * Parse a list of spark-submit command line options.
 * <p>
 * See SparkSubmitArguments.scala for a more formal description of available options.
 *
 * @throws IllegalArgumentException If an error is found during parsing.
 */
 
/**
 * Callback for when an option with an argument is parsed.
 *
 * @param opt The long name of the cli option (might differ from actual command line).
 * @param value The value. This will be <i>null</i> if the option does not take a value.
 * @return Whether to continue parsing the argument list.
 */
 
/**
 * Callback for when an unrecognized option is parsed.
 *
 * @param opt Unrecognized option from the command line.
 * @return Whether to continue parsing the argument list.
 */
 
/**
 * Callback for remaining command line arguments after either {@link #handle(String, String)} or
 * {@link #handleUnknown(String)} return ""false"". This will be called at the end of parsing even
 * when there are no remaining arguments.
 *
 * @param extra List of remaining arguments.
 */
 
","{
    Pattern eqSeparatedOpt = Pattern.compile(""(--[^=]+)=(.+)"");
    int idx = 0;
    for (idx = 0; idx < args.size(); idx++) {
        String arg = args.get(idx);
        String value = null;
        Matcher m = eqSeparatedOpt.matcher(arg);
        if (m.matches()) {
            arg = m.group(1);
            value = m.group(2);
        }
        // Look for options with a value.
        String name = findCliOption(arg, opts);
        if (name != null) {
            if (value == null) {
                if (idx == args.size() - 1) {
                    throw new IllegalArgumentException(String.format(""Missing argument for option '%s'."", arg));
                }
                idx++;
                value = args.get(idx);
            }
            if (!handle(name, value)) {
                break;
            }
            continue;
        }
        // Look for a switch.
        name = findCliOption(arg, switches);
        if (name != null) {
            if (!handle(name, null)) {
                break;
            }
            continue;
        }
        if (!handleUnknown(arg)) {
            break;
        }
    }
    if (idx < args.size()) {
        idx++;
    }
    handleExtraArgs(args.subList(idx, args.size()));
} 
{
    throw new UnsupportedOperationException();
} 
{
    throw new UnsupportedOperationException();
} 
{
    throw new UnsupportedOperationException();
} 
",,,"/**
 * This is the canonical list of spark-submit options. Each entry in the array contains the
 * different aliases for the same option; the first element of each entry is the ""official""
 * name of the option, passed to {@link #handle(String, String)}.
 * <p>
 * Options not listed here nor in the ""switch"" list below will result in a call to
 * {@link #handleUnknown(String)}.
 * <p>
 * These two arrays are visible for tests.
 */
 
/**
 * List of switches (command line options that do not take parameters) recognized by spark-submit.
 */
 
","Field opts
Field switches
",5,263
AbstractAppHandle.java,29,113,0.25663716814159293,,"/**
 * Mark the handle as disposed, and set it as LOST in case the current state is not final.
 *
 * This method should be called only when there's a reasonable expectation that the communication
 * with the child application is not needed anymore, either because the code managing the handle
 * has said so, or because the child application is finished.
 */
 
","{
    if (!isDisposed()) {
        // First wait for all data from the connection to be read. Then unregister the handle.
        // Otherwise, unregistering might cause the server to be stopped and all child connections
        // to be closed.
        if (connection != null) {
            try {
                connection.waitForClose();
            } catch (IOException ioe) {
            // no-op.
            }
        }
        server.unregister(this);
        // Set state to LOST if not yet final.
        setState(State.LOST, false);
        this.disposed = true;
    }
} 
",,,,,1,0
FilteredObjectInputStream.java,20,25,0.8,"
 * An object input stream that only allows classes used by the launcher protocol to be in the
 * serialized stream. See SPARK-20922.
 ",,,,,,,2,130
LauncherConnection.java,25,70,0.35714285714285715,"
 * Encapsulates a connection between a launcher server and client. This takes care of the
 * communication (sending and receiving messages), while processing of messages is left for
 * the implementations.
 ",,,,,,,3,201
InProcessAppHandle.java,18,54,0.3333333333333333,,,,,,,,1,0
OutputRedirector.java,27,74,0.36486486486486486,"
 * Redirects lines read from a given input stream to a j.u.l.Logger (at INFO level).
 ","/**
 * This method just stops the output of the process from showing up in the local logs.
 * The child's output will still be read (and, thus, the redirect thread will still be
 * alive) to avoid the child process hanging because of lack of output buffer.
 */
 
/**
 * Copied from Apache Commons Lang {@code StringUtils#containsIgnoreCase(String, String)}
 */
 
","{
    active = false;
} 
{
    if (str == null || searchStr == null) {
        return false;
    }
    int len = searchStr.length();
    int max = str.length() - len;
    for (int i = 0; i <= max; i++) {
        if (str.regionMatches(true, i, searchStr, 0, len)) {
            return true;
        }
    }
    return false;
} 
",,,,,1,84
AbstractLauncher.java,142,135,1.0518518518518518,"
 * Base class for launcher implementations.
 *
 * @since Spark 2.3.0
 ","/**
 * Set a custom properties file with Spark configuration for the application.
 *
 * @param path Path to custom properties file to use.
 * @return This launcher.
 */
 
/**
 * Set a single configuration value for the application.
 *
 * @param key Configuration key.
 * @param value The value to use.
 * @return This launcher.
 */
 
/**
 * Set the application name.
 *
 * @param appName Application name.
 * @return This launcher.
 */
 
/**
 * Set the Spark master for the application.
 *
 * @param master Spark master.
 * @return This launcher.
 */
 
/**
 * Set the deploy mode for the application.
 *
 * @param mode Deploy mode.
 * @return This launcher.
 */
 
/**
 * Set the main application resource. This should be the location of a jar file for Scala/Java
 * applications, or a python script for PySpark applications.
 *
 * @param resource Path to the main application resource.
 * @return This launcher.
 */
 
/**
 * Sets the application class name for Java/Scala applications.
 *
 * @param mainClass Application's main class.
 * @return This launcher.
 */
 
/**
 * Adds a no-value argument to the Spark invocation. If the argument is known, this method
 * validates whether the argument is indeed a no-value argument, and throws an exception
 * otherwise.
 * <p>
 * Use this method with caution. It is possible to create an invalid Spark command by passing
 * unknown arguments to this method, since those are allowed for forward compatibility.
 *
 * @since 1.5.0
 * @param arg Argument to add.
 * @return This launcher.
 */
 
/**
 * Adds an argument with a value to the Spark invocation. If the argument name corresponds to
 * a known argument, the code validates that the argument actually expects a value, and throws
 * an exception otherwise.
 * <p>
 * It is safe to add arguments modified by other methods in this class (such as
 * {@link #setMaster(String)} - the last invocation will be the one to take effect.
 * <p>
 * Use this method with caution. It is possible to create an invalid Spark command by passing
 * unknown arguments to this method, since those are allowed for forward compatibility.
 *
 * @since 1.5.0
 * @param name Name of argument to add.
 * @param value Value of the argument.
 * @return This launcher.
 */
 
/**
 * Adds command line arguments for the application.
 *
 * @param args Arguments to pass to the application's main class.
 * @return This launcher.
 */
 
/**
 * Adds a jar file to be submitted with the application.
 *
 * @param jar Path to the jar file.
 * @return This launcher.
 */
 
/**
 * Adds a file to be submitted with the application.
 *
 * @param file Path to the file.
 * @return This launcher.
 */
 
/**
 * Adds a python file / zip / egg to be submitted with the application.
 *
 * @param file Path to the file.
 * @return This launcher.
 */
 
/**
 * Enables verbose reporting for SparkSubmit.
 *
 * @param verbose Whether to enable verbose output.
 * @return This launcher.
 */
 
/**
 * Starts a Spark application.
 *
 * <p>
 * This method returns a handle that provides information about the running application and can
 * be used to do basic interaction with it.
 * <p>
 * The returned handle assumes that the application will instantiate a single SparkContext
 * during its lifetime. Once that context reports a final state (one that indicates the
 * SparkContext has stopped), the handle will not perform new state transitions, so anything
 * that happens after that cannot be monitored. If the underlying application is launched as
 * a child process, {@link SparkAppHandle#kill()} can still be used to kill the child process.
 *
 * @since 1.6.0
 * @param listeners Listeners to add to the handle before the app is launched.
 * @return A handle for the launched application.
 */
 
","{
    checkNotNull(path, ""path"");
    builder.setPropertiesFile(path);
    return self();
} 
{
    checkNotNull(key, ""key"");
    checkNotNull(value, ""value"");
    checkArgument(key.startsWith(""spark.""), ""'key' must start with 'spark.'"");
    builder.conf.put(key, value);
    return self();
} 
{
    checkNotNull(appName, ""appName"");
    builder.appName = appName;
    return self();
} 
{
    checkNotNull(master, ""master"");
    builder.master = master;
    return self();
} 
{
    checkNotNull(mode, ""mode"");
    builder.deployMode = mode;
    return self();
} 
{
    checkNotNull(resource, ""resource"");
    builder.appResource = resource;
    return self();
} 
{
    checkNotNull(mainClass, ""mainClass"");
    builder.mainClass = mainClass;
    return self();
} 
{
    SparkSubmitOptionParser validator = new ArgumentValidator(false);
    validator.parse(Arrays.asList(arg));
    builder.userArgs.add(arg);
    return self();
} 
{
    SparkSubmitOptionParser validator = new ArgumentValidator(true);
    if (validator.MASTER.equals(name)) {
        setMaster(value);
    } else if (validator.PROPERTIES_FILE.equals(name)) {
        setPropertiesFile(value);
    } else if (validator.CONF.equals(name)) {
        String[] vals = value.split(""="", 2);
        setConf(vals[0], vals[1]);
    } else if (validator.CLASS.equals(name)) {
        setMainClass(value);
    } else if (validator.JARS.equals(name)) {
        builder.jars.clear();
        for (String jar : value.split("","")) {
            addJar(jar);
        }
    } else if (validator.FILES.equals(name)) {
        builder.files.clear();
        for (String file : value.split("","")) {
            addFile(file);
        }
    } else if (validator.PY_FILES.equals(name)) {
        builder.pyFiles.clear();
        for (String file : value.split("","")) {
            addPyFile(file);
        }
    } else {
        validator.parse(Arrays.asList(name, value));
        builder.userArgs.add(name);
        builder.userArgs.add(value);
    }
    return self();
} 
{
    for (String arg : args) {
        checkNotNull(arg, ""arg"");
        builder.appArgs.add(arg);
    }
    return self();
} 
{
    checkNotNull(jar, ""jar"");
    builder.jars.add(jar);
    return self();
} 
{
    checkNotNull(file, ""file"");
    builder.files.add(file);
    return self();
} 
{
    checkNotNull(file, ""file"");
    builder.pyFiles.add(file);
    return self();
} 
{
    builder.verbose = verbose;
    return self();
} 
startApplication 
",,,,,3,64
SparkSubmitCommandBuilder.java,114,353,0.32294617563739375,"
 * Special command builder for handling a CLI invocation of SparkSubmit.
 * <p>
 * This builder adds command line parsing compatible with SparkSubmit. It handles setting
 * driver-side options and special parsing behavior needed for the special-casing certain internal
 * Spark applications.
 * <p>
 * This class has also some special features to aid launching shells (pyspark and sparkR) and also
 * examples.
 ","/**
 * Return whether the given main class represents a thrift server.
 */
 
","{
    return (mainClass != null && mainClass.equals(""org.apache.spark.sql.hive.thriftserver.HiveThriftServer2""));
} 
","/**
 * This constructor is used when creating a user-configurable launcher. It allows the
 * spark-submit argument list to be modified after creation.
 */
 
/**
 * This constructor is used when invoking spark-submit; it parses and validates arguments
 * provided by the user on the command line.
 */
 
","{
    this.isSpecialCommand = false;
    this.isExample = false;
    this.parsedArgs = new ArrayList<>();
    this.userArgs = new ArrayList<>();
} 
{
    this.allowsMixedArguments = false;
    this.parsedArgs = new ArrayList<>();
    boolean isExample = false;
    List<String> submitArgs = args;
    this.userArgs = Collections.emptyList();
    if (args.size() > 0) {
        switch(args.get(0)) {
            case PYSPARK_SHELL:
                this.allowsMixedArguments = true;
                appResource = PYSPARK_SHELL;
                submitArgs = args.subList(1, args.size());
                break;
            case SPARKR_SHELL:
                this.allowsMixedArguments = true;
                appResource = SPARKR_SHELL;
                submitArgs = args.subList(1, args.size());
                break;
            case RUN_EXAMPLE:
                isExample = true;
                appResource = SparkLauncher.NO_RESOURCE;
                submitArgs = args.subList(1, args.size());
        }
        this.isExample = isExample;
        OptionParser parser = new OptionParser(true);
        parser.parse(submitArgs);
        this.isSpecialCommand = parser.isSpecialCommand;
    } else {
        this.isExample = isExample;
        this.isSpecialCommand = true;
    }
} 
","/**
 * Name of the app resource used to identify the PySpark shell. The command line parser expects
 * the resource name to be the very first argument to spark-submit in this case.
 *
 * NOTE: this cannot be ""pyspark-shell"" since that identifies the PySpark shell to SparkSubmit
 * (see java_gateway.py), and can cause this code to enter into an infinite loop.
 */
 
/**
 * This is the actual resource name that identifies the PySpark shell to SparkSubmit.
 */
 
/**
 * Name of the app resource used to identify the SparkR shell. The command line parser expects
 * the resource name to be the very first argument to spark-submit in this case.
 *
 * NOTE: this cannot be ""sparkr-shell"" since that identifies the SparkR shell to SparkSubmit
 * (see sparkR.R), and can cause this code to enter into an infinite loop.
 */
 
/**
 * This is the actual resource name that identifies the SparkR shell to SparkSubmit.
 */
 
/**
 * Name of app resource used to identify examples. When running examples, args[0] should be
 * this name. The app resource will identify the example class to run.
 */
 
/**
 * Prefix for example class names.
 */
 
/**
 * This map must match the class names for available special classes, since this modifies the way
 * command line parsing works. This maps the class name to the resource to use when calling
 * spark-submit.
 */
 
/**
 * Controls whether mixing spark-submit arguments with app arguments is allowed. This is needed
 * to parse the command lines for things like bin/spark-shell, which allows users to mix and
 * match arguments (e.g. ""bin/spark-shell SparkShellArg --master foo"").
 */
 
","Field PYSPARK_SHELL
Field PYSPARK_SHELL_RESOURCE
Field SPARKR_SHELL
Field SPARKR_SHELL_RESOURCE
Field RUN_EXAMPLE
Field EXAMPLE_CLASS_PREFIX
Field specialClasses
Field allowsMixedArguments
",8,396
Main.java,59,106,0.5566037735849056,"
 * Command line interface for the Spark launcher. Used internally by Spark scripts.
 |
   * A parser used when command line parsing fails for spark-submit. It's used as a best-effort
   * at trying to identify the class the user wanted to invoke, since that may require special
   * usage strings (handled by SparkSubmitArguments).
   ","/**
 * Usage: Main [class] [class args]
 * <p>
 * This CLI works in two different modes:
 * <ul>
 *   <li>""spark-submit"": if <i>class</i> is ""org.apache.spark.deploy.SparkSubmit"", the
 *   {@link SparkLauncher} class is used to launch a Spark application.</li>
 *   <li>""spark-class"": if another class is provided, an internal Spark class is run.</li>
 * </ul>
 *
 * This class works in tandem with the ""bin/spark-class"" script on Unix-like systems, and
 * ""bin/spark-class2.cmd"" batch script on Windows to execute the final command.
 * <p>
 * On Unix-like systems, the output is a list of command arguments, separated by the NULL
 * character. On Windows, the output is a command line suitable for direct execution from the
 * script.
 */
 
/**
 * Prepare spark commands with the appropriate command builder.
 * If printLaunchCommand is set then the commands will be printed to the stderr.
 */
 
/**
 * Prepare a command line for execution from a Windows batch script.
 *
 * The method quotes all arguments so that spaces are handled as expected. Quotes within arguments
 * are ""double quoted"" (which is batch for escaping a quote). This page has more details about
 * quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html
 */
 
/**
 * Prepare the command for execution from a bash script. The final command will have commands to
 * set up any needed environment variables needed by the child process.
 */
 
","{
    checkArgument(argsArray.length > 0, ""Not enough arguments: missing class name."");
    List<String> args = new ArrayList<>(Arrays.asList(argsArray));
    String className = args.remove(0);
    boolean printLaunchCommand = !isEmpty(System.getenv(""SPARK_PRINT_LAUNCH_COMMAND""));
    Map<String, String> env = new HashMap<>();
    List<String> cmd;
    if (className.equals(""org.apache.spark.deploy.SparkSubmit"")) {
        try {
            AbstractCommandBuilder builder = new SparkSubmitCommandBuilder(args);
            cmd = buildCommand(builder, env, printLaunchCommand);
        } catch (IllegalArgumentException e) {
            printLaunchCommand = false;
            System.err.println(""Error: "" + e.getMessage());
            System.err.println();
            MainClassOptionParser parser = new MainClassOptionParser();
            try {
                parser.parse(args);
            } catch (Exception ignored) {
            // Ignore parsing exceptions.
            }
            List<String> help = new ArrayList<>();
            if (parser.className != null) {
                help.add(parser.CLASS);
                help.add(parser.className);
            }
            help.add(parser.USAGE_ERROR);
            AbstractCommandBuilder builder = new SparkSubmitCommandBuilder(help);
            cmd = buildCommand(builder, env, printLaunchCommand);
        }
    } else {
        AbstractCommandBuilder builder = new SparkClassCommandBuilder(className, args);
        cmd = buildCommand(builder, env, printLaunchCommand);
    }
    if (isWindows()) {
        System.out.println(prepareWindowsCommand(cmd, env));
    } else {
        // A sequence of NULL character and newline separates command-strings and others.
        System.out.println('\0');
        // In bash, use NULL as the arg separator since it cannot be used in an argument.
        List<String> bashCmd = prepareBashCommand(cmd, env);
        for (String c : bashCmd) {
            System.out.print(c);
            System.out.print('\0');
        }
    }
} 
{
    List<String> cmd = builder.buildCommand(env);
    if (printLaunchCommand) {
        System.err.println(""Spark Command: "" + join("" "", cmd));
        System.err.println(""========================================"");
    }
    return cmd;
} 
{
    StringBuilder cmdline = new StringBuilder();
    for (Map.Entry<String, String> e : childEnv.entrySet()) {
        cmdline.append(String.format(""set %s=%s"", e.getKey(), e.getValue()));
        cmdline.append("" && "");
    }
    for (String arg : cmd) {
        cmdline.append(quoteForBatchScript(arg));
        cmdline.append("" "");
    }
    return cmdline.toString();
} 
{
    if (childEnv.isEmpty()) {
        return cmd;
    }
    List<String> newCmd = new ArrayList<>();
    newCmd.add(""env"");
    for (Map.Entry<String, String> e : childEnv.entrySet()) {
        newCmd.add(String.format(""%s=%s"", e.getKey(), e.getValue()));
    }
    newCmd.addAll(cmd);
    return newCmd;
} 
",,,,,5,326
CommandBuilderUtils.java,72,232,0.3103448275862069,"
 * Helper methods for command builders.
 ","/**
 * Returns whether the given string is null or empty.
 */
 
/**
 * Joins a list of strings using the given separator.
 */
 
/**
 * Joins a list of strings using the given separator.
 */
 
/**
 * Returns the first non-empty value mapped to the given key in the given maps, or null otherwise.
 */
 
/**
 * Returns the first non-empty, non-null string in the given list, or null otherwise.
 */
 
/**
 * Returns the name of the env variable that holds the native library path.
 */
 
/**
 * Returns whether the OS is Windows.
 */
 
/**
 * Updates the user environment, appending the given pathList to the existing value of the given
 * environment variable (or setting it if it hasn't yet been set).
 */
 
/**
 * Parse a string as if it were a list of arguments, following bash semantics.
 * For example:
 *
 * Input: ""\""ab cd\"" efgh 'i \"" j'""
 * Output: [ ""ab cd"", ""efgh"", ""i \"" j"" ]
 */
 
/**
 * Throws IllegalArgumentException if the given object is null.
 */
 
/**
 * Throws IllegalArgumentException with the given message if the check is false.
 */
 
/**
 * Throws IllegalStateException with the given message if the check is false.
 */
 
/**
 * Quote a command argument for a command to be run by a Windows batch script, if the argument
 * needs quoting. Arguments only seem to need quotes in batch scripts if they have certain
 * special characters, some of which need extra (and different) escaping.
 *
 *  For example:
 *    original single argument: ab=""cde fgh""
 *    quoted: ""ab^=""""cde fgh""""""
 */
 
/**
 * Quotes a string so that it can be used in a command string.
 * Basically, just add simple escapes. E.g.:
 *    original single argument : ab ""cd"" ef
 *    after: ""ab \""cd\"" ef""
 *
 * This can be parsed back into a single argument by python's ""shlex.split()"" function.
 */
 
/**
 * Get the major version of the java version string supplied. This method
 * accepts any JEP-223-compliant strings (9-ea, 9+100), as well as legacy
 * version strings such as 1.7.0_79
 */
 
/**
 * Find the location of the Spark jars dir, depending on whether we're looking at a build
 * or a distribution directory.
 */
 
","{
    return s == null || s.isEmpty();
} 
{
    StringBuilder sb = new StringBuilder();
    for (String e : elements) {
        if (e != null) {
            if (sb.length() > 0) {
                sb.append(sep);
            }
            sb.append(e);
        }
    }
    return sb.toString();
} 
{
    StringBuilder sb = new StringBuilder();
    for (String e : elements) {
        if (e != null) {
            if (sb.length() > 0) {
                sb.append(sep);
            }
            sb.append(e);
        }
    }
    return sb.toString();
} 
{
    for (Map<?, ?> map : maps) {
        String value = (String) map.get(key);
        if (!isEmpty(value)) {
            return value;
        }
    }
    return null;
} 
{
    for (String s : candidates) {
        if (!isEmpty(s)) {
            return s;
        }
    }
    return null;
} 
{
    if (isWindows()) {
        return ""PATH"";
    }
    String os = System.getProperty(""os.name"");
    if (os.startsWith(""Mac OS X"")) {
        return ""DYLD_LIBRARY_PATH"";
    } else {
        return ""LD_LIBRARY_PATH"";
    }
} 
{
    String os = System.getProperty(""os.name"");
    return os.startsWith(""Windows"");
} 
{
    if (!isEmpty(pathList)) {
        String current = firstNonEmpty(userEnv.get(envKey), System.getenv(envKey));
        userEnv.put(envKey, join(File.pathSeparator, current, pathList));
    }
} 
{
    List<String> opts = new ArrayList<>();
    StringBuilder opt = new StringBuilder();
    boolean inOpt = false;
    boolean inSingleQuote = false;
    boolean inDoubleQuote = false;
    boolean escapeNext = false;
    // This is needed to detect when a quoted empty string is used as an argument ("""" or '').
    boolean hasData = false;
    for (int i = 0; i < s.length(); i++) {
        int c = s.codePointAt(i);
        if (escapeNext) {
            opt.appendCodePoint(c);
            escapeNext = false;
        } else if (inOpt) {
            switch(c) {
                case '\\':
                    if (inSingleQuote) {
                        opt.appendCodePoint(c);
                    } else {
                        escapeNext = true;
                    }
                    break;
                case '\'':
                    if (inDoubleQuote) {
                        opt.appendCodePoint(c);
                    } else {
                        inSingleQuote = !inSingleQuote;
                    }
                    break;
                case '""':
                    if (inSingleQuote) {
                        opt.appendCodePoint(c);
                    } else {
                        inDoubleQuote = !inDoubleQuote;
                    }
                    break;
                default:
                    if (!Character.isWhitespace(c) || inSingleQuote || inDoubleQuote) {
                        opt.appendCodePoint(c);
                    } else {
                        opts.add(opt.toString());
                        opt.setLength(0);
                        inOpt = false;
                        hasData = false;
                    }
            }
        } else {
            switch(c) {
                case '\'':
                    inSingleQuote = true;
                    inOpt = true;
                    hasData = true;
                    break;
                case '""':
                    inDoubleQuote = true;
                    inOpt = true;
                    hasData = true;
                    break;
                case '\\':
                    escapeNext = true;
                    inOpt = true;
                    hasData = true;
                    break;
                default:
                    if (!Character.isWhitespace(c)) {
                        inOpt = true;
                        hasData = true;
                        opt.appendCodePoint(c);
                    }
            }
        }
    }
    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, ""Invalid option string: %s"", s);
    if (hasData) {
        opts.add(opt.toString());
    }
    return opts;
} 
{
    if (o == null) {
        throw new IllegalArgumentException(String.format(""'%s' must not be null."", arg));
    }
} 
{
    if (!check) {
        throw new IllegalArgumentException(String.format(msg, args));
    }
} 
{
    if (!check) {
        throw new IllegalStateException(String.format(msg, args));
    }
} 
{
    boolean needsQuotes = false;
    for (int i = 0; i < arg.length(); i++) {
        int c = arg.codePointAt(i);
        if (Character.isWhitespace(c) || c == '""' || c == '=' || c == ',' || c == ';') {
            needsQuotes = true;
            break;
        }
    }
    if (!needsQuotes) {
        return arg;
    }
    StringBuilder quoted = new StringBuilder();
    quoted.append(""\"""");
    for (int i = 0; i < arg.length(); i++) {
        int cp = arg.codePointAt(i);
        switch(cp) {
            case '""':
                quoted.append('""');
                break;
            default:
                break;
        }
        quoted.appendCodePoint(cp);
    }
    if (arg.codePointAt(arg.length() - 1) == '\\') {
        quoted.append(""\\"");
    }
    quoted.append(""\"""");
    return quoted.toString();
} 
{
    StringBuilder quoted = new StringBuilder().append('""');
    for (int i = 0; i < s.length(); i++) {
        int cp = s.codePointAt(i);
        if (cp == '""' || cp == '\\') {
            quoted.appendCodePoint('\\');
        }
        quoted.appendCodePoint(cp);
    }
    return quoted.append('""').toString();
} 
{
    String[] version = javaVersion.split(""[+.\\-]+"");
    int major = Integer.parseInt(version[0]);
    // if major > 1, we're using the JEP-223 version string, e.g., 9-ea, 9+120
    // otherwise the second number is the major version
    if (major > 1) {
        return major;
    } else {
        return Integer.parseInt(version[1]);
    }
} 
{
    // TODO: change to the correct directory once the assembly build is changed.
    File libdir = new File(sparkHome, ""jars"");
    if (!libdir.isDirectory()) {
        libdir = new File(sparkHome, String.format(""assembly/target/scala-%s/jars"", scalaVersion));
        if (!libdir.isDirectory()) {
            checkState(!failIfNotFound, ""Library directory '%s' does not exist; make sure Spark is built."", libdir.getAbsolutePath());
            return null;
        }
    }
    return libdir.getAbsolutePath();
} 
",,,,,1,39
ChildProcAppHandle.java,39,81,0.48148148148148145,"
 * Handle implementation for monitoring apps started as a child process.
 ","/**
 * Parses the logs of {@code spark-submit} and returns the last exception thrown.
 * <p>
 * Since {@link SparkLauncher} runs {@code spark-submit} in a sub-process, it's difficult to
 * accurately retrieve the full {@link Throwable} from the {@code spark-submit} process.
 * This method parses the logs of the sub-process and provides a best-effort attempt at
 * returning the last exception thrown by the {@code spark-submit} process. Only the exception
 * message is parsed, the associated stacktrace is meaningless.
 *
 * @return an {@link Optional} containing a {@link RuntimeException} with the parsed
 * exception, otherwise returns a {@link Optional#EMPTY}
 */
 
/**
 * Wait for the child process to exit and update the handle's state if necessary, according to
 * the exit code.
 */
 
","{
    return redirector != null ? Optional.ofNullable(redirector.getError()) : Optional.empty();
} 
{
    Process proc = childProc;
    if (proc == null) {
        // Process may have already been disposed of, e.g. by calling kill().
        return;
    }
    while (proc.isAlive()) {
        try {
            proc.waitFor();
        } catch (Exception e) {
            LOG.log(Level.WARNING, ""Exception waiting for child process to exit."", e);
        }
    }
    synchronized (this) {
        if (isDisposed()) {
            return;
        }
        int ec;
        try {
            ec = proc.exitValue();
        } catch (Exception e) {
            LOG.log(Level.WARNING, ""Exception getting child process exit code, assuming failure."", e);
            ec = 1;
        }
        if (ec != 0) {
            State currState = getState();
            // Override state with failure if the current state is not final, or is success.
            if (!currState.isFinal() || currState == State.FINISHED) {
                setState(State.FAILED, true);
            }
        }
        dispose();
    }
} 
",,,,,1,72
SparkAppHandle.java,85,32,2.65625,"
 * A handle to a running Spark application.
 * <p>
 * Provides runtime information about the underlying Spark application, and actions to control it.
 *
 * @since 1.6.0
 |
   * Represents the application's state. A state can be ""final"", in which case it will not change
   * after it's reached, and means the application is not running anymore.
   *
   * @since 1.6.0
   |
   * Listener for updates to a handle's state. The callbacks do not receive information about
   * what exactly has changed, just that an update has occurred.
   *
   * @since 1.6.0
   ","/**
 * Whether this state is a final state, meaning the application is not running anymore
 * once it's reached.
 */
 
/**
 * Adds a listener to be notified of changes to the handle's information. Listeners will be called
 * from the thread processing updates from the application, so they should avoid blocking or
 * long-running operations.
 *
 * @param l Listener to add.
 */
 
/**
 * Returns the current application state.
 */
 
/**
 * Returns the application ID, or <code>null</code> if not yet known.
 */
 
/**
 * Asks the application to stop. This is best-effort, since the application may fail to receive
 * or act on the command. Callers should watch for a state transition that indicates the
 * application has really stopped.
 */
 
/**
 * Tries to kill the underlying application. Implies {@link #disconnect()}. This will not send
 * a {@link #stop()} message to the application, so it's recommended that users first try to
 * stop the application cleanly and only resort to this method if that fails.
 */
 
/**
 * Disconnects the handle from the application, without stopping it. After this method is called,
 * the handle will not be able to communicate with the application anymore.
 */
 
/**
 * If the application failed due to an error, return the underlying error. If the app
 * succeeded, this method returns an empty {@link Optional}.
 */
 
/**
 * Callback for changes in the handle's state.
 *
 * @param handle The updated handle.
 * @see SparkAppHandle#getState()
 */
 
/**
 * Callback for changes in any information that is not the handle's state.
 *
 * @param handle The updated handle.
 */
 
","{
    return isFinal;
} 
addListener 
getState 
getAppId 
stop 
kill 
disconnect 
getError 
stateChanged 
infoChanged 
",,,,,15,530
Java8RDDAPISuite.java,24,282,0.0851063829787234,"
 * Most of these tests replicate org.apache.spark.JavaAPISuite using java 8
 * lambda syntax.
 ",,,,,,,2,91
JavaSparkContextSuite.java,19,33,0.5757575757575758,"
 * Java apps can use both Java-friendly JavaSparkContext and Scala SparkContext.
 ",,,,,,,1,80
JavaAPISuite.java,63,1363,0.046221570066030816," The test suite itself is Serializable so that anonymous Function implementations can be| serialized, as an alternative to converting these anonymous classes to static inner classes;| see http://stackoverflow.com/questions/758570/.",,,,,,,1,231
JavaTaskContextCompileCheck.java,28,41,0.6829268292682927,"
 * Something to make sure that TaskContext can be used in Java.
 |
   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and
   * TaskContext is Java friendly.
   |
   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and
   * TaskContext is Java friendly.
   ",,,,,,,7,329
JavaJdbcRDDSuite.java,19,65,0.2923076923076923,,,,,,,,1,0
SparkLauncherSuite.java,51,243,0.20987654320987653,"
 * These tests require the Spark assembly to be built before they can be run.
 |
   * Similar to {@link InProcessTestApp} except it throws an exception
   ",,,,,"/**
 * SPARK-23020: there's a race caused by a child app finishing too quickly. This would cause
 * the InProcessAppHandle to dispose of itself even before the child connection was properly
 * established, so no state changes would be detected for the application and its final
 * state would be LOST.
 *
 * It's not really possible to fix that race safely in the handle code itself without changing
 * the way in-process apps talk to the launcher library, so we work around that in the test by
 * synchronizing on this object.
 */
 
","Field LOCK
",3,150
ShuffleInMemoryRadixSorterSuite.java,16,5,3.2,,,,,,,,1,0
PackedRecordPointerSuite.java,18,82,0.21951219512195122,,,,,,,,1,0
UnsafeShuffleWriterSuite.java,32,490,0.0653061224489796,,,,,,,,1,0
ShuffleInMemorySorterSuite.java,18,115,0.1565217391304348,,,,,,,,1,0
TestJavaSerializerImpl.java,19,54,0.35185185185185186,"
 * A simple Serializer implementation to make sure the API is Java-friendly.
 ",,,,,,,1,76
TestMemoryConsumer.java,16,31,0.5161290322580645,,,,,,,,1,0
TaskMemoryManagerSuite.java,20,165,0.12121212121212122,,,,,,,,1,0
TestTimSort.java,61,61,1.0,"
 * This codes generates a int array which fails the standard TimSort.
 *
 * The blog that reported the bug
 * http://www.envisage-project.eu/timsort-specification-and-verification/
 *
 * This codes was originally wrote by Stijn de Gouw, modified by Evan Yu to adapt to
 * our test suite.
 *
 * https://github.com/abstools/java-timsort-bug
 * https://github.com/abstools/java-timsort-bug/blob/master/LICENSE
 ","/**
 * Returns an array of integers that demonstrate the bug in TimSort
 */
 
/**
 * Fills <code>runs</code> with a sequence of run lengths of the form<br>
 * Y_n     x_{n,1}   x_{n,2}   ... x_{n,l_n} <br>
 * Y_{n-1} x_{n-1,1} x_{n-1,2} ... x_{n-1,l_{n-1}} <br>
 * ... <br>
 * Y_1     x_{1,1}   x_{1,2}   ... x_{1,l_1}<br>
 * The Y_i's are chosen to satisfy the invariant throughout execution,
 * but the x_{i,j}'s are merged (by <code>TimSort.mergeCollapse</code>)
 * into an X_i that violates the invariant.
 *
 * @param length The sum of all run lengths that will be added to <code>runs</code>.
 */
 
/**
 * Adds a sequence x_1, ..., x_n of run lengths to <code>runs</code> such that:<br>
 * 1. X = x_1 + ... + x_n <br>
 * 2. x_j >= minRun for all j <br>
 * 3. x_1 + ... + x_{j-2}  <  x_j  <  x_1 + ... + x_{j-1} for all j <br>
 * These conditions guarantee that TimSort merges all x_j's one by one
 * (resulting in X) using only merges on the second-to-last element.
 *
 * @param X The sum of the sequence that should be added to runs.
 */
 
","{
    int minRun = minRunLength(length);
    List<Long> runs = runsJDKWorstCase(minRun, length);
    return createArray(runs, length);
} 
{
    List<Long> runs = new ArrayList<>();
    long runningTotal = 0, Y = minRun + 4, X = minRun;
    while (runningTotal + Y + X <= length) {
        runningTotal += X + Y;
        generateJDKWrongElem(runs, minRun, X);
        runs.add(0, Y);
        // X_{i+1} = Y_i + x_{i,1} + 1, since runs.get(1) = x_{i,1}
        X = Y + runs.get(1) + 1;
        // Y_{i+1} = X_{i+1} + Y_i + 1
        Y += X + 1;
    }
    if (runningTotal + X <= length) {
        runningTotal += X;
        generateJDKWrongElem(runs, minRun, X);
    }
    runs.add(length - runningTotal);
    return runs;
} 
{
    for (long newTotal; X >= 2 * minRun + 1; X = newTotal) {
        // Default strategy
        newTotal = X / 2 + 1;
        // Specialized strategies
        if (3 * minRun + 3 <= X && X <= 4 * minRun + 1) {
            // add x_1=MIN+1, x_2=MIN, x_3=X-newTotal  to runs
            newTotal = 2 * minRun + 1;
        } else if (5 * minRun + 5 <= X && X <= 6 * minRun + 5) {
            // add x_1=MIN+1, x_2=MIN, x_3=MIN+2, x_4=X-newTotal  to runs
            newTotal = 3 * minRun + 3;
        } else if (8 * minRun + 9 <= X && X <= 10 * minRun + 9) {
            // add x_1=MIN+1, x_2=MIN, x_3=MIN+2, x_4=2MIN+2, x_5=X-newTotal  to runs
            newTotal = 5 * minRun + 5;
        } else if (13 * minRun + 15 <= X && X <= 16 * minRun + 17) {
            // add x_1=MIN+1, x_2=MIN, x_3=MIN+2, x_4=2MIN+2, x_5=3MIN+4, x_6=X-newTotal to runs
            newTotal = 8 * minRun + 9;
        }
        runs.add(0, X - newTotal);
    }
    runs.add(0, X);
} 
",,,,,10,388
UnsafeExternalSorterRadixSortSuite.java,16,5,3.2,,,,,,,,1,0
UnsafeExternalSorterSuite.java,48,463,0.10367170626349892,,,,,,,,1,0
UnsafeInMemorySorterRadixSortSuite.java,16,5,3.2,,,,,,,,1,0
UnsafeInMemorySorterSuite.java,29,152,0.19078947368421054,,,,,,,,1,0
SerializableConfigurationSuite.java,16,30,0.5333333333333333,,,,,,,,1,0
GenericFileInputStreamSuite.java,23,105,0.21904761904761905,"
 * Tests functionality of {@link NioBufferedFileInputStream}
 ",,,,,,,1,60
NioBufferedInputStreamSuite.java,19,14,1.3571428571428572,"
 * Tests functionality of {@link NioBufferedFileInputStream}
 ",,,,,,,1,60
ReadAheadInputStreamSuite.java,24,17,1.411764705882353,"
 * Tests functionality of {@link ReadAheadInputStreamSuite}
 ",,,,,,,1,59
BytesToBytesMapOffHeapSuite.java,16,7,2.2857142857142856,,,,,,,,1,0
BytesToBytesMapOnHeapSuite.java,16,7,2.2857142857142856,,,,,,,,1,0
AbstractBytesToBytesMapSuite.java,54,638,0.08463949843260188,,"/**
 * Fast equality checking for byte arrays, since these comparisons are a bottleneck
 * in our stress tests.
 */
 
","{
    return (actualLengthBytes == expected.length) && ByteArrayMethods.arrayEquals(expected, Platform.BYTE_ARRAY_OFFSET, base, offset, expected.length);
} 
",,,,,1,0
OptionalSuite.java,19,63,0.30158730158730157,"
 * Tests {@link Optional}.
 ",,,,,,,1,26
ExecutorPluginSuite.java,25,129,0.1937984496124031,,,,,,,,1,0
ExecutorPlugin.java,48,7,6.857142857142857,"
 * A plugin which can be automatically instantiated within each Spark executor.  Users can specify
 * plugins which should be created with the ""spark.executor.plugins"" configuration.  An instance
 * of each plugin will be created for every executor, including those created by dynamic allocation,
 * before the executor starts running any tasks.
 *
 * The specific api exposed to the end users still considered to be very unstable.  We will
 * hopefully be able to keep compatibility by providing default implementations for any methods
 * added, but make no guarantees this will always be possible across all Spark releases.
 *
 * Spark does nothing to verify the plugin is doing legitimate things, or to manage the resources
 * it uses.  A plugin acquires the same privileges as the user running the task.  A bad plugin
 * could also interfere with task execution and make the executor fail in unexpected ways.
 ","/**
 * Initialize the executor plugin.
 *
 * <p>Each executor will, during its initialization, invoke this method on each
 * plugin provided in the spark.executor.plugins configuration. The Spark executor
 * will wait on the completion of the execution of the init method.</p>
 *
 * <p>Plugins should create threads in their implementation of this method for
 * any polling, blocking, or intensive computation.</p>
 *
 * @param pluginContext Context information for the executor where the plugin is running.
 */
 
/**
 * Clean up and terminate this plugin.
 *
 * <p>This function is called during the executor shutdown phase. The executor
 * will wait for the plugin to terminate before continuing its own shutdown.</p>
 */
 
","{
} 
{
} 
",,,,,12,890
SpillInfo.java,19,13,1.4615384615384615,"
 * Metadata for a block of data written by {@link ShuffleExternalSorter}.
 ",,,,,,,1,73
UnsafeShuffleWriter.java,91,401,0.22693266832917705," Subclass of ByteArrayOutputStream that exposes `buf` directly. ","/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * This convenience method should only be called in test code.
 */
 
/**
 * Merge zero or more spill files together, choosing the fastest merging strategy based on the
 * number of spills and the IO compression codec.
 *
 * @return the partition lengths in the merged file.
 */
 
/**
 * Merges spill files using Java FileStreams. This code path is typically slower than
 * the NIO-based merge, {@link UnsafeShuffleWriter#mergeSpillsWithTransferTo(SpillInfo[],
 * ShuffleMapOutputWriter)}, and it's mostly used in cases where the IO compression codec
 * does not support concatenation of compressed data, when encryption is enabled, or when
 * users have explicitly disabled use of {@code transferTo} in order to work around kernel bugs.
 * This code path might also be faster in cases where individual partition size in a spill
 * is small and UnsafeShuffleWriter#mergeSpillsWithTransferTo method performs many small
 * disk ios which is inefficient. In those case, Using large buffers for input and output
 * files helps reducing the number of disk ios, making the file merging faster.
 *
 * @param spills the spills to merge.
 * @param mapWriter the map output writer to use for output.
 * @param compressionCodec the IO compression codec, or null if shuffle compression is disabled.
 * @return the partition lengths in the merged file.
 */
 
/**
 * Merges spill files by using NIO's transferTo to concatenate spill partitions' bytes.
 * This is only safe when the IO compression codec and serializer support concatenation of
 * serialized streams.
 *
 * @param spills the spills to merge.
 * @param mapWriter the map output writer to use for output.
 * @return the partition lengths in the merged file.
 */
 
","{
    updatePeakMemoryUsed();
    return peakMemoryUsedBytes;
} 
{
    write(JavaConverters.asScalaIteratorConverter(records).asScala());
} 
{
    long[] partitionLengths;
    if (spills.length == 0) {
        final ShuffleMapOutputWriter mapWriter = shuffleExecutorComponents.createMapOutputWriter(shuffleId, mapId, partitioner.numPartitions());
        return mapWriter.commitAllPartitions();
    } else if (spills.length == 1) {
        Optional<SingleSpillShuffleMapOutputWriter> maybeSingleFileWriter = shuffleExecutorComponents.createSingleFileMapOutputWriter(shuffleId, mapId);
        if (maybeSingleFileWriter.isPresent()) {
            // Here, we don't need to perform any metrics updates because the bytes written to this
            // output file would have already been counted as shuffle bytes written.
            partitionLengths = spills[0].partitionLengths;
            maybeSingleFileWriter.get().transferMapSpillFile(spills[0].file, partitionLengths);
        } else {
            partitionLengths = mergeSpillsUsingStandardWriter(spills);
        }
    } else {
        partitionLengths = mergeSpillsUsingStandardWriter(spills);
    }
    return partitionLengths;
} 
{
    final int numPartitions = partitioner.numPartitions();
    final InputStream[] spillInputStreams = new InputStream[spills.length];
    boolean threwException = true;
    try {
        for (int i = 0; i < spills.length; i++) {
            spillInputStreams[i] = new NioBufferedFileInputStream(spills[i].file, inputBufferSizeInBytes);
        }
        for (int partition = 0; partition < numPartitions; partition++) {
            boolean copyThrewException = true;
            ShufflePartitionWriter writer = mapWriter.getPartitionWriter(partition);
            OutputStream partitionOutput = writer.openStream();
            try {
                partitionOutput = new TimeTrackingOutputStream(writeMetrics, partitionOutput);
                partitionOutput = blockManager.serializerManager().wrapForEncryption(partitionOutput);
                if (compressionCodec != null) {
                    partitionOutput = compressionCodec.compressedOutputStream(partitionOutput);
                }
                for (int i = 0; i < spills.length; i++) {
                    final long partitionLengthInSpill = spills[i].partitionLengths[partition];
                    if (partitionLengthInSpill > 0) {
                        InputStream partitionInputStream = null;
                        boolean copySpillThrewException = true;
                        try {
                            partitionInputStream = new LimitedInputStream(spillInputStreams[i], partitionLengthInSpill, false);
                            partitionInputStream = blockManager.serializerManager().wrapForEncryption(partitionInputStream);
                            if (compressionCodec != null) {
                                partitionInputStream = compressionCodec.compressedInputStream(partitionInputStream);
                            }
                            ByteStreams.copy(partitionInputStream, partitionOutput);
                            copySpillThrewException = false;
                        } finally {
                            Closeables.close(partitionInputStream, copySpillThrewException);
                        }
                    }
                }
                copyThrewException = false;
            } finally {
                Closeables.close(partitionOutput, copyThrewException);
            }
            long numBytesWritten = writer.getNumBytesWritten();
            writeMetrics.incBytesWritten(numBytesWritten);
        }
        threwException = false;
    } finally {
        // To avoid masking exceptions that caused us to prematurely enter the finally block, only
        // throw exceptions during cleanup if threwException == false.
        for (InputStream stream : spillInputStreams) {
            Closeables.close(stream, threwException);
        }
    }
} 
{
    final int numPartitions = partitioner.numPartitions();
    final FileChannel[] spillInputChannels = new FileChannel[spills.length];
    final long[] spillInputChannelPositions = new long[spills.length];
    boolean threwException = true;
    try {
        for (int i = 0; i < spills.length; i++) {
            spillInputChannels[i] = new FileInputStream(spills[i].file).getChannel();
        }
        for (int partition = 0; partition < numPartitions; partition++) {
            boolean copyThrewException = true;
            ShufflePartitionWriter writer = mapWriter.getPartitionWriter(partition);
            WritableByteChannelWrapper resolvedChannel = writer.openChannelWrapper().orElseGet(() -> new StreamFallbackChannelWrapper(openStreamUnchecked(writer)));
            try {
                for (int i = 0; i < spills.length; i++) {
                    long partitionLengthInSpill = spills[i].partitionLengths[partition];
                    final FileChannel spillInputChannel = spillInputChannels[i];
                    final long writeStartTime = System.nanoTime();
                    Utils.copyFileStreamNIO(spillInputChannel, resolvedChannel.channel(), spillInputChannelPositions[i], partitionLengthInSpill);
                    copyThrewException = false;
                    spillInputChannelPositions[i] += partitionLengthInSpill;
                    writeMetrics.incWriteTime(System.nanoTime() - writeStartTime);
                }
            } finally {
                Closeables.close(resolvedChannel, copyThrewException);
            }
            long numBytes = writer.getNumBytesWritten();
            writeMetrics.incBytesWritten(numBytes);
        }
        threwException = false;
    } finally {
        // To avoid masking exceptions that caused us to prematurely enter the finally block, only
        // throw exceptions during cleanup if threwException == false.
        for (int i = 0; i < spills.length; i++) {
            assert (spillInputChannelPositions[i] == spills[i].file.length());
            Closeables.close(spillInputChannels[i], threwException);
        }
    }
} 
",,,"/**
 * Are we in the process of stopping? Because map tasks can call stop() with success = true
 * and then call stop() with success = false if they get an exception, we want to make sure
 * we don't try deleting files, etc twice.
 */
 
","Field stopping
",1,64
ShuffleSortDataFormat.java,17,49,0.3469387755102041,,,,,,,,1,0
LocalDiskShuffleDataIO.java,20,19,1.0526315789473684,"
 * Implementation of the {@link ShuffleDataIO} plugin system that replicates the local shuffle
 * storage and index file functionality that has historically been used from Spark 2.4 and earlier.
 ",,,,,,,2,192
LocalDiskShuffleDriverComponents.java,17,24,0.7083333333333334,,,,,,,,1,0
LocalDiskShuffleMapOutputWriter.java,28,214,0.1308411214953271,"
 * Implementation of {@link ShuffleMapOutputWriter} that replicates the functionality of shuffle
 * persisting shuffle data to local disk alongside index files, identical to Spark's historic
 * canonical shuffle storage mechanism.
 ",,,,,,,3,226
LocalDiskSingleSpillMapOutputWriter.java,18,30,0.6,,,,,,,,1,0
LocalDiskShuffleExecutorComponents.java,16,58,0.27586206896551724,,,,,,,,1,0
BypassMergeSortShuffleWriter.java,57,214,0.26635514018691586,"
 * This class implements sort-based shuffle's hash-style shuffle fallback path. This write path
 * writes incoming records to separate files, one file per reduce partition, then concatenates these
 * per-partition files to form a single output file, regions of which are served to reducers.
 * Records are not buffered in memory. It writes output in a format
 * that can be served / consumed via {@link org.apache.spark.shuffle.IndexShuffleBlockResolver}.
 * <p>
 * This write path is inefficient for shuffles with large numbers of reduce partitions because it
 * simultaneously opens separate serializers and file streams for all partitions. As a result,
 * {@link SortShuffleManager} only selects this write path when
 * <ul>
 *    <li>no map-side combine is specified, and</li>
 *    <li>the number of partitions is less than or equal to
 *      <code>spark.shuffle.sort.bypassMergeThreshold</code>.</li>
 * </ul>
 *
 * This code used to be part of {@link org.apache.spark.util.collection.ExternalSorter} but was
 * refactored into its own class in order to reduce code complexity; see SPARK-7855 for details.
 * <p>
 * There have been proposals to completely remove this code path; see SPARK-6026 for details.
 ","/**
 * Concatenate all of the per-partition files into a single combined file.
 *
 * @return array of lengths, in bytes, of each partition of the file (used by map output tracker).
 */
 
","{
    // Track location of the partition starts in the output file
    if (partitionWriters != null) {
        final long writeStartTime = System.nanoTime();
        try {
            for (int i = 0; i < numPartitions; i++) {
                final File file = partitionWriterSegments[i].file();
                ShufflePartitionWriter writer = mapOutputWriter.getPartitionWriter(i);
                if (file.exists()) {
                    if (transferToEnabled) {
                        // Using WritableByteChannelWrapper to make resource closing consistent between
                        // this implementation and UnsafeShuffleWriter.
                        Optional<WritableByteChannelWrapper> maybeOutputChannel = writer.openChannelWrapper();
                        if (maybeOutputChannel.isPresent()) {
                            writePartitionedDataWithChannel(file, maybeOutputChannel.get());
                        } else {
                            writePartitionedDataWithStream(file, writer);
                        }
                    } else {
                        writePartitionedDataWithStream(file, writer);
                    }
                    if (!file.delete()) {
                        logger.error(""Unable to delete file for partition {}"", i);
                    }
                }
            }
        } finally {
            writeMetrics.incWriteTime(System.nanoTime() - writeStartTime);
        }
        partitionWriters = null;
    }
    return mapOutputWriter.commitAllPartitions();
} 
",,,"/**
 * Array of file writers, one for each partition
 */
 
/**
 * Are we in the process of stopping? Because map tasks can call stop() with success = true
 * and then call stop() with success = false if they get an exception, we want to make sure
 * we don't try deleting files, etc twice.
 */
 
","Field partitionWriters
Field stopping
",19,1177
ShuffleExternalSorter.java,135,256,0.52734375,"
 * An external sorter that is specialized for sort-based shuffle.
 * <p>
 * Incoming records are appended to data pages. When all records have been inserted (or when the
 * current thread's shuffle memory limit is reached), the in-memory records are sorted according to
 * their partition ids (using a {@link ShuffleInMemorySorter}). The sorted records are then
 * written to a single output file (or multiple files, if we've spilled). The format of the output
 * files is the same as the format of the final output file written by
 * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are
 * written as a single serialized, compressed stream that can be read with a new decompression and
 * deserialization stream.
 * <p>
 * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its
 * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a
 * specialized merge procedure that avoids extra serialization/deserialization.
 ","/**
 * Sorts the in-memory records and writes the sorted records to an on-disk file.
 * This method does not free the sort data structures.
 *
 * @param isLastFile if true, this indicates that we're writing the final output file and that the
 *                   bytes written should be counted towards shuffle spill metrics rather than
 *                   shuffle write metrics.
 */
 
/**
 * Sort and spill the current records in response to memory pressure.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * Force all memory and spill files to be deleted; called by shuffle error-handling code.
 */
 
/**
 * Checks whether there is enough space to insert an additional record in to the sort pointer
 * array and grows the array if additional space is required. If the required space cannot be
 * obtained, then the in-memory data will be spilled to disk.
 */
 
/**
 * Allocates more memory in order to insert an additional record. This will request additional
 * memory from the memory manager and spill if the requested memory can not be obtained.
 *
 * @param required the required space in the data page, in bytes, including space for storing
 *                      the record size. This must be less than or equal to the page size (records
 *                      that exceed the page size are handled via a different code path which uses
 *                      special overflow pages).
 */
 
/**
 * Write a record to the shuffle sorter.
 */
 
/**
 * Close the sorter, causing any buffered data to be sorted and written out to disk.
 *
 * @return metadata for the spill files written by this sorter. If no records were ever inserted
 *         into this sorter, then this will return an empty array.
 */
 
","{
    // This call performs the actual sort.
    final ShuffleInMemorySorter.ShuffleSorterIterator sortedRecords = inMemSorter.getSortedIterator();
    // If there are no sorted records, so we don't need to create an empty spill file.
    if (!sortedRecords.hasNext()) {
        return;
    }
    final ShuffleWriteMetricsReporter writeMetricsToUse;
    if (isLastFile) {
        // We're writing the final non-spill file, so we _do_ want to count this as shuffle bytes.
        writeMetricsToUse = writeMetrics;
    } else {
        // We're spilling, so bytes written should be counted towards spill rather than write.
        // Create a dummy WriteMetrics object to absorb these metrics, since we don't want to count
        // them towards shuffle bytes written.
        writeMetricsToUse = new ShuffleWriteMetrics();
    }
    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn't seem to
    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer
    // data through a byte array. This array does not need to be large enough to hold a single
    // record;
    final byte[] writeBuffer = new byte[diskWriteBufferSize];
    // Because this output will be read during shuffle, its compression codec must be controlled by
    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use
    // createTempShuffleBlock here; see SPARK-3426 for more details.
    final Tuple2<TempShuffleBlockId, File> spilledFileInfo = blockManager.diskBlockManager().createTempShuffleBlock();
    final File file = spilledFileInfo._2();
    final TempShuffleBlockId blockId = spilledFileInfo._1();
    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);
    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.
    // Our write path doesn't actually use this serializer (since we end up calling the `write()`
    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work
    // around this, we pass a dummy no-op serializer.
    final SerializerInstance ser = DummySerializerInstance.INSTANCE;
    int currentPartition = -1;
    final FileSegment committedSegment;
    try (DiskBlockObjectWriter writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse)) {
        final int uaoSize = UnsafeAlignedOffset.getUaoSize();
        while (sortedRecords.hasNext()) {
            sortedRecords.loadNext();
            final int partition = sortedRecords.packedRecordPointer.getPartitionId();
            assert (partition >= currentPartition);
            if (partition != currentPartition) {
                // Switch to the new partition
                if (currentPartition != -1) {
                    final FileSegment fileSegment = writer.commitAndGet();
                    spillInfo.partitionLengths[currentPartition] = fileSegment.length();
                }
                currentPartition = partition;
            }
            final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();
            final Object recordPage = taskMemoryManager.getPage(recordPointer);
            final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);
            int dataRemaining = UnsafeAlignedOffset.getSize(recordPage, recordOffsetInPage);
            // skip over record length
            long recordReadPosition = recordOffsetInPage + uaoSize;
            while (dataRemaining > 0) {
                final int toTransfer = Math.min(diskWriteBufferSize, dataRemaining);
                Platform.copyMemory(recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);
                writer.write(writeBuffer, 0, toTransfer);
                recordReadPosition += toTransfer;
                dataRemaining -= toTransfer;
            }
            writer.recordWritten();
        }
        committedSegment = writer.commitAndGet();
    }
    // If `writeSortedFile()` was called from `closeAndGetSpills()` and no records were inserted,
    // then the file might be empty. Note that it might be better to avoid calling
    // writeSortedFile() in that case.
    if (currentPartition != -1) {
        spillInfo.partitionLengths[currentPartition] = committedSegment.length();
        spills.add(spillInfo);
    }
    if (!isLastFile) {
        // i.e. this is a spill file
        // The current semantics of `shuffleRecordsWritten` seem to be that it's updated when records
        // are written to disk, not when they enter the shuffle sorting code. DiskBlockObjectWriter
        // relies on its `recordWritten()` method being called in order to trigger periodic updates to
        // `shuffleBytesWritten`. If we were to remove the `recordWritten()` call and increment that
        // counter at a higher-level, then the in-progress metrics for records written and bytes
        // written would get out of sync.
        // 
        // When writing the last file, we pass `writeMetrics` directly to the DiskBlockObjectWriter;
        // in all other cases, we pass in a dummy write metrics to capture metrics, then copy those
        // metrics to the true write metrics here. The reason for performing this copying is so that
        // we can avoid reporting spilled bytes as shuffle write bytes.
        // 
        // Note that we intentionally ignore the value of `writeMetricsToUse.shuffleWriteTime()`.
        // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.
        // SPARK-3577 tracks the spill time separately.
        // This is guaranteed to be a ShuffleWriteMetrics based on the if check in the beginning
        // of this method.
        writeMetrics.incRecordsWritten(((ShuffleWriteMetrics) writeMetricsToUse).recordsWritten());
        taskContext.taskMetrics().incDiskBytesSpilled(((ShuffleWriteMetrics) writeMetricsToUse).bytesWritten());
    }
} 
{
    if (trigger != this || inMemSorter == null || inMemSorter.numRecords() == 0) {
        return 0L;
    }
    logger.info(""Thread {} spilling sort data of {} to disk ({} {} so far)"", Thread.currentThread().getId(), Utils.bytesToString(getMemoryUsage()), spills.size(), spills.size() > 1 ? "" times"" : "" time"");
    writeSortedFile(false);
    final long spillSize = freeMemory();
    inMemSorter.reset();
    // Reset the in-memory sorter's pointer array only after freeing up the memory pages holding the
    // records. Otherwise, if the task is over allocated memory, then without freeing the memory
    // pages, we might not be able to get memory for the pointer array.
    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);
    return spillSize;
} 
{
    updatePeakMemoryUsed();
    return peakMemoryUsedBytes;
} 
{
    freeMemory();
    if (inMemSorter != null) {
        inMemSorter.free();
        inMemSorter = null;
    }
    for (SpillInfo spill : spills) {
        if (spill.file.exists() && !spill.file.delete()) {
            logger.error(""Unable to delete spill file {}"", spill.file.getPath());
        }
    }
} 
{
    assert (inMemSorter != null);
    if (!inMemSorter.hasSpaceForAnotherRecord()) {
        long used = inMemSorter.getMemoryUsage();
        LongArray array;
        try {
            // could trigger spilling
            array = allocateArray(used / 8 * 2);
        } catch (TooLargePageException e) {
            // The pointer array is too big to fix in a single page, spill.
            spill();
            return;
        } catch (SparkOutOfMemoryError e) {
            // should have trigger spilling
            if (!inMemSorter.hasSpaceForAnotherRecord()) {
                logger.error(""Unable to grow the pointer array"");
                throw e;
            }
            return;
        }
        // check if spilling is triggered or not
        if (inMemSorter.hasSpaceForAnotherRecord()) {
            freeArray(array);
        } else {
            inMemSorter.expandPointerArray(array);
        }
    }
} 
{
    if (currentPage == null || pageCursor + required > currentPage.getBaseOffset() + currentPage.size()) {
        // TODO: try to find space in previous pages
        currentPage = allocatePage(required);
        pageCursor = currentPage.getBaseOffset();
        allocatedPages.add(currentPage);
    }
} 
{
    // for tests
    assert (inMemSorter != null);
    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {
        logger.info(""Spilling data because number of spilledRecords crossed the threshold "" + numElementsForSpillThreshold);
        spill();
    }
    growPointerArrayIfNecessary();
    final int uaoSize = UnsafeAlignedOffset.getUaoSize();
    // Need 4 or 8 bytes to store the record length.
    final int required = length + uaoSize;
    acquireNewPageIfNecessary(required);
    assert (currentPage != null);
    final Object base = currentPage.getBaseObject();
    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);
    UnsafeAlignedOffset.putSize(base, pageCursor, length);
    pageCursor += uaoSize;
    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);
    pageCursor += length;
    inMemSorter.insertRecord(recordAddress, partitionId);
} 
{
    if (inMemSorter != null) {
        // Do not count the final file towards the spill count.
        writeSortedFile(true);
        freeMemory();
        inMemSorter.free();
        inMemSorter = null;
    }
    return spills.toArray(new SpillInfo[spills.size()]);
} 
",,,"/**
 * Force this sorter to spill when there are this many elements in memory.
 */
 
/**
 * The buffer size to use when writing spills using DiskBlockObjectWriter
 */
 
/**
 * The buffer size to use when writing the sorted records to an on-disk file
 */
 
/**
 * Memory pages that hold the records being sorted. The pages in this list are freed when
 * spilling, although in principle we could recycle these pages across spills (on the other hand,
 * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager
 * itself).
 */
 
/**
 * Peak memory used by this sorter so far, in bytes. *
 */
 
","Field numElementsForSpillThreshold
Field fileBufferSizeBytes
Field diskWriteBufferSize
Field allocatedPages
Field peakMemoryUsedBytes
",14,1008
ShuffleInMemorySorter.java,58,118,0.4915254237288136,"
   * An iterator-like class that's used instead of Java's Iterator in order to facilitate inlining.
   ","/**
 * Inserts a record to be sorted.
 *
 * @param recordPointer a pointer to the record, encoded by the task memory manager. Due to
 *                      certain pointer compression techniques used by the sorter, the sort can
 *                      only operate on pointers that point to locations in the first
 *                      {@link PackedRecordPointer#MAXIMUM_PAGE_SIZE_BYTES} bytes of a data page.
 * @param partitionId the partition id, which must be less than or equal to
 *                    {@link PackedRecordPointer#MAXIMUM_PARTITION_ID}.
 */
 
/**
 * Return an iterator over record pointers in sorted order.
 */
 
","{
    if (!hasSpaceForAnotherRecord()) {
        throw new IllegalStateException(""There is no space for new record"");
    }
    array.set(pos, PackedRecordPointer.packPointer(recordPointer, partitionId));
    pos++;
} 
{
    int offset = 0;
    if (useRadixSort) {
        offset = RadixSort.sort(array, pos, PackedRecordPointer.PARTITION_ID_START_BYTE_INDEX, PackedRecordPointer.PARTITION_ID_END_BYTE_INDEX, false, false);
    } else {
        MemoryBlock unused = new MemoryBlock(array.getBaseObject(), array.getBaseOffset() + pos * 8L, (array.size() - pos) * 8L);
        LongArray buffer = new LongArray(unused);
        Sorter<PackedRecordPointer, LongArray> sorter = new Sorter<>(new ShuffleSortDataFormat(buffer));
        sorter.sort(array, 0, pos, SORT_COMPARATOR);
    }
    return new ShuffleSorterIterator(pos, array, offset);
} 
",,,"/**
 * An array of record pointers and partition ids that have been encoded by
 * {@link PackedRecordPointer}. The sort operates on this array instead of directly manipulating
 * records.
 *
 * Only part of the array will be used to store the pointers, the rest part is preserved as
 * temporary buffer for sorting.
 */
 
/**
 * Whether to use radix sort for sorting in-memory partition ids. Radix sort is much faster
 * but requires additional memory to be reserved memory as pointers are added.
 */
 
/**
 * The position in the pointer array where new records can be inserted.
 */
 
/**
 * How many records could be inserted, because part of the array should be left for sorting.
 */
 
","Field array
Field useRadixSort
Field pos
Field usableCapacity
",1,101
PackedRecordPointer.java,55,30,1.8333333333333333,"
 * Wrapper around an 8-byte word that holds a 24-bit partition number and 40-bit record pointer.
 * <p>
 * Within the long, the data is laid out as follows:
 * <pre>
 *   [24 bit partition number][13 bit memory page number][27 bit offset in page]
 * </pre>
 * This implies that the maximum addressable page size is 2^27 bits = 128 megabytes, assuming that
 * our offsets in pages are not 8-byte-word-aligned. Since we have 2^13 pages (based off the
 * 13-bit page numbers assigned by {@link org.apache.spark.memory.TaskMemoryManager}), this
 * implies that we can address 2^13 * 128 megabytes = 1 terabyte of RAM per task.
 * <p>
 * Assuming word-alignment would allow for a 1 gigabyte maximum page size, but we leave this
 * optimization to future work as it will require more careful design to ensure that addresses are
 * properly aligned (e.g. by padding records).
 ","/**
 * Pack a record address and partition id into a single word.
 *
 * @param recordPointer a record pointer encoded by TaskMemoryManager.
 * @param partitionId a shuffle partition id (maximum value of 2^24).
 * @return a packed pointer that can be decoded using the {@link PackedRecordPointer} class.
 */
 
","{
    assert (partitionId <= MAXIMUM_PARTITION_ID);
    // Note that without word alignment we can address 2^27 bytes = 128 megabytes per page.
    // Also note that this relies on some internals of how TaskMemoryManager encodes its addresses.
    final long pageNumber = (recordPointer & MASK_LONG_UPPER_13_BITS) >>> 24;
    final long compressedAddress = pageNumber | (recordPointer & MASK_LONG_LOWER_27_BITS);
    return (((long) partitionId) << 40) | compressedAddress;
} 
",,,"/**
 * The index of the first byte of the partition id, counting from the least significant byte.
 */
 
/**
 * The index of the last byte of the partition id, counting from the least significant byte.
 */
 
/**
 * Bit mask for the lower 40 bits of a long.
 */
 
/**
 * Bit mask for the upper 24 bits of a long
 */
 
/**
 * Bit mask for the lower 27 bits of a long.
 */
 
/**
 * Bit mask for the lower 51 bits of a long.
 */
 
/**
 * Bit mask for the upper 13 bits of a long
 */
 
","Field PARTITION_ID_START_BYTE_INDEX
Field PARTITION_ID_END_BYTE_INDEX
Field MASK_LONG_LOWER_40_BITS
Field MASK_LONG_UPPER_24_BITS
Field MASK_LONG_LOWER_27_BITS
Field MASK_LONG_LOWER_51_BITS
Field MASK_LONG_UPPER_13_BITS
",14,841
ShuffleExecutorComponents.java,16,58,0.27586206896551724,"
 * :: Private ::
 * An interface for building shuffle support for Executors.
 *
 * @since 3.0.0
 ","/**
 * Called once per executor to bootstrap this module with state that is specific to
 * that executor, specifically the application ID and executor ID.
 *
 * @param appId The Spark application id
 * @param execId The unique identifier of the executor being initialized
 * @param extraConfigs Extra configs that were returned by
 *                     {@link ShuffleDriverComponents#initializeApplication()}
 */
 
/**
 * Called once per map task to create a writer that will be responsible for persisting all the
 * partitioned bytes written by that map task.
 *
 * @param shuffleId Unique identifier for the shuffle the map task is a part of
 * @param mapTaskId An ID of the map task. The ID is unique within this Spark application.
 * @param numPartitions The number of partitions that will be written by the map task. Some of
 *                      these partitions may be empty.
 */
 
/**
 * An optional extension for creating a map output writer that can optimize the transfer of a
 * single partition file, as the entire result of a map task, to the backing store.
 * <p>
 * Most implementations should return the default {@link Optional#empty()} to indicate that
 * they do not support this optimization. This primarily is for backwards-compatibility in
 * preserving an optimization in the local disk shuffle storage implementation.
 *
 * @param shuffleId Unique identifier for the shuffle the map task is a part of
 * @param mapId An ID of the map task. The ID is unique within this Spark application.
 */
 
","initializeExecutor 
createMapOutputWriter 
{
    return Optional.empty();
} 
",,,,,4,89
ShufflePartitionWriter.java,78,13,6.0,"
 * :: Private ::
 * An interface for opening streams to persist partition bytes to a backing data store.
 * <p>
 * This writer stores bytes for one (mapper, reducer) pair, corresponding to one shuffle
 * block.
 *
 * @since 3.0.0
 ","/**
 * Open and return an {@link OutputStream} that can write bytes to the underlying
 * data store.
 * <p>
 * This method will only be called once on this partition writer in the map task, to write the
 * bytes to the partition. The output stream will only be used to write the bytes for this
 * partition. The map task closes this output stream upon writing all the bytes for this
 * block, or if the write fails for any reason.
 * <p>
 * Implementations that intend on combining the bytes for all the partitions written by this
 * map task should reuse the same OutputStream instance across all the partition writers provided
 * by the parent {@link ShuffleMapOutputWriter}. If one does so, ensure that
 * {@link OutputStream#close()} does not close the resource, since it will be reused across
 * partition writes. The underlying resources should be cleaned up in
 * {@link ShuffleMapOutputWriter#commitAllPartitions()} and
 * {@link ShuffleMapOutputWriter#abort(Throwable)}.
 */
 
/**
 * Opens and returns a {@link WritableByteChannelWrapper} for transferring bytes from
 * input byte channels to the underlying shuffle data store.
 * <p>
 * This method will only be called once on this partition writer in the map task, to write the
 * bytes to the partition. The channel will only be used to write the bytes for this
 * partition. The map task closes this channel upon writing all the bytes for this
 * block, or if the write fails for any reason.
 * <p>
 * Implementations that intend on combining the bytes for all the partitions written by this
 * map task should reuse the same channel instance across all the partition writers provided
 * by the parent {@link ShuffleMapOutputWriter}. If one does so, ensure that
 * {@link WritableByteChannelWrapper#close()} does not close the resource, since the channel
 * will be reused across partition writes. The underlying resources should be cleaned up in
 * {@link ShuffleMapOutputWriter#commitAllPartitions()} and
 * {@link ShuffleMapOutputWriter#abort(Throwable)}.
 * <p>
 * This method is primarily for advanced optimizations where bytes can be copied from the input
 * spill files to the output channel without copying data into memory. If such optimizations are
 * not supported, the implementation should return {@link Optional#empty()}. By default, the
 * implementation returns {@link Optional#empty()}.
 * <p>
 * Note that the returned {@link WritableByteChannelWrapper} itself is closed, but not the
 * underlying channel that is returned by {@link WritableByteChannelWrapper#channel()}. Ensure
 * that the underlying channel is cleaned up in {@link WritableByteChannelWrapper#close()},
 * {@link ShuffleMapOutputWriter#commitAllPartitions()}, or
 * {@link ShuffleMapOutputWriter#abort(Throwable)}.
 */
 
/**
 * Returns the number of bytes written either by this writer's output stream opened by
 * {@link #openStream()} or the byte channel opened by {@link #openChannelWrapper()}.
 * <p>
 * This can be different from the number of bytes given by the caller. For example, the
 * stream might compress or encrypt the bytes before persisting the data to the backing
 * data store.
 */
 
","openStream 
{
    return Optional.empty();
} 
getNumBytesWritten 
",,,,,7,217
ShuffleMapOutputWriter.java,28,214,0.1308411214953271,"
 * :: Private ::
 * A top-level writer that returns child writers for persisting the output of a map task,
 * and then commits all of the writes as one atomic operation.
 *
 * @since 3.0.0
 ","/**
 * Creates a writer that can open an output stream to persist bytes targeted for a given reduce
 * partition id.
 * <p>
 * The chunk corresponds to bytes in the given reduce partition. This will not be called twice
 * for the same partition within any given map task. The partition identifier will be in the
 * range of precisely 0 (inclusive) to numPartitions (exclusive), where numPartitions was
 * provided upon the creation of this map output writer via
 * {@link ShuffleExecutorComponents#createMapOutputWriter(int, long, int)}.
 * <p>
 * Calls to this method will be invoked with monotonically increasing reducePartitionIds; each
 * call to this method will be called with a reducePartitionId that is strictly greater than
 * the reducePartitionIds given to any previous call to this method. This method is not
 * guaranteed to be called for every partition id in the above described range. In particular,
 * no guarantees are made as to whether or not this method will be called for empty partitions.
 */
 
/**
 * Commits the writes done by all partition writers returned by all calls to this object's
 * {@link #getPartitionWriter(int)}, and returns the number of bytes written for each
 * partition.
 * <p>
 * This should ensure that the writes conducted by this module's partition writers are
 * available to downstream reduce tasks. If this method throws any exception, this module's
 * {@link #abort(Throwable)} method will be invoked before propagating the exception.
 * <p>
 * This can also close any resources and clean up temporary state if necessary.
 * <p>
 * The returned array should contain, for each partition from (0) to (numPartitions - 1), the
 * number of bytes written by the partition writer for that partition id.
 */
 
/**
 * Abort all of the writes done by any writers returned by {@link #getPartitionWriter(int)}.
 * <p>
 * This should invalidate the results of writing bytes. This can also close any resources and
 * clean up temporary state if necessary.
 */
 
","getPartitionWriter 
commitAllPartitions 
abort 
",,,,,5,180
SingleSpillShuffleMapOutputWriter.java,23,8,2.875,"
 * Optional extension for partition writing that is optimized for transferring a single
 * file to the backing store.
 ","/**
 * Transfer a file that contains the bytes of all the partitions written by this map task.
 */
 
","transferMapSpillFile 
",,,,,2,115
WritableByteChannelWrapper.java,29,8,3.625,"
 * :: Private ::
 *
 * A thin wrapper around a {@link WritableByteChannel}.
 * <p>
 * This is primarily provided for the local disk shuffle implementation to provide a
 * {@link java.nio.channels.FileChannel} that keeps the channel open across partition writes.
 *
 * @since 3.0.0
 ","/**
 * The underlying channel to write bytes into.
 */
 
","channel 
",,,,,8,266
ShuffleDriverComponents.java,17,24,0.7083333333333334,"
 * :: Private ::
 * An interface for building shuffle support modules for the Driver.
 ","/**
 * Called once in the driver to bootstrap this module that is specific to this application.
 * This method is called before submitting executor requests to the cluster manager.
 *
 * This method should prepare the module with its shuffle components i.e. registering against
 * an external file servers or shuffle services, or creating tables in a shuffle
 * storage data database.
 *
 * @return additional SparkConf settings necessary for initializing the executor components.
 * This would include configurations that cannot be statically set on the application, like
 * the host:port of external services for shuffle storage.
 */
 
/**
 * Called once at the end of the Spark application to clean up any existing shuffle state.
 */
 
/**
 * Called once per shuffle id when the shuffle id is first generated for a shuffle stage.
 *
 * @param shuffleId The unique identifier for the shuffle stage.
 */
 
/**
 * Removes shuffle data associated with the given shuffle.
 *
 * @param shuffleId The unique identifier for the shuffle stage.
 * @param blocking Whether this call should block on the deletion of the data.
 */
 
","initializeApplication 
cleanupApplication 
{
} 
{
} 
",,,,,2,83
ShuffleDataIO.java,20,19,1.0526315789473684,"
 * :: Private ::
 * An interface for plugging in modules for storing and reading temporary shuffle data.
 * <p>
 * This is the root of a plugin system for storing shuffle bytes to arbitrary storage
 * backends in the sort-based shuffle algorithm implemented by the
 * {@link org.apache.spark.shuffle.sort.SortShuffleManager}. If another shuffle algorithm is
 * needed instead of sort-based shuffle, one should implement
 * {@link org.apache.spark.shuffle.ShuffleManager} instead.
 * <p>
 * A single instance of this module is loaded per process in the Spark application.
 * The default implementation reads and writes shuffle data from the local disks of
 * the executor, and is the implementation of shuffle file storage that has remained
 * consistent throughout most of Spark's history.
 * <p>
 * Alternative implementations of shuffle data storage can be loaded via setting
 * <code>spark.shuffle.sort.io.plugin.class</code>.
 * @since 3.0.0
 ","/**
 * Called once on executor processes to bootstrap the shuffle data storage modules that
 * are only invoked on the executors.
 */
 
/**
 * Called once on driver process to bootstrap the shuffle metadata modules that
 * are maintained by the driver.
 */
 
","executor 
driver 
",,,,,17,913
ExecutorPluginContext.java,19,25,0.76,"
 * Encapsulates information about the executor when initializing {@link ExecutorPlugin} instances.
 ",,,,,,,1,98
DummySerializerInstance.java,24,54,0.4444444444444444,"
 * Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.
 * Our shuffle write path doesn't actually use this serializer (since we end up calling the
 * `write() OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work
 * around this, we pass a dummy no-op serializer.
 ",,,,,,,4,331
TooLargePageException.java,16,6,2.6666666666666665,,,,,,,,1,0
TaskMemoryManager.java,161,264,0.6098484848484849,"
 * Manages the memory allocated by an individual task.
 * <p>
 * Most of the complexity in this class deals with encoding of off-heap addresses into 64-bit longs.
 * In off-heap mode, memory can be directly addressed with 64-bit longs. In on-heap mode, memory is
 * addressed by the combination of a base Object reference and a 64-bit offset within that object.
 * This is a problem when we want to store pointers to data structures inside of other structures,
 * such as record pointers inside hashmaps or sorting buffers. Even if we decided to use 128 bits
 * to address memory, we can't just store the address of the base object since it's not guaranteed
 * to remain stable as the heap gets reorganized due to GC.
 * <p>
 * Instead, we use the following approach to encode record pointers in 64-bit longs: for off-heap
 * mode, just store the raw address, and for on-heap mode use the upper 13 bits of the address to
 * store a ""page number"" and the lower 51 bits to store an offset within this page. These page
 * numbers are used to index into a ""page table"" array inside of the MemoryManager in order to
 * retrieve the base object.
 * <p>
 * This allows us to address 8192 pages. In on-heap mode, the maximum page size is limited by the
 * maximum size of a long[] array, allowing us to address 8192 * (2^31 - 1) * 8 bytes, which is
 * approximately 140 terabytes of memory.
 ","/**
 * Acquire N bytes of memory for a consumer. If there is no enough memory, it will call
 * spill() of consumers to release more memory.
 *
 * @return number of bytes successfully granted (<= N).
 */
 
/**
 * Release N bytes of execution memory for a MemoryConsumer.
 */
 
/**
 * Dump the memory usage of all consumers.
 */
 
/**
 * Return the page size in bytes.
 */
 
/**
 * Allocate a block of memory that will be tracked in the MemoryManager's page table; this is
 * intended for allocating large blocks of Tungsten memory that will be shared between operators.
 *
 * Returns `null` if there was not enough memory to allocate the page. May return a page that
 * contains fewer bytes than requested, so callers should verify the size of returned pages.
 *
 * @throws TooLargePageException
 */
 
/**
 * Free a block of memory allocated via {@link TaskMemoryManager#allocatePage}.
 */
 
/**
 * Given a memory page and offset within that page, encode this address into a 64-bit long.
 * This address will remain valid as long as the corresponding page has not been freed.
 *
 * @param page a data page allocated by {@link TaskMemoryManager#allocatePage}/
 * @param offsetInPage an offset in this page which incorporates the base offset. In other words,
 *                     this should be the value that you would pass as the base offset into an
 *                     UNSAFE call (e.g. page.baseOffset() + something).
 * @return an encoded page address.
 */
 
/**
 * Get the page associated with an address encoded by
 * {@link TaskMemoryManager#encodePageNumberAndOffset(MemoryBlock, long)}
 */
 
/**
 * Get the offset associated with an address encoded by
 * {@link TaskMemoryManager#encodePageNumberAndOffset(MemoryBlock, long)}
 */
 
/**
 * Clean up all allocated memory and pages. Returns the number of bytes freed. A non-zero return
 * value can be used to detect memory leaks.
 */
 
/**
 * Returns the memory consumption, in bytes, for the current task.
 */
 
/**
 * Returns Tungsten memory mode
 */
 
","{
    assert (required >= 0);
    assert (consumer != null);
    MemoryMode mode = consumer.getMode();
    // If we are allocating Tungsten pages off-heap and receive a request to allocate on-heap
    // memory here, then it may not make sense to spill since that would only end up freeing
    // off-heap memory. This is subject to change, though, so it may be risky to make this
    // optimization now in case we forget to undo it late when making changes.
    synchronized (this) {
        long got = memoryManager.acquireExecutionMemory(required, taskAttemptId, mode);
        // Try to release memory from other consumers first, then we can reduce the frequency of
        // spilling, avoid to have too many spilled files.
        if (got < required) {
            // Call spill() on other consumers to release memory
            // Sort the consumers according their memory usage. So we avoid spilling the same consumer
            // which is just spilled in last few times and re-spilling on it will produce many small
            // spill files.
            TreeMap<Long, List<MemoryConsumer>> sortedConsumers = new TreeMap<>();
            for (MemoryConsumer c : consumers) {
                if (c != consumer && c.getUsed() > 0 && c.getMode() == mode) {
                    long key = c.getUsed();
                    List<MemoryConsumer> list = sortedConsumers.computeIfAbsent(key, k -> new ArrayList<>(1));
                    list.add(c);
                }
            }
            while (!sortedConsumers.isEmpty()) {
                // Get the consumer using the least memory more than the remaining required memory.
                Map.Entry<Long, List<MemoryConsumer>> currentEntry = sortedConsumers.ceilingEntry(required - got);
                // No consumer has used memory more than the remaining required memory.
                // Get the consumer of largest used memory.
                if (currentEntry == null) {
                    currentEntry = sortedConsumers.lastEntry();
                }
                List<MemoryConsumer> cList = currentEntry.getValue();
                MemoryConsumer c = cList.get(cList.size() - 1);
                try {
                    long released = c.spill(required - got, consumer);
                    if (released > 0) {
                        logger.debug(""Task {} released {} from {} for {}"", taskAttemptId, Utils.bytesToString(released), c, consumer);
                        got += memoryManager.acquireExecutionMemory(required - got, taskAttemptId, mode);
                        if (got >= required) {
                            break;
                        }
                    } else {
                        cList.remove(cList.size() - 1);
                        if (cList.isEmpty()) {
                            sortedConsumers.remove(currentEntry.getKey());
                        }
                    }
                } catch (ClosedByInterruptException e) {
                    // This called by user to kill a task (e.g: speculative task).
                    logger.error(""error while calling spill() on "" + c, e);
                    throw new RuntimeException(e.getMessage());
                } catch (IOException e) {
                    logger.error(""error while calling spill() on "" + c, e);
                    // checkstyle.off: RegexpSinglelineJava
                    throw new SparkOutOfMemoryError(""error while calling spill() on "" + c + "" : "" + e.getMessage());
                // checkstyle.on: RegexpSinglelineJava
                }
            }
        }
        // call spill() on itself
        if (got < required) {
            try {
                long released = consumer.spill(required - got, consumer);
                if (released > 0) {
                    logger.debug(""Task {} released {} from itself ({})"", taskAttemptId, Utils.bytesToString(released), consumer);
                    got += memoryManager.acquireExecutionMemory(required - got, taskAttemptId, mode);
                }
            } catch (ClosedByInterruptException e) {
                // This called by user to kill a task (e.g: speculative task).
                logger.error(""error while calling spill() on "" + consumer, e);
                throw new RuntimeException(e.getMessage());
            } catch (IOException e) {
                logger.error(""error while calling spill() on "" + consumer, e);
                // checkstyle.off: RegexpSinglelineJava
                throw new SparkOutOfMemoryError(""error while calling spill() on "" + consumer + "" : "" + e.getMessage());
            // checkstyle.on: RegexpSinglelineJava
            }
        }
        consumers.add(consumer);
        logger.debug(""Task {} acquired {} for {}"", taskAttemptId, Utils.bytesToString(got), consumer);
        return got;
    }
} 
{
    logger.debug(""Task {} release {} from {}"", taskAttemptId, Utils.bytesToString(size), consumer);
    memoryManager.releaseExecutionMemory(size, taskAttemptId, consumer.getMode());
} 
{
    logger.info(""Memory used in task "" + taskAttemptId);
    synchronized (this) {
        long memoryAccountedForByConsumers = 0;
        for (MemoryConsumer c : consumers) {
            long totalMemUsage = c.getUsed();
            memoryAccountedForByConsumers += totalMemUsage;
            if (totalMemUsage > 0) {
                logger.info(""Acquired by "" + c + "": "" + Utils.bytesToString(totalMemUsage));
            }
        }
        long memoryNotAccountedFor = memoryManager.getExecutionMemoryUsageForTask(taskAttemptId) - memoryAccountedForByConsumers;
        logger.info(""{} bytes of memory were used by task {} but are not associated with specific consumers"", memoryNotAccountedFor, taskAttemptId);
        logger.info(""{} bytes of memory are used for execution and {} bytes of memory are used for storage"", memoryManager.executionMemoryUsed(), memoryManager.storageMemoryUsed());
    }
} 
{
    return memoryManager.pageSizeBytes();
} 
{
    assert (consumer != null);
    assert (consumer.getMode() == tungstenMemoryMode);
    if (size > MAXIMUM_PAGE_SIZE_BYTES) {
        throw new TooLargePageException(size);
    }
    long acquired = acquireExecutionMemory(size, consumer);
    if (acquired <= 0) {
        return null;
    }
    final int pageNumber;
    synchronized (this) {
        pageNumber = allocatedPages.nextClearBit(0);
        if (pageNumber >= PAGE_TABLE_SIZE) {
            releaseExecutionMemory(acquired, consumer);
            throw new IllegalStateException(""Have already allocated a maximum of "" + PAGE_TABLE_SIZE + "" pages"");
        }
        allocatedPages.set(pageNumber);
    }
    MemoryBlock page = null;
    try {
        page = memoryManager.tungstenMemoryAllocator().allocate(acquired);
    } catch (OutOfMemoryError e) {
        logger.warn(""Failed to allocate a page ({} bytes), try again."", acquired);
        // there is no enough memory actually, it means the actual free memory is smaller than
        // MemoryManager thought, we should keep the acquired memory.
        synchronized (this) {
            acquiredButNotUsed += acquired;
            allocatedPages.clear(pageNumber);
        }
        // this could trigger spilling to free some pages.
        return allocatePage(size, consumer);
    }
    page.pageNumber = pageNumber;
    pageTable[pageNumber] = page;
    if (logger.isTraceEnabled()) {
        logger.trace(""Allocate page number {} ({} bytes)"", pageNumber, acquired);
    }
    return page;
} 
{
    assert (page.pageNumber != MemoryBlock.NO_PAGE_NUMBER) : ""Called freePage() on memory that wasn't allocated with allocatePage()"";
    assert (page.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) : ""Called freePage() on a memory block that has already been freed"";
    assert (page.pageNumber != MemoryBlock.FREED_IN_TMM_PAGE_NUMBER) : ""Called freePage() on a memory block that has already been freed"";
    assert (allocatedPages.get(page.pageNumber));
    pageTable[page.pageNumber] = null;
    synchronized (this) {
        allocatedPages.clear(page.pageNumber);
    }
    if (logger.isTraceEnabled()) {
        logger.trace(""Freed page number {} ({} bytes)"", page.pageNumber, page.size());
    }
    long pageSize = page.size();
    // Clear the page number before passing the block to the MemoryAllocator's free().
    // Doing this allows the MemoryAllocator to detect when a TaskMemoryManager-managed
    // page has been inappropriately directly freed without calling TMM.freePage().
    page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;
    memoryManager.tungstenMemoryAllocator().free(page);
    releaseExecutionMemory(pageSize, consumer);
} 
{
    if (tungstenMemoryMode == MemoryMode.OFF_HEAP) {
        // In off-heap mode, an offset is an absolute address that may require a full 64 bits to
        // encode. Due to our page size limitation, though, we can convert this into an offset that's
        // relative to the page's base offset; this relative offset will fit in 51 bits.
        offsetInPage -= page.getBaseOffset();
    }
    return encodePageNumberAndOffset(page.pageNumber, offsetInPage);
} 
{
    if (tungstenMemoryMode == MemoryMode.ON_HEAP) {
        final int pageNumber = decodePageNumber(pagePlusOffsetAddress);
        assert (pageNumber >= 0 && pageNumber < PAGE_TABLE_SIZE);
        final MemoryBlock page = pageTable[pageNumber];
        assert (page != null);
        assert (page.getBaseObject() != null);
        return page.getBaseObject();
    } else {
        return null;
    }
} 
{
    final long offsetInPage = decodeOffset(pagePlusOffsetAddress);
    if (tungstenMemoryMode == MemoryMode.ON_HEAP) {
        return offsetInPage;
    } else {
        // In off-heap mode, an offset is an absolute address. In encodePageNumberAndOffset, we
        // converted the absolute address into a relative address. Here, we invert that operation:
        final int pageNumber = decodePageNumber(pagePlusOffsetAddress);
        assert (pageNumber >= 0 && pageNumber < PAGE_TABLE_SIZE);
        final MemoryBlock page = pageTable[pageNumber];
        assert (page != null);
        return page.getBaseOffset() + offsetInPage;
    }
} 
{
    synchronized (this) {
        for (MemoryConsumer c : consumers) {
            if (c != null && c.getUsed() > 0) {
                // In case of failed task, it's normal to see leaked memory
                logger.debug(""unreleased "" + Utils.bytesToString(c.getUsed()) + "" memory from "" + c);
            }
        }
        consumers.clear();
        for (MemoryBlock page : pageTable) {
            if (page != null) {
                logger.debug(""unreleased page: "" + page + "" in task "" + taskAttemptId);
                page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;
                memoryManager.tungstenMemoryAllocator().free(page);
            }
        }
        Arrays.fill(pageTable, null);
    }
    // release the memory that is not used by any consumer (acquired for pages in tungsten mode).
    memoryManager.releaseExecutionMemory(acquiredButNotUsed, taskAttemptId, tungstenMemoryMode);
    return memoryManager.releaseAllExecutionMemoryForTask(taskAttemptId);
} 
{
    return memoryManager.getExecutionMemoryUsageForTask(taskAttemptId);
} 
{
    return tungstenMemoryMode;
} 
","/**
 * Construct a new TaskMemoryManager.
 */
 
","{
    this.tungstenMemoryMode = memoryManager.tungstenMemoryMode();
    this.memoryManager = memoryManager;
    this.taskAttemptId = taskAttemptId;
    this.consumers = new HashSet<>();
} 
","/**
 * The number of bits used to address the page table.
 */
 
/**
 * The number of bits used to encode offsets in data pages.
 */
 
/**
 * The number of entries in the page table.
 */
 
/**
 * Maximum supported data page size (in bytes). In principle, the maximum addressable page size is
 * (1L &lt;&lt; OFFSET_BITS) bytes, which is 2+ petabytes. However, the on-heap allocator's
 * maximum page size is limited by the maximum amount of data that can be stored in a long[]
 * array, which is (2^31 - 1) * 8 bytes (or about 17 gigabytes). Therefore, we cap this at 17
 * gigabytes.
 */
 
/**
 * Bit mask for the lower 51 bits of a long.
 */
 
/**
 * Similar to an operating system's page table, this array maps page numbers into base object
 * pointers, allowing us to translate between the hashtable's internal 64-bit address
 * representation and the baseObject+offset representation which we use to support both on- and
 * off-heap addresses. When using an off-heap allocator, every entry in this map will be `null`.
 * When using an on-heap allocator, the entries in this map will point to pages' base objects.
 * Entries are added to this map as new data pages are allocated.
 */
 
/**
 * Bitmap for tracking free pages.
 */
 
/**
 * Tracks whether we're on-heap or off-heap. For off-heap, we short-circuit most of these methods
 * without doing any masking or lookups. Since this branching should be well-predicted by the JIT,
 * this extra layer of indirection / abstraction hopefully shouldn't be too expensive.
 */
 
/**
 * Tracks spillable memory consumers.
 */
 
/**
 * The amount of memory that is acquired but not used.
 */
 
","Field PAGE_NUMBER_BITS
Field OFFSET_BITS
Field PAGE_TABLE_SIZE
Field MAXIMUM_PAGE_SIZE_BYTES
Field MASK_LONG_LOWER_51_BITS
Field pageTable
Field allocatedPages
Field tungstenMemoryMode
Field consumers
Field acquiredButNotUsed
",19,1344
MemoryMode.java,16,7,2.2857142857142856,,,,,,,,1,0
SparkOutOfMemoryError.java,21,11,1.9090909090909092,"
 * This exception is thrown when a task can not acquire memory from the Memory manager.
 * Instead of throwing {@link OutOfMemoryError}, which kills the executor,
 * we should use throw this exception, which just kills the current task.
 ",,,,,,,3,232
MemoryConsumer.java,72,71,1.0140845070422535,"
 * A memory consumer of {@link TaskMemoryManager} that supports spilling.
 *
 * Note: this only supports allocation / spilling of Tungsten memory.
 ","/**
 * Returns the memory mode, {@link MemoryMode#ON_HEAP} or {@link MemoryMode#OFF_HEAP}.
 */
 
/**
 * Returns the size of used memory in bytes.
 */
 
/**
 * Force spill during building.
 */
 
/**
 * Spill some data to disk to release memory, which will be called by TaskMemoryManager
 * when there is not enough memory for the task.
 *
 * This should be implemented by subclass.
 *
 * Note: In order to avoid possible deadlock, should not call acquireMemory() from spill().
 *
 * Note: today, this only frees Tungsten-managed pages.
 *
 * @param size the amount of memory should be released
 * @param trigger the MemoryConsumer that trigger this spilling
 * @return the amount of released memory in bytes
 */
 
/**
 * Allocates a LongArray of `size`. Note that this method may throw `SparkOutOfMemoryError`
 * if Spark doesn't have enough memory for this allocation, or throw `TooLargePageException`
 * if this `LongArray` is too large to fit in a single page. The caller side should take care of
 * these two exceptions, or make sure the `size` is small enough that won't trigger exceptions.
 *
 * @throws SparkOutOfMemoryError
 * @throws TooLargePageException
 */
 
/**
 * Frees a LongArray.
 */
 
/**
 * Allocate a memory block with at least `required` bytes.
 *
 * @throws SparkOutOfMemoryError
 */
 
/**
 * Free a memory block.
 */
 
/**
 * Allocates memory of `size`.
 */
 
/**
 * Release N bytes of memory.
 */
 
","{
    return mode;
} 
{
    return used;
} 
{
    spill(Long.MAX_VALUE, this);
} 
spill 
{
    long required = size * 8L;
    MemoryBlock page = taskMemoryManager.allocatePage(required, this);
    if (page == null || page.size() < required) {
        throwOom(page, required);
    }
    used += required;
    return new LongArray(page);
} 
{
    freePage(array.memoryBlock());
} 
{
    MemoryBlock page = taskMemoryManager.allocatePage(Math.max(pageSize, required), this);
    if (page == null || page.size() < required) {
        throwOom(page, required);
    }
    used += page.size();
    return page;
} 
{
    used -= page.size();
    taskMemoryManager.freePage(page, this);
} 
{
    long granted = taskMemoryManager.acquireExecutionMemory(size, this);
    used += granted;
    return granted;
} 
{
    taskMemoryManager.releaseExecutionMemory(size, this);
    used -= size;
} 
",,,,,3,142
SparkExecutorInfo.java,22,12,1.8333333333333333,"
 * Exposes information about Spark Executors.
 *
 * This interface is not designed to be implemented outside of Spark.  We may add additional methods
 * which may break binary compatibility with outside implementations.
 ",,,,,,,4,213
TimSort.java,411,470,0.874468085106383,"
 * Based on TimSort.java from the Android Open Source Project
 *
 *  Copyright (C) 2008 The Android Open Source Project
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *       http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 |
 * A port of the Android TimSort class, which utilizes a ""stable, adaptive, iterative mergesort.""
 * See the method comment on sort() for more details.
 *
 * This has been kept in Java with the original style in order to match very closely with the
 * Android source code, and thus be easy to verify correctness. The class is package private. We put
 * a simple Scala wrapper {@link org.apache.spark.util.collection.Sorter}, which is available to
 * package org.apache.spark.
 *
 * The purpose of the port is to generalize the interface to the sort to accept input data formats
 * besides simple arrays where every element is sorted individually. For instance, the AppendOnlyMap
 * uses this to sort an Array with alternating elements of the form [key, value, key, value].
 * This generalization comes with minimal overhead -- see SortDataFormat for more information.
 *
 * We allow key reuse to prevent creating many key objects -- see SortDataFormat.
 *
 * @see org.apache.spark.util.collection.SortDataFormat
 * @see org.apache.spark.util.collection.Sorter
 ","/**
 * A stable, adaptive, iterative mergesort that requires far fewer than
 * n lg(n) comparisons when running on partially sorted arrays, while
 * offering performance comparable to a traditional mergesort when run
 * on random arrays.  Like all proper mergesorts, this sort is stable and
 * runs O(n log n) time (worst case).  In the worst case, this sort requires
 * temporary storage space for n/2 object references; in the best case,
 * it requires only a small constant amount of space.
 *
 * This implementation was adapted from Tim Peters's list sort for
 * Python, which is described in detail here:
 *
 *   http://svn.python.org/projects/python/trunk/Objects/listsort.txt
 *
 * Tim's C code may be found here:
 *
 *   http://svn.python.org/projects/python/trunk/Objects/listobject.c
 *
 * The underlying techniques are described in this paper (and may have
 * even earlier origins):
 *
 *  ""Optimistic Sorting and Information Theoretic Complexity""
 *  Peter McIlroy
 *  SODA (Fourth Annual ACM-SIAM Symposium on Discrete Algorithms),
 *  pp 467-474, Austin, Texas, 25-27 January 1993.
 *
 * While the API to this class consists solely of static methods, it is
 * (privately) instantiable; a TimSort instance holds the state of an ongoing
 * sort, assuming the input array is large enough to warrant the full-blown
 * TimSort. Small arrays are sorted in place, using a binary insertion sort.
 *
 * @author Josh Bloch
 */
 
/**
 * Sorts the specified portion of the specified array using a binary
 * insertion sort.  This is the best method for sorting small numbers
 * of elements.  It requires O(n log n) compares, but O(n^2) data
 * movement (worst case).
 *
 * If the initial part of the specified range is already sorted,
 * this method can take advantage of it: the method assumes that the
 * elements from index {@code lo}, inclusive, to {@code start},
 * exclusive are already sorted.
 *
 * @param a the array in which a range is to be sorted
 * @param lo the index of the first element in the range to be sorted
 * @param hi the index after the last element in the range to be sorted
 * @param start the index of the first element in the range that is
 *        not already known to be sorted ({@code lo <= start <= hi})
 * @param c comparator to used for the sort
 */
 
/**
 * Returns the length of the run beginning at the specified position in
 * the specified array and reverses the run if it is descending (ensuring
 * that the run will always be ascending when the method returns).
 *
 * A run is the longest ascending sequence with:
 *
 *    a[lo] <= a[lo + 1] <= a[lo + 2] <= ...
 *
 * or the longest descending sequence with:
 *
 *    a[lo] >  a[lo + 1] >  a[lo + 2] >  ...
 *
 * For its intended use in a stable mergesort, the strictness of the
 * definition of ""descending"" is needed so that the call can safely
 * reverse a descending sequence without violating stability.
 *
 * @param a the array in which a run is to be counted and possibly reversed
 * @param lo index of the first element in the run
 * @param hi index after the last element that may be contained in the run.
 *  It is required that {@code lo < hi}.
 * @param c the comparator to used for the sort
 * @return  the length of the run beginning at the specified position in
 *          the specified array
 */
 
/**
 * Reverse the specified range of the specified array.
 *
 * @param a the array in which a range is to be reversed
 * @param lo the index of the first element in the range to be reversed
 * @param hi the index after the last element in the range to be reversed
 */
 
/**
 * Returns the minimum acceptable run length for an array of the specified
 * length. Natural runs shorter than this will be extended with
 * {@link #binarySort}.
 *
 * Roughly speaking, the computation is:
 *
 *  If n < MIN_MERGE, return n (it's too small to bother with fancy stuff).
 *  Else if n is an exact power of 2, return MIN_MERGE/2.
 *  Else return an int k, MIN_MERGE/2 <= k <= MIN_MERGE, such that n/k
 *   is close to, but strictly less than, an exact power of 2.
 *
 * For the rationale, see listsort.txt.
 *
 * @param n the length of the array to be sorted
 * @return the length of the minimum run to be merged
 */
 
/**
 * Pushes the specified run onto the pending-run stack.
 *
 * @param runBase index of the first element in the run
 * @param runLen  the number of elements in the run
 */
 
/**
 * Examines the stack of runs waiting to be merged and merges adjacent runs
 * until the stack invariants are reestablished:
 *
 *     1. runLen[i - 3] > runLen[i - 2] + runLen[i - 1]
 *     2. runLen[i - 2] > runLen[i - 1]
 *
 * This method is called each time a new run is pushed onto the stack,
 * so the invariants are guaranteed to hold for i < stackSize upon
 * entry to the method.
 *
 * Thanks to Stijn de Gouw, Jurriaan Rot, Frank S. de Boer,
 * Richard Bubel and Reiner Hahnle, this is fixed with respect to
 * the analysis in ""On the Worst-Case Complexity of TimSort"" by
 * Nicolas Auger, Vincent Jug, Cyril Nicaud, and Carine Pivoteau.
 */
 
/**
 * Merges all runs on the stack until only one remains.  This method is
 * called once, to complete the sort.
 */
 
/**
 * Merges the two runs at stack indices i and i+1.  Run i must be
 * the penultimate or antepenultimate run on the stack.  In other words,
 * i must be equal to stackSize-2 or stackSize-3.
 *
 * @param i stack index of the first of the two runs to merge
 */
 
/**
 * Locates the position at which to insert the specified key into the
 * specified sorted range; if the range contains an element equal to key,
 * returns the index of the leftmost equal element.
 *
 * @param key the key whose insertion point to search for
 * @param a the array in which to search
 * @param base the index of the first element in the range
 * @param len the length of the range; must be > 0
 * @param hint the index at which to begin the search, 0 <= hint < n.
 *     The closer hint is to the result, the faster this method will run.
 * @param c the comparator used to order the range, and to search
 * @return the int k,  0 <= k <= n such that a[b + k - 1] < key <= a[b + k],
 *    pretending that a[b - 1] is minus infinity and a[b + n] is infinity.
 *    In other words, key belongs at index b + k; or in other words,
 *    the first k elements of a should precede key, and the last n - k
 *    should follow it.
 */
 
/**
 * Like gallopLeft, except that if the range contains an element equal to
 * key, gallopRight returns the index after the rightmost equal element.
 *
 * @param key the key whose insertion point to search for
 * @param a the array in which to search
 * @param base the index of the first element in the range
 * @param len the length of the range; must be > 0
 * @param hint the index at which to begin the search, 0 <= hint < n.
 *     The closer hint is to the result, the faster this method will run.
 * @param c the comparator used to order the range, and to search
 * @return the int k,  0 <= k <= n such that a[b + k - 1] <= key < a[b + k]
 */
 
/**
 * Merges two adjacent runs in place, in a stable fashion.  The first
 * element of the first run must be greater than the first element of the
 * second run (a[base1] > a[base2]), and the last element of the first run
 * (a[base1 + len1-1]) must be greater than all elements of the second run.
 *
 * For performance, this method should be called only when len1 <= len2;
 * its twin, mergeHi should be called if len1 >= len2.  (Either method
 * may be called if len1 == len2.)
 *
 * @param base1 index of first element in first run to be merged
 * @param len1  length of first run to be merged (must be > 0)
 * @param base2 index of first element in second run to be merged
 *        (must be aBase + aLen)
 * @param len2  length of second run to be merged (must be > 0)
 */
 
/**
 * Like mergeLo, except that this method should be called only if
 * len1 >= len2; mergeLo should be called if len1 <= len2.  (Either method
 * may be called if len1 == len2.)
 *
 * @param base1 index of first element in first run to be merged
 * @param len1  length of first run to be merged (must be > 0)
 * @param base2 index of first element in second run to be merged
 *        (must be aBase + aLen)
 * @param len2  length of second run to be merged (must be > 0)
 */
 
/**
 * Ensures that the external array tmp has at least the specified
 * number of elements, increasing its size if necessary.  The size
 * increases exponentially to ensure amortized linear time complexity.
 *
 * @param minCapacity the minimum required capacity of the tmp array
 * @return tmp, whether or not it grew
 */
 
","{
    assert c != null;
    int nRemaining = hi - lo;
    if (nRemaining < 2)
        // Arrays of size 0 and 1 are always sorted
        return;
    // If array is small, do a ""mini-TimSort"" with no merges
    if (nRemaining < MIN_MERGE) {
        int initRunLen = countRunAndMakeAscending(a, lo, hi, c);
        binarySort(a, lo, hi, lo + initRunLen, c);
        return;
    }
    /**
     * March over the array once, left to right, finding natural runs,
     * extending short natural runs to minRun elements, and merging runs
     * to maintain stack invariant.
     */
    SortState sortState = new SortState(a, c, hi - lo);
    int minRun = minRunLength(nRemaining);
    do {
        // Identify next run
        int runLen = countRunAndMakeAscending(a, lo, hi, c);
        // If run is short, extend to min(minRun, nRemaining)
        if (runLen < minRun) {
            int force = nRemaining <= minRun ? nRemaining : minRun;
            binarySort(a, lo, lo + force, lo + runLen, c);
            runLen = force;
        }
        // Push run onto pending-run stack, and maybe merge
        sortState.pushRun(lo, runLen);
        sortState.mergeCollapse();
        // Advance to find next run
        lo += runLen;
        nRemaining -= runLen;
    } while (nRemaining != 0);
    // Merge all remaining runs to complete sort
    assert lo == hi;
    sortState.mergeForceCollapse();
    assert sortState.stackSize == 1;
} 
{
    assert lo <= start && start <= hi;
    if (start == lo)
        start++;
    K key0 = s.newKey();
    K key1 = s.newKey();
    Buffer pivotStore = s.allocate(1);
    for (; start < hi; start++) {
        s.copyElement(a, start, pivotStore, 0);
        K pivot = s.getKey(pivotStore, 0, key0);
        // Set left (and right) to the index where a[start] (pivot) belongs
        int left = lo;
        int right = start;
        assert left <= right;
        /*
       * Invariants:
       *   pivot >= all in [lo, left).
       *   pivot <  all in [right, start).
       */
        while (left < right) {
            int mid = (left + right) >>> 1;
            if (c.compare(pivot, s.getKey(a, mid, key1)) < 0)
                right = mid;
            else
                left = mid + 1;
        }
        assert left == right;
        /*
       * The invariants still hold: pivot >= all in [lo, left) and
       * pivot < all in [left, start), so pivot belongs at left.  Note
       * that if there are elements equal to pivot, left points to the
       * first slot after them -- that's why this sort is stable.
       * Slide elements over to make room for pivot.
       */
        // The number of elements to move
        int n = start - left;
        // Switch is just an optimization for arraycopy in default case
        switch(n) {
            case 2:
                s.copyElement(a, left + 1, a, left + 2);
            case 1:
                s.copyElement(a, left, a, left + 1);
                break;
            default:
                s.copyRange(a, left, a, left + 1, n);
        }
        s.copyElement(pivotStore, 0, a, left);
    }
} 
{
    assert lo < hi;
    int runHi = lo + 1;
    if (runHi == hi)
        return 1;
    K key0 = s.newKey();
    K key1 = s.newKey();
    // Find end of run, and reverse range if descending
    if (c.compare(s.getKey(a, runHi++, key0), s.getKey(a, lo, key1)) < 0) {
        // Descending
        while (runHi < hi && c.compare(s.getKey(a, runHi, key0), s.getKey(a, runHi - 1, key1)) < 0) runHi++;
        reverseRange(a, lo, runHi);
    } else {
        // Ascending
        while (runHi < hi && c.compare(s.getKey(a, runHi, key0), s.getKey(a, runHi - 1, key1)) >= 0) runHi++;
    }
    return runHi - lo;
} 
{
    hi--;
    while (lo < hi) {
        s.swap(a, lo, hi);
        lo++;
        hi--;
    }
} 
{
    assert n >= 0;
    // Becomes 1 if any 1 bits are shifted off
    int r = 0;
    while (n >= MIN_MERGE) {
        r |= (n & 1);
        n >>= 1;
    }
    return n + r;
} 
{
    this.runBase[stackSize] = runBase;
    this.runLen[stackSize] = runLen;
    stackSize++;
} 
{
    while (stackSize > 1) {
        int n = stackSize - 2;
        if (n > 0 && runLen[n - 1] <= runLen[n] + runLen[n + 1] || n > 1 && runLen[n - 2] <= runLen[n] + runLen[n - 1]) {
            if (runLen[n - 1] < runLen[n + 1])
                n--;
        } else if (n < 0 || runLen[n] > runLen[n + 1]) {
            // Invariant is established
            break;
        }
        mergeAt(n);
    }
} 
{
    while (stackSize > 1) {
        int n = stackSize - 2;
        if (n > 0 && runLen[n - 1] < runLen[n + 1])
            n--;
        mergeAt(n);
    }
} 
{
    assert stackSize >= 2;
    assert i >= 0;
    assert i == stackSize - 2 || i == stackSize - 3;
    int base1 = runBase[i];
    int len1 = runLen[i];
    int base2 = runBase[i + 1];
    int len2 = runLen[i + 1];
    assert len1 > 0 && len2 > 0;
    assert base1 + len1 == base2;
    /*
       * Record the length of the combined runs; if i is the 3rd-last
       * run now, also slide over the last run (which isn't involved
       * in this merge).  The current run (i+1) goes away in any case.
       */
    runLen[i] = len1 + len2;
    if (i == stackSize - 3) {
        runBase[i + 1] = runBase[i + 2];
        runLen[i + 1] = runLen[i + 2];
    }
    stackSize--;
    K key0 = s.newKey();
    /*
       * Find where the first element of run2 goes in run1. Prior elements
       * in run1 can be ignored (because they're already in place).
       */
    int k = gallopRight(s.getKey(a, base2, key0), a, base1, len1, 0, c);
    assert k >= 0;
    base1 += k;
    len1 -= k;
    if (len1 == 0)
        return;
    /*
       * Find where the last element of run1 goes in run2. Subsequent elements
       * in run2 can be ignored (because they're already in place).
       */
    len2 = gallopLeft(s.getKey(a, base1 + len1 - 1, key0), a, base2, len2, len2 - 1, c);
    assert len2 >= 0;
    if (len2 == 0)
        return;
    // Merge remaining runs, using tmp array with min(len1, len2) elements
    if (len1 <= len2)
        mergeLo(base1, len1, base2, len2);
    else
        mergeHi(base1, len1, base2, len2);
} 
{
    assert len > 0 && hint >= 0 && hint < len;
    int lastOfs = 0;
    int ofs = 1;
    K key0 = s.newKey();
    if (c.compare(key, s.getKey(a, base + hint, key0)) > 0) {
        // Gallop right until a[base+hint+lastOfs] < key <= a[base+hint+ofs]
        int maxOfs = len - hint;
        while (ofs < maxOfs && c.compare(key, s.getKey(a, base + hint + ofs, key0)) > 0) {
            lastOfs = ofs;
            ofs = (ofs << 1) + 1;
            if (// int overflow
            ofs <= 0)
                ofs = maxOfs;
        }
        if (ofs > maxOfs)
            ofs = maxOfs;
        // Make offsets relative to base
        lastOfs += hint;
        ofs += hint;
    } else {
        // key <= a[base + hint]
        // Gallop left until a[base+hint-ofs] < key <= a[base+hint-lastOfs]
        final int maxOfs = hint + 1;
        while (ofs < maxOfs && c.compare(key, s.getKey(a, base + hint - ofs, key0)) <= 0) {
            lastOfs = ofs;
            ofs = (ofs << 1) + 1;
            if (// int overflow
            ofs <= 0)
                ofs = maxOfs;
        }
        if (ofs > maxOfs)
            ofs = maxOfs;
        // Make offsets relative to base
        int tmp = lastOfs;
        lastOfs = hint - ofs;
        ofs = hint - tmp;
    }
    assert -1 <= lastOfs && lastOfs < ofs && ofs <= len;
    /*
       * Now a[base+lastOfs] < key <= a[base+ofs], so key belongs somewhere
       * to the right of lastOfs but no farther right than ofs.  Do a binary
       * search, with invariant a[base + lastOfs - 1] < key <= a[base + ofs].
       */
    lastOfs++;
    while (lastOfs < ofs) {
        int m = lastOfs + ((ofs - lastOfs) >>> 1);
        if (c.compare(key, s.getKey(a, base + m, key0)) > 0)
            // a[base + m] < key
            lastOfs = m + 1;
        else
            // key <= a[base + m]
            ofs = m;
    }
    // so a[base + ofs - 1] < key <= a[base + ofs]
    assert lastOfs == ofs;
    return ofs;
} 
{
    assert len > 0 && hint >= 0 && hint < len;
    int ofs = 1;
    int lastOfs = 0;
    K key1 = s.newKey();
    if (c.compare(key, s.getKey(a, base + hint, key1)) < 0) {
        // Gallop left until a[b+hint - ofs] <= key < a[b+hint - lastOfs]
        int maxOfs = hint + 1;
        while (ofs < maxOfs && c.compare(key, s.getKey(a, base + hint - ofs, key1)) < 0) {
            lastOfs = ofs;
            ofs = (ofs << 1) + 1;
            if (// int overflow
            ofs <= 0)
                ofs = maxOfs;
        }
        if (ofs > maxOfs)
            ofs = maxOfs;
        // Make offsets relative to b
        int tmp = lastOfs;
        lastOfs = hint - ofs;
        ofs = hint - tmp;
    } else {
        // a[b + hint] <= key
        // Gallop right until a[b+hint + lastOfs] <= key < a[b+hint + ofs]
        int maxOfs = len - hint;
        while (ofs < maxOfs && c.compare(key, s.getKey(a, base + hint + ofs, key1)) >= 0) {
            lastOfs = ofs;
            ofs = (ofs << 1) + 1;
            if (// int overflow
            ofs <= 0)
                ofs = maxOfs;
        }
        if (ofs > maxOfs)
            ofs = maxOfs;
        // Make offsets relative to b
        lastOfs += hint;
        ofs += hint;
    }
    assert -1 <= lastOfs && lastOfs < ofs && ofs <= len;
    /*
       * Now a[b + lastOfs] <= key < a[b + ofs], so key belongs somewhere to
       * the right of lastOfs but no farther right than ofs.  Do a binary
       * search, with invariant a[b + lastOfs - 1] <= key < a[b + ofs].
       */
    lastOfs++;
    while (lastOfs < ofs) {
        int m = lastOfs + ((ofs - lastOfs) >>> 1);
        if (c.compare(key, s.getKey(a, base + m, key1)) < 0)
            // key < a[b + m]
            ofs = m;
        else
            // a[b + m] <= key
            lastOfs = m + 1;
    }
    // so a[b + ofs - 1] <= key < a[b + ofs]
    assert lastOfs == ofs;
    return ofs;
} 
{
    assert len1 > 0 && len2 > 0 && base1 + len1 == base2;
    // Copy first run into temp array
    // For performance
    Buffer a = this.a;
    Buffer tmp = ensureCapacity(len1);
    s.copyRange(a, base1, tmp, 0, len1);
    // Indexes into tmp array
    int cursor1 = 0;
    // Indexes int a
    int cursor2 = base2;
    // Indexes int a
    int dest = base1;
    // Move first element of second run and deal with degenerate cases
    s.copyElement(a, cursor2++, a, dest++);
    if (--len2 == 0) {
        s.copyRange(tmp, cursor1, a, dest, len1);
        return;
    }
    if (len1 == 1) {
        s.copyRange(a, cursor2, a, dest, len2);
        // Last elt of run 1 to end of merge
        s.copyElement(tmp, cursor1, a, dest + len2);
        return;
    }
    K key0 = s.newKey();
    K key1 = s.newKey();
    // Use local variable for performance
    Comparator<? super K> c = this.c;
    // ""    ""       ""     ""      ""
    int minGallop = this.minGallop;
    outer: while (true) {
        // Number of times in a row that first run won
        int count1 = 0;
        // Number of times in a row that second run won
        int count2 = 0;
        /*
         * Do the straightforward thing until (if ever) one run starts
         * winning consistently.
         */
        do {
            assert len1 > 1 && len2 > 0;
            if (c.compare(s.getKey(a, cursor2, key0), s.getKey(tmp, cursor1, key1)) < 0) {
                s.copyElement(a, cursor2++, a, dest++);
                count2++;
                count1 = 0;
                if (--len2 == 0)
                    break outer;
            } else {
                s.copyElement(tmp, cursor1++, a, dest++);
                count1++;
                count2 = 0;
                if (--len1 == 1)
                    break outer;
            }
        } while ((count1 | count2) < minGallop);
        /*
         * One run is winning so consistently that galloping may be a
         * huge win. So try that, and continue galloping until (if ever)
         * neither run appears to be winning consistently anymore.
         */
        do {
            assert len1 > 1 && len2 > 0;
            count1 = gallopRight(s.getKey(a, cursor2, key0), tmp, cursor1, len1, 0, c);
            if (count1 != 0) {
                s.copyRange(tmp, cursor1, a, dest, count1);
                dest += count1;
                cursor1 += count1;
                len1 -= count1;
                if (// len1 == 1 || len1 == 0
                len1 <= 1)
                    break outer;
            }
            s.copyElement(a, cursor2++, a, dest++);
            if (--len2 == 0)
                break outer;
            count2 = gallopLeft(s.getKey(tmp, cursor1, key0), a, cursor2, len2, 0, c);
            if (count2 != 0) {
                s.copyRange(a, cursor2, a, dest, count2);
                dest += count2;
                cursor2 += count2;
                len2 -= count2;
                if (len2 == 0)
                    break outer;
            }
            s.copyElement(tmp, cursor1++, a, dest++);
            if (--len1 == 1)
                break outer;
            minGallop--;
        } while (count1 >= MIN_GALLOP | count2 >= MIN_GALLOP);
        if (minGallop < 0)
            minGallop = 0;
        // Penalize for leaving gallop mode
        minGallop += 2;
    }
    // End of ""outer"" loop
    // Write back to field
    this.minGallop = minGallop < 1 ? 1 : minGallop;
    if (len1 == 1) {
        assert len2 > 0;
        s.copyRange(a, cursor2, a, dest, len2);
        // Last elt of run 1 to end of merge
        s.copyElement(tmp, cursor1, a, dest + len2);
    } else if (len1 == 0) {
        throw new IllegalArgumentException(""Comparison method violates its general contract!"");
    } else {
        assert len2 == 0;
        assert len1 > 1;
        s.copyRange(tmp, cursor1, a, dest, len1);
    }
} 
{
    assert len1 > 0 && len2 > 0 && base1 + len1 == base2;
    // Copy second run into temp array
    // For performance
    Buffer a = this.a;
    Buffer tmp = ensureCapacity(len2);
    s.copyRange(a, base2, tmp, 0, len2);
    // Indexes into a
    int cursor1 = base1 + len1 - 1;
    // Indexes into tmp array
    int cursor2 = len2 - 1;
    // Indexes into a
    int dest = base2 + len2 - 1;
    K key0 = s.newKey();
    K key1 = s.newKey();
    // Move last element of first run and deal with degenerate cases
    s.copyElement(a, cursor1--, a, dest--);
    if (--len1 == 0) {
        s.copyRange(tmp, 0, a, dest - (len2 - 1), len2);
        return;
    }
    if (len2 == 1) {
        dest -= len1;
        cursor1 -= len1;
        s.copyRange(a, cursor1 + 1, a, dest + 1, len1);
        s.copyElement(tmp, cursor2, a, dest);
        return;
    }
    // Use local variable for performance
    Comparator<? super K> c = this.c;
    // ""    ""       ""     ""      ""
    int minGallop = this.minGallop;
    outer: while (true) {
        // Number of times in a row that first run won
        int count1 = 0;
        // Number of times in a row that second run won
        int count2 = 0;
        /*
         * Do the straightforward thing until (if ever) one run
         * appears to win consistently.
         */
        do {
            assert len1 > 0 && len2 > 1;
            if (c.compare(s.getKey(tmp, cursor2, key0), s.getKey(a, cursor1, key1)) < 0) {
                s.copyElement(a, cursor1--, a, dest--);
                count1++;
                count2 = 0;
                if (--len1 == 0)
                    break outer;
            } else {
                s.copyElement(tmp, cursor2--, a, dest--);
                count2++;
                count1 = 0;
                if (--len2 == 1)
                    break outer;
            }
        } while ((count1 | count2) < minGallop);
        /*
         * One run is winning so consistently that galloping may be a
         * huge win. So try that, and continue galloping until (if ever)
         * neither run appears to be winning consistently anymore.
         */
        do {
            assert len1 > 0 && len2 > 1;
            count1 = len1 - gallopRight(s.getKey(tmp, cursor2, key0), a, base1, len1, len1 - 1, c);
            if (count1 != 0) {
                dest -= count1;
                cursor1 -= count1;
                len1 -= count1;
                s.copyRange(a, cursor1 + 1, a, dest + 1, count1);
                if (len1 == 0)
                    break outer;
            }
            s.copyElement(tmp, cursor2--, a, dest--);
            if (--len2 == 1)
                break outer;
            count2 = len2 - gallopLeft(s.getKey(a, cursor1, key0), tmp, 0, len2, len2 - 1, c);
            if (count2 != 0) {
                dest -= count2;
                cursor2 -= count2;
                len2 -= count2;
                s.copyRange(tmp, cursor2 + 1, a, dest + 1, count2);
                if (// len2 == 1 || len2 == 0
                len2 <= 1)
                    break outer;
            }
            s.copyElement(a, cursor1--, a, dest--);
            if (--len1 == 0)
                break outer;
            minGallop--;
        } while (count1 >= MIN_GALLOP | count2 >= MIN_GALLOP);
        if (minGallop < 0)
            minGallop = 0;
        // Penalize for leaving gallop mode
        minGallop += 2;
    }
    // End of ""outer"" loop
    // Write back to field
    this.minGallop = minGallop < 1 ? 1 : minGallop;
    if (len2 == 1) {
        assert len1 > 0;
        dest -= len1;
        cursor1 -= len1;
        s.copyRange(a, cursor1 + 1, a, dest + 1, len1);
        // Move first elt of run2 to front of merge
        s.copyElement(tmp, cursor2, a, dest);
    } else if (len2 == 0) {
        throw new IllegalArgumentException(""Comparison method violates its general contract!"");
    } else {
        assert len1 == 0;
        assert len2 > 0;
        s.copyRange(tmp, 0, a, dest - (len2 - 1), len2);
    }
} 
{
    if (tmpLength < minCapacity) {
        // Compute smallest power of 2 > minCapacity
        int newSize = minCapacity;
        newSize |= newSize >> 1;
        newSize |= newSize >> 2;
        newSize |= newSize >> 4;
        newSize |= newSize >> 8;
        newSize |= newSize >> 16;
        newSize++;
        if (// Not bloody likely!
        newSize < 0)
            newSize = minCapacity;
        else
            newSize = Math.min(newSize, aLength >>> 1);
        tmp = s.allocate(newSize);
        tmpLength = newSize;
    }
    return tmp;
} 
","/**
 * Creates a TimSort instance to maintain the state of an ongoing sort.
 *
 * @param a the array to be sorted
 * @param c the comparator to determine the order of the sort
 */
 
","{
    this.aLength = len;
    this.a = a;
    this.c = c;
    // Allocate temp storage (which may be increased later if necessary)
    tmpLength = len < 2 * INITIAL_TMP_STORAGE_LENGTH ? len >>> 1 : INITIAL_TMP_STORAGE_LENGTH;
    tmp = s.allocate(tmpLength);
    /*
       * Allocate runs-to-be-merged stack (which cannot be expanded).  The
       * stack length requirements are described in listsort.txt.  The C
       * version always uses the same stack length (85), but this was
       * measured to be too expensive when sorting ""mid-sized"" arrays (e.g.,
       * 100 elements) in Java.  Therefore, we use smaller (but sufficiently
       * large) stack lengths for smaller arrays.  The ""magic numbers"" in the
       * computation below must be changed if MIN_MERGE is decreased.  See
       * the MIN_MERGE declaration above for more information.
       * The maximum value of 49 allows for an array up to length
       * Integer.MAX_VALUE-4, if array is filled by the worst case stack size
       * increasing scenario. More explanations are given in section 4 of:
       * http://envisage-project.eu/wp-content/uploads/2015/02/sorting.pdf
       */
    int stackLen = (len < 120 ? 5 : len < 1542 ? 10 : len < 119151 ? 24 : 49);
    runBase = new int[stackLen];
    runLen = new int[stackLen];
} 
","/**
 * This is the minimum sized sequence that will be merged.  Shorter
 * sequences will be lengthened by calling binarySort.  If the entire
 * array is less than this length, no merges will be performed.
 *
 * This constant should be a power of two.  It was 64 in Tim Peter's C
 * implementation, but 32 was empirically determined to work better in
 * this implementation.  In the unlikely event that you set this constant
 * to be a number that's not a power of two, you'll need to change the
 * minRunLength computation.
 *
 * If you decrease this constant, you must change the stackLen
 * computation in the TimSort constructor, or you risk an
 * ArrayOutOfBounds exception.  See listsort.txt for a discussion
 * of the minimum stack length required as a function of the length
 * of the array being sorted and the minimum merge sequence length.
 */
 
/**
 * The Buffer being sorted.
 */
 
/**
 * Length of the sort Buffer.
 */
 
/**
 * The comparator for this sort.
 */
 
/**
 * When we get into galloping mode, we stay there until both runs win less
 * often than MIN_GALLOP consecutive times.
 */
 
/**
 * This controls when we get *into* galloping mode.  It is initialized
 * to MIN_GALLOP.  The mergeLo and mergeHi methods nudge it higher for
 * random data, and lower for highly structured data.
 */
 
/**
 * Maximum initial size of tmp array, which is used for merging.  The array
 * can grow to accommodate demand.
 *
 * Unlike Tim's original C version, we do not allocate this much storage
 * when sorting smaller arrays.  This change was required for performance.
 */
 
/**
 * Length of the temp storage.
 */
 
","Field MIN_MERGE
Field a
Field aLength
Field c
Field MIN_GALLOP
Field minGallop
Field INITIAL_TMP_STORAGE_LENGTH
Field tmpLength
",33,1687
UnsafeSortDataFormat.java,24,55,0.43636363636363634,"
 * Supports sorting an array of (record pointer, key prefix) pairs.
 * Used in {@link UnsafeInMemorySorter}.
 * <p>
 * Within each long[] buffer, position {@code 2 * i} holds a pointer to the record at
 * index {@code i}, while position {@code 2 * i + 1} in the array holds an 8-byte key prefix.
 ",,,,,,,5,285
UnsafeSorterSpillReader.java,27,106,0.25471698113207547,"
 * Reads spill files written by {@link UnsafeSorterSpillWriter} (see that class for a description
 * of the file format).
 ",,,,,,,2,119
UnsafeSorterSpillWriter.java,43,105,0.4095238095238095,"
 * Spills a list of sorted records to disk. Spill files have the following format:
 *
 *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]
 ","/**
 * Write a record to a spill file.
 *
 * @param baseObject the base object / memory page containing the record
 * @param baseOffset the base offset which points directly to the record data.
 * @param recordLength the length of the record.
 * @param keyPrefix a sort key prefix
 */
 
","{
    if (numRecordsSpilled == numRecordsToWrite) {
        throw new IllegalStateException(""Number of records written exceeded numRecordsToWrite = "" + numRecordsToWrite);
    } else {
        numRecordsSpilled++;
    }
    writeIntToBuffer(recordLength, 0);
    writeLongToBuffer(keyPrefix, 4);
    int dataRemaining = recordLength;
    // space used by prefix + len
    int freeSpaceInWriteBuffer = diskWriteBufferSize - 4 - 8;
    long recordReadPosition = baseOffset;
    while (dataRemaining > 0) {
        final int toTransfer = Math.min(freeSpaceInWriteBuffer, dataRemaining);
        Platform.copyMemory(baseObject, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET + (diskWriteBufferSize - freeSpaceInWriteBuffer), toTransfer);
        writer.write(writeBuffer, 0, (diskWriteBufferSize - freeSpaceInWriteBuffer) + toTransfer);
        recordReadPosition += toTransfer;
        dataRemaining -= toTransfer;
        freeSpaceInWriteBuffer = diskWriteBufferSize;
    }
    if (freeSpaceInWriteBuffer < diskWriteBufferSize) {
        writer.write(writeBuffer, 0, (diskWriteBufferSize - freeSpaceInWriteBuffer));
    }
    writer.recordWritten();
} 
",,,"/**
 * The buffer size to use when writing the sorted records to an on-disk file, and
 * this space used by prefix + len + recordLength must be greater than 4 + 8 bytes.
 */
 
","Field diskWriteBufferSize
",3,153
UnsafeSorterSpillMerger.java,24,63,0.38095238095238093,,"/**
 * Add an UnsafeSorterIterator to this merger
 */
 
","{
    if (spillReader.hasNext()) {
        // We only add the spillReader to the priorityQueue if it is not empty. We do this to
        // make sure the hasNext method of UnsafeSorterIterator returned by getSortedIterator
        // does not return wrong result because hasNext will return true
        // at least priorityQueue.size() times. If we allow n spillReaders in the
        // priorityQueue, we will have n extra empty records in the result of UnsafeSorterIterator.
        spillReader.loadNext();
        priorityQueue.add(spillReader);
        numRecords += spillReader.getNumRecords();
    }
} 
",,,,,1,0
UnsafeSorterIterator.java,16,11,1.4545454545454546,,,,,,,,1,0
RadixSort.java,87,163,0.5337423312883436,,"/**
 * Sorts a given array of longs using least-significant-digit radix sort. This routine assumes
 * you have extra space at the end of the array at least equal to the number of records. The
 * sort is destructive and may relocate the data positioned within the array.
 *
 * @param array array of long elements followed by at least that many empty slots.
 * @param numRecords number of data records in the array.
 * @param startByteIndex the first byte (in range [0, 7]) to sort each long by, counting from the
 *                       least significant byte.
 * @param endByteIndex the last byte (in range [0, 7]) to sort each long by, counting from the
 *                     least significant byte. Must be greater than startByteIndex.
 * @param desc whether this is a descending (binary-order) sort.
 * @param signed whether this is a signed (two's complement) sort.
 *
 * @return The starting index of the sorted data within the given array. We return this instead
 *         of always copying the data back to position zero for efficiency.
 */
 
/**
 * Performs a partial sort by copying data into destination offsets for each byte value at the
 * specified byte offset.
 *
 * @param array array to partially sort.
 * @param numRecords number of data records in the array.
 * @param counts counts for each byte value. This routine destructively modifies this array.
 * @param byteIdx the byte in a long to sort at, counting from the least significant byte.
 * @param inIndex the starting index in the array where input data is located.
 * @param outIndex the starting index where sorted output data should be written.
 * @param desc whether this is a descending (binary-order) sort.
 * @param signed whether this is a signed (two's complement) sort (only applies to last byte).
 */
 
/**
 * Computes a value histogram for each byte in the given array.
 *
 * @param array array to count records in.
 * @param numRecords number of data records in the array.
 * @param startByteIndex the first byte to compute counts for (the prior are skipped).
 * @param endByteIndex the last byte to compute counts for.
 *
 * @return an array of eight 256-byte count arrays, one for each byte starting from the least
 *         significant byte. If the byte does not need sorting the array will be null.
 */
 
/**
 * Transforms counts into the proper unsafe output offsets for the sort type.
 *
 * @param counts counts for each byte value. This routine destructively modifies this array.
 * @param numRecords number of data records in the original data array.
 * @param outputOffset output offset in bytes from the base array object.
 * @param bytesPerRecord size of each record (8 for plain sort, 16 for key-prefix sort).
 * @param desc whether this is a descending (binary-order) sort.
 * @param signed whether this is a signed (two's complement) sort.
 *
 * @return the input counts array.
 */
 
/**
 * Specialization of sort() for key-prefix arrays. In this type of array, each record consists
 * of two longs, only the second of which is sorted on.
 *
 * @param startIndex starting index in the array to sort from. This parameter is not supported
 *    in the plain sort() implementation.
 */
 
/**
 * Specialization of getCounts() for key-prefix arrays. We could probably combine this with
 * getCounts with some added parameters but that seems to hurt in benchmarks.
 */
 
/**
 * Specialization of sortAtByte() for key-prefix arrays.
 */
 
","{
    assert startByteIndex >= 0 : ""startByteIndex ("" + startByteIndex + "") should >= 0"";
    assert endByteIndex <= 7 : ""endByteIndex ("" + endByteIndex + "") should <= 7"";
    assert endByteIndex > startByteIndex;
    assert numRecords * 2 <= array.size();
    long inIndex = 0;
    long outIndex = numRecords;
    if (numRecords > 0) {
        long[][] counts = getCounts(array, numRecords, startByteIndex, endByteIndex);
        for (int i = startByteIndex; i <= endByteIndex; i++) {
            if (counts[i] != null) {
                sortAtByte(array, numRecords, counts[i], i, inIndex, outIndex, desc, signed && i == endByteIndex);
                long tmp = inIndex;
                inIndex = outIndex;
                outIndex = tmp;
            }
        }
    }
    return Ints.checkedCast(inIndex);
} 
{
    assert counts.length == 256;
    long[] offsets = transformCountsToOffsets(counts, numRecords, array.getBaseOffset() + outIndex * 8L, 8, desc, signed);
    Object baseObject = array.getBaseObject();
    long baseOffset = array.getBaseOffset() + inIndex * 8L;
    long maxOffset = baseOffset + numRecords * 8L;
    for (long offset = baseOffset; offset < maxOffset; offset += 8) {
        long value = Platform.getLong(baseObject, offset);
        int bucket = (int) ((value >>> (byteIdx * 8)) & 0xff);
        Platform.putLong(baseObject, offsets[bucket], value);
        offsets[bucket] += 8;
    }
} 
{
    long[][] counts = new long[8][];
    // Optimization: do a fast pre-pass to determine which byte indices we can skip for sorting.
    // If all the byte values at a particular index are the same we don't need to count it.
    long bitwiseMax = 0;
    long bitwiseMin = -1L;
    long maxOffset = array.getBaseOffset() + numRecords * 8L;
    Object baseObject = array.getBaseObject();
    for (long offset = array.getBaseOffset(); offset < maxOffset; offset += 8) {
        long value = Platform.getLong(baseObject, offset);
        bitwiseMax |= value;
        bitwiseMin &= value;
    }
    long bitsChanged = bitwiseMin ^ bitwiseMax;
    // Compute counts for each byte index.
    for (int i = startByteIndex; i <= endByteIndex; i++) {
        if (((bitsChanged >>> (i * 8)) & 0xff) != 0) {
            counts[i] = new long[256];
            // TODO(ekl) consider computing all the counts in one pass.
            for (long offset = array.getBaseOffset(); offset < maxOffset; offset += 8) {
                counts[i][(int) ((Platform.getLong(baseObject, offset) >>> (i * 8)) & 0xff)]++;
            }
        }
    }
    return counts;
} 
{
    assert counts.length == 256;
    // output the negative records first (values 129-255).
    int start = signed ? 128 : 0;
    if (desc) {
        long pos = numRecords;
        for (int i = start; i < start + 256; i++) {
            pos -= counts[i & 0xff];
            counts[i & 0xff] = outputOffset + pos * bytesPerRecord;
        }
    } else {
        long pos = 0;
        for (int i = start; i < start + 256; i++) {
            long tmp = counts[i & 0xff];
            counts[i & 0xff] = outputOffset + pos * bytesPerRecord;
            pos += tmp;
        }
    }
    return counts;
} 
{
    assert startByteIndex >= 0 : ""startByteIndex ("" + startByteIndex + "") should >= 0"";
    assert endByteIndex <= 7 : ""endByteIndex ("" + endByteIndex + "") should <= 7"";
    assert endByteIndex > startByteIndex;
    assert numRecords * 4 <= array.size();
    long inIndex = startIndex;
    long outIndex = startIndex + numRecords * 2L;
    if (numRecords > 0) {
        long[][] counts = getKeyPrefixArrayCounts(array, startIndex, numRecords, startByteIndex, endByteIndex);
        for (int i = startByteIndex; i <= endByteIndex; i++) {
            if (counts[i] != null) {
                sortKeyPrefixArrayAtByte(array, numRecords, counts[i], i, inIndex, outIndex, desc, signed && i == endByteIndex);
                long tmp = inIndex;
                inIndex = outIndex;
                outIndex = tmp;
            }
        }
    }
    return Ints.checkedCast(inIndex);
} 
{
    long[][] counts = new long[8][];
    long bitwiseMax = 0;
    long bitwiseMin = -1L;
    long baseOffset = array.getBaseOffset() + startIndex * 8L;
    long limit = baseOffset + numRecords * 16L;
    Object baseObject = array.getBaseObject();
    for (long offset = baseOffset; offset < limit; offset += 16) {
        long value = Platform.getLong(baseObject, offset + 8);
        bitwiseMax |= value;
        bitwiseMin &= value;
    }
    long bitsChanged = bitwiseMin ^ bitwiseMax;
    for (int i = startByteIndex; i <= endByteIndex; i++) {
        if (((bitsChanged >>> (i * 8)) & 0xff) != 0) {
            counts[i] = new long[256];
            for (long offset = baseOffset; offset < limit; offset += 16) {
                counts[i][(int) ((Platform.getLong(baseObject, offset + 8) >>> (i * 8)) & 0xff)]++;
            }
        }
    }
    return counts;
} 
{
    assert counts.length == 256;
    long[] offsets = transformCountsToOffsets(counts, numRecords, array.getBaseOffset() + outIndex * 8L, 16, desc, signed);
    Object baseObject = array.getBaseObject();
    long baseOffset = array.getBaseOffset() + inIndex * 8L;
    long maxOffset = baseOffset + numRecords * 16L;
    for (long offset = baseOffset; offset < maxOffset; offset += 16) {
        long key = Platform.getLong(baseObject, offset);
        long prefix = Platform.getLong(baseObject, offset + 8);
        int bucket = (int) ((prefix >>> (byteIdx * 8)) & 0xff);
        long dest = offsets[bucket];
        Platform.putLong(baseObject, dest, key);
        Platform.putLong(baseObject, dest + 8, prefix);
        offsets[bucket] += 16;
    }
} 
",,,,,1,0
UnsafeInMemorySorter.java,84,255,0.32941176470588235,"
 * Sorts records using an AlphaSort-style key-prefix sort. This sort stores pointers to records
 * alongside a user-defined prefix of the record's sorting key. When the underlying sort algorithm
 * compares records, it will first compare the stored key prefixes; if the prefixes are not equal,
 * then we do not need to traverse the record pointers to compare the actual records. Avoiding these
 * random memory accesses improves cache hit rates.
 ","/**
 * Free the memory used by pointer array.
 */
 
/**
 * @return the number of records that have been inserted into this sorter.
 */
 
/**
 * @return the total amount of time spent sorting data (in-memory only).
 */
 
/**
 * Inserts a record to be sorted. Assumes that the record pointer points to a record length
 * stored as a 4-byte integer, followed by the record's bytes.
 *
 * @param recordPointer pointer to a record in a data page, encoded by {@link TaskMemoryManager}.
 * @param keyPrefix a user-defined key prefix
 */
 
/**
 * Return an iterator over record pointers in sorted order. For efficiency, all calls to
 * {@code next()} will return the same mutable object.
 */
 
","{
    if (consumer != null) {
        if (array != null) {
            consumer.freeArray(array);
        }
        array = null;
    }
} 
{
    return pos / 2;
} 
{
    return totalSortTimeNanos;
} 
{
    if (!hasSpaceForAnotherRecord()) {
        throw new IllegalStateException(""There is no space for new record"");
    }
    if (prefixIsNull && radixSortSupport != null) {
        // Swap forward a non-null record to make room for this one at the beginning of the array.
        array.set(pos, array.get(nullBoundaryPos));
        pos++;
        array.set(pos, array.get(nullBoundaryPos + 1));
        pos++;
        // Place this record in the vacated position.
        array.set(nullBoundaryPos, recordPointer);
        nullBoundaryPos++;
        array.set(nullBoundaryPos, keyPrefix);
        nullBoundaryPos++;
    } else {
        array.set(pos, recordPointer);
        pos++;
        array.set(pos, keyPrefix);
        pos++;
    }
} 
{
    int offset = 0;
    long start = System.nanoTime();
    if (sortComparator != null) {
        if (this.radixSortSupport != null) {
            offset = RadixSort.sortKeyPrefixArray(array, nullBoundaryPos, (pos - nullBoundaryPos) / 2L, 0, 7, radixSortSupport.sortDescending(), radixSortSupport.sortSigned());
        } else {
            MemoryBlock unused = new MemoryBlock(array.getBaseObject(), array.getBaseOffset() + pos * 8L, (array.size() - pos) * 8L);
            LongArray buffer = new LongArray(unused);
            Sorter<RecordPointerAndKeyPrefix, LongArray> sorter = new Sorter<>(new UnsafeSortDataFormat(buffer));
            sorter.sort(array, 0, pos / 2, sortComparator);
        }
    }
    totalSortTimeNanos += System.nanoTime() - start;
    if (nullBoundaryPos > 0) {
        assert radixSortSupport != null : ""Nulls are only stored separately with radix sort"";
        LinkedList<UnsafeSorterIterator> queue = new LinkedList<>();
        // The null order is either LAST or FIRST, regardless of sorting direction (ASC|DESC)
        if (radixSortSupport.nullsFirst()) {
            queue.add(new SortedIterator(nullBoundaryPos / 2, 0));
            queue.add(new SortedIterator((pos - nullBoundaryPos) / 2, offset));
        } else {
            queue.add(new SortedIterator((pos - nullBoundaryPos) / 2, offset));
            queue.add(new SortedIterator(nullBoundaryPos / 2, 0));
        }
        return new UnsafeExternalSorter.ChainedIterator(queue);
    } else {
        return new SortedIterator(pos / 2, offset);
    }
} 
",,,"/**
 * If non-null, specifies the radix sort parameters and that radix sort will be used.
 */
 
/**
 * Within this buffer, position {@code 2 * i} holds a pointer to the record at
 * index {@code i}, while position {@code 2 * i + 1} in the array holds an 8-byte key prefix.
 *
 * Only part of the array will be used to store the pointers, the rest part is preserved as
 * temporary buffer for sorting.
 */
 
/**
 * The position in the sort buffer where new records can be inserted.
 */
 
/**
 * If sorting with radix sort, specifies the starting position in the sort buffer where records
 * with non-null prefixes are kept. Positions [0..nullBoundaryPos) will contain null-prefixed
 * records, and positions [nullBoundaryPos..pos) non-null prefixed records. This lets us avoid
 * radix sorting over null values.
 */
 
","Field radixSortSupport
Field array
Field pos
Field nullBoundaryPos
",5,438
RecordPointerAndKeyPrefix.java,23,5,4.6,,,,,,"/**
 * A pointer to a record; see {@link org.apache.spark.memory.TaskMemoryManager} for a
 * description of how these addresses are encoded.
 */
 
/**
 * A key prefix, for use in comparisons.
 */
 
","Field recordPointer
Field keyPrefix
",1,0
PrefixComparators.java,35,116,0.3017241379310345,"| Standard prefix comparator implementations||
   * Provides radix sort parameters. Comparators implementing this also are indicating that the
   * ordering they define is compatible with radix sort.
   ","/**
 * Converts the double into a value that compares correctly as an unsigned long. For more
 * details see http://stereopsis.com/radix.html.
 */
 
/**
 * @return Whether the sort should be descending in binary sort order.
 */
 
/**
 * @return Whether the sort should take into account the sign bit.
 */
 
/**
 * @return Whether the sort should put nulls first or last.
 */
 
","{
    // normalize -0.0 to 0.0, as they should be equal
    value = value == -0.0 ? 0.0 : value;
    // Java's doubleToLongBits already canonicalizes all NaN values to the smallest possible
    // positive NaN, so there's nothing special we need to do for NaNs.
    long bits = Double.doubleToLongBits(value);
    // Negative floats compare backwards due to their sign-magnitude representation, so flip
    // all the bits in this case.
    long mask = -(bits >>> 63) | 0x8000000000000000L;
    return bits ^ mask;
} 
sortDescending 
sortSigned 
nullsFirst 
",,,,,3,198
PrefixComparator.java,20,6,3.3333333333333335,"
 * Compares 8-byte key prefixes in prefix sort. Subclasses may implement type-specific
 * comparisons, such as lexicographic comparison for strings.
 ",,,,,,,2,146
RecordComparator.java,26,10,2.6,"
 * Compares records for ordering. In cases where the entire sorting key can fit in the 8-byte
 * prefix, this may simply return 0.
 ","/**
 * Compare two records for order.
 *
 * @return a negative integer, zero, or a positive integer as the first record is less than,
 *         equal to, or greater than the second.
 */
 
","compare 
",,,,,2,128
UnsafeExternalSorter.java,144,503,0.28628230616302186,"
 * External sorter based on {@link UnsafeInMemorySorter}.
 |
   * An UnsafeSorterIterator that support spilling.
   |
   * Chain multiple UnsafeSorterIterator together as single one.
   ","/**
 * Marks the current page as no-more-space-available, and as a result, either allocate a
 * new page or spill when we see the next record.
 */
 
/**
 * Sort and spill the current records in response to memory pressure.
 */
 
/**
 * Return the total memory usage of this sorter, including the data pages and the sorter's pointer
 * array.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * @return the total amount of time spent sorting data (in-memory only).
 */
 
/**
 * Return the total number of bytes that has been spilled into disk so far.
 */
 
/**
 * Free this sorter's data pages.
 *
 * @return the number of bytes freed.
 */
 
/**
 * Deletes any spill files created by this sorter.
 */
 
/**
 * Frees this sorter's in-memory data structures and cleans up its spill files.
 */
 
/**
 * Checks whether there is enough space to insert an additional record in to the sort pointer
 * array and grows the array if additional space is required. If the required space cannot be
 * obtained, then the in-memory data will be spilled to disk.
 */
 
/**
 * Allocates more memory in order to insert an additional record. This will request additional
 * memory from the memory manager and spill if the requested memory can not be obtained.
 *
 * @param required the required space in the data page, in bytes, including space for storing
 *                      the record size. This must be less than or equal to the page size (records
 *                      that exceed the page size are handled via a different code path which uses
 *                      special overflow pages).
 */
 
/**
 * Write a record to the sorter.
 */
 
/**
 * Write a key-value record to the sorter. The key and value will be put together in-memory,
 * using the following format:
 *
 * record length (4 bytes), key length (4 bytes), key data, value data
 *
 * record length = key length + value length + 4
 */
 
/**
 * Merges another UnsafeExternalSorters into this one, the other one will be emptied.
 */
 
/**
 * Returns a sorted iterator. It is the caller's responsibility to call `cleanupResources()`
 * after consuming this iterator.
 */
 
/**
 * Returns an iterator starts from startIndex, which will return the rows in the order as
 * inserted.
 *
 * It is the caller's responsibility to call `cleanupResources()`
 * after consuming this iterator.
 *
 * TODO: support forced spilling
 */
 
","{
    if (currentPage != null) {
        pageCursor = currentPage.getBaseOffset() + currentPage.size();
    }
} 
{
    if (trigger != this) {
        if (readingIterator != null) {
            return readingIterator.spill();
        }
        // this should throw exception
        return 0L;
    }
    if (inMemSorter == null || inMemSorter.numRecords() <= 0) {
        return 0L;
    }
    logger.info(""Thread {} spilling sort data of {} to disk ({} {} so far)"", Thread.currentThread().getId(), Utils.bytesToString(getMemoryUsage()), spillWriters.size(), spillWriters.size() > 1 ? "" times"" : "" time"");
    ShuffleWriteMetrics writeMetrics = new ShuffleWriteMetrics();
    final UnsafeSorterSpillWriter spillWriter = new UnsafeSorterSpillWriter(blockManager, fileBufferSizeBytes, writeMetrics, inMemSorter.numRecords());
    spillWriters.add(spillWriter);
    spillIterator(inMemSorter.getSortedIterator(), spillWriter);
    final long spillSize = freeMemory();
    // Note that this is more-or-less going to be a multiple of the page size, so wasted space in
    // pages will currently be counted as memory spilled even though that space isn't actually
    // written to disk. This also counts the space needed to store the sorter's pointer array.
    inMemSorter.reset();
    // Reset the in-memory sorter's pointer array only after freeing up the memory pages holding the
    // records. Otherwise, if the task is over allocated memory, then without freeing the memory
    // pages, we might not be able to get memory for the pointer array.
    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);
    taskContext.taskMetrics().incDiskBytesSpilled(writeMetrics.bytesWritten());
    totalSpillBytes += spillSize;
    return spillSize;
} 
{
    long totalPageSize = 0;
    for (MemoryBlock page : allocatedPages) {
        totalPageSize += page.size();
    }
    return ((inMemSorter == null) ? 0 : inMemSorter.getMemoryUsage()) + totalPageSize;
} 
{
    updatePeakMemoryUsed();
    return peakMemoryUsedBytes;
} 
{
    UnsafeInMemorySorter sorter = inMemSorter;
    if (sorter != null) {
        return sorter.getSortTimeNanos();
    }
    return totalSortTimeNanos;
} 
{
    return totalSpillBytes;
} 
{
    updatePeakMemoryUsed();
    long memoryFreed = 0;
    for (MemoryBlock block : allocatedPages) {
        memoryFreed += block.size();
        freePage(block);
    }
    allocatedPages.clear();
    currentPage = null;
    pageCursor = 0;
    return memoryFreed;
} 
{
    for (UnsafeSorterSpillWriter spill : spillWriters) {
        File file = spill.getFile();
        if (file != null && file.exists()) {
            if (!file.delete()) {
                logger.error(""Was unable to delete spill file {}"", file.getAbsolutePath());
            }
        }
    }
} 
{
    synchronized (this) {
        deleteSpillFiles();
        freeMemory();
        if (inMemSorter != null) {
            inMemSorter.free();
            inMemSorter = null;
        }
    }
} 
{
    assert (inMemSorter != null);
    if (!inMemSorter.hasSpaceForAnotherRecord()) {
        long used = inMemSorter.getMemoryUsage();
        LongArray array;
        try {
            // could trigger spilling
            array = allocateArray(used / 8 * 2);
        } catch (TooLargePageException e) {
            // The pointer array is too big to fix in a single page, spill.
            spill();
            return;
        } catch (SparkOutOfMemoryError e) {
            // should have trigger spilling
            if (!inMemSorter.hasSpaceForAnotherRecord()) {
                logger.error(""Unable to grow the pointer array"");
                throw e;
            }
            return;
        }
        // check if spilling is triggered or not
        if (inMemSorter.hasSpaceForAnotherRecord()) {
            freeArray(array);
        } else {
            inMemSorter.expandPointerArray(array);
        }
    }
} 
{
    if (currentPage == null || pageCursor + required > currentPage.getBaseOffset() + currentPage.size()) {
        // TODO: try to find space on previous pages
        currentPage = allocatePage(required);
        pageCursor = currentPage.getBaseOffset();
        allocatedPages.add(currentPage);
    }
} 
{
    assert (inMemSorter != null);
    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {
        logger.info(""Spilling data because number of spilledRecords crossed the threshold "" + numElementsForSpillThreshold);
        spill();
    }
    growPointerArrayIfNecessary();
    int uaoSize = UnsafeAlignedOffset.getUaoSize();
    // Need 4 or 8 bytes to store the record length.
    final int required = length + uaoSize;
    acquireNewPageIfNecessary(required);
    final Object base = currentPage.getBaseObject();
    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);
    UnsafeAlignedOffset.putSize(base, pageCursor, length);
    pageCursor += uaoSize;
    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);
    pageCursor += length;
    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);
} 
{
    growPointerArrayIfNecessary();
    int uaoSize = UnsafeAlignedOffset.getUaoSize();
    final int required = keyLen + valueLen + (2 * uaoSize);
    acquireNewPageIfNecessary(required);
    final Object base = currentPage.getBaseObject();
    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);
    UnsafeAlignedOffset.putSize(base, pageCursor, keyLen + valueLen + uaoSize);
    pageCursor += uaoSize;
    UnsafeAlignedOffset.putSize(base, pageCursor, keyLen);
    pageCursor += uaoSize;
    Platform.copyMemory(keyBase, keyOffset, base, pageCursor, keyLen);
    pageCursor += keyLen;
    Platform.copyMemory(valueBase, valueOffset, base, pageCursor, valueLen);
    pageCursor += valueLen;
    assert (inMemSorter != null);
    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);
} 
{
    other.spill();
    spillWriters.addAll(other.spillWriters);
    // remove them from `spillWriters`, or the files will be deleted in `cleanupResources`.
    other.spillWriters.clear();
    other.cleanupResources();
} 
{
    assert (recordComparatorSupplier != null);
    if (spillWriters.isEmpty()) {
        assert (inMemSorter != null);
        readingIterator = new SpillableIterator(inMemSorter.getSortedIterator());
        return readingIterator;
    } else {
        final UnsafeSorterSpillMerger spillMerger = new UnsafeSorterSpillMerger(recordComparatorSupplier.get(), prefixComparator, spillWriters.size());
        for (UnsafeSorterSpillWriter spillWriter : spillWriters) {
            spillMerger.addSpillIfNotEmpty(spillWriter.getReader(serializerManager));
        }
        if (inMemSorter != null) {
            readingIterator = new SpillableIterator(inMemSorter.getSortedIterator());
            spillMerger.addSpillIfNotEmpty(readingIterator);
        }
        return spillMerger.getSortedIterator();
    }
} 
{
    if (spillWriters.isEmpty()) {
        assert (inMemSorter != null);
        UnsafeSorterIterator iter = inMemSorter.getSortedIterator();
        moveOver(iter, startIndex);
        return iter;
    } else {
        LinkedList<UnsafeSorterIterator> queue = new LinkedList<>();
        int i = 0;
        for (UnsafeSorterSpillWriter spillWriter : spillWriters) {
            if (i + spillWriter.recordsSpilled() > startIndex) {
                UnsafeSorterIterator iter = spillWriter.getReader(serializerManager);
                moveOver(iter, startIndex - i);
                queue.add(iter);
            }
            i += spillWriter.recordsSpilled();
        }
        if (inMemSorter != null) {
            UnsafeSorterIterator iter = inMemSorter.getSortedIterator();
            moveOver(iter, startIndex - i);
            queue.add(iter);
        }
        return new ChainedIterator(queue);
    }
} 
",,,"/**
 * {@link RecordComparator} may probably keep the reference to the records they compared last
 * time, so we should not keep a {@link RecordComparator} instance inside
 * {@link UnsafeExternalSorter}, because {@link UnsafeExternalSorter} is referenced by
 * {@link TaskContext} and thus can not be garbage collected until the end of the task.
 */
 
/**
 * The buffer size to use when writing spills using DiskBlockObjectWriter
 */
 
/**
 * Force this sorter to spill when there are this many elements in memory.
 */
 
/**
 * Memory pages that hold the records being sorted. The pages in this list are freed when
 * spilling, although in principle we could recycle these pages across spills (on the other hand,
 * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager
 * itself).
 */
 
","Field recordComparatorSupplier
Field fileBufferSizeBytes
Field numElementsForSpillThreshold
Field allocatedPages
",5,178
MutableURLClassLoader.java,19,15,1.2666666666666666,"
 * URL class loader that exposes the `addURL` method in URLClassLoader.
 ",,,,,,,1,71
EnumUtil.java,16,20,0.8,,,,,,,,1,0
ChildFirstURLClassLoader.java,20,39,0.5128205128205128,"
 * A mutable class loader that gives preference to its own URLs over the parent class loader
 * when loading classes and resources.
 ",,,,,,,2,129
ParentClassLoader.java,19,17,1.1176470588235294,"
 * A class loader which makes some protected methods in ClassLoader accessible.
 ",,,,,,,1,79
ReadAheadInputStream.java,71,292,0.24315068493150685,"
 * {@link InputStream} implementation which asynchronously reads ahead from the underlying input
 * stream when specified amount of data has been read from the current buffer. It does it by
 * maintaining two buffers - active buffer and read ahead buffer. Active buffer contains data
 * which should be returned when a read() call is issued. The read ahead buffer is used to
 * asynchronously read from the underlying input stream and once the current active buffer is
 * exhausted, we flip the two buffers so that we can start reading from the read ahead buffer
 * without being blocked in disk I/O.
 ","/**
 * Read data from underlyingInputStream to readAheadBuffer asynchronously.
 */
 
/**
 * flip the active and read ahead buffer
 */
 
/**
 * Internal skip function which should be called only from skip() api. The assumption is that
 * the stateChangeLock is already acquired in the caller before calling this function.
 */
 
","{
    stateChangeLock.lock();
    final byte[] arr = readAheadBuffer.array();
    try {
        if (endOfStream || readInProgress) {
            return;
        }
        checkReadException();
        readAheadBuffer.position(0);
        readAheadBuffer.flip();
        readInProgress = true;
    } finally {
        stateChangeLock.unlock();
    }
    executorService.execute(() -> {
        stateChangeLock.lock();
        try {
            if (isClosed) {
                readInProgress = false;
                return;
            }
            // Flip this so that the close method will not close the underlying input stream when we
            // are reading.
            isReading = true;
        } finally {
            stateChangeLock.unlock();
        }
        // Please note that it is safe to release the lock and read into the read ahead buffer
        // because either of following two conditions will hold - 1. The active buffer has
        // data available to read so the reader will not read from the read ahead buffer.
        // 2. This is the first time read is called or the active buffer is exhausted,
        // in that case the reader waits for this async read to complete.
        // So there is no race condition in both the situations.
        int read = 0;
        int off = 0, len = arr.length;
        Throwable exception = null;
        try {
            // try to fill the read ahead buffer.
            // if a reader is waiting, possibly return early.
            do {
                read = underlyingInputStream.read(arr, off, len);
                if (read <= 0)
                    break;
                off += read;
                len -= read;
            } while (len > 0 && !isWaiting.get());
        } catch (Throwable ex) {
            exception = ex;
            if (ex instanceof Error) {
                // `readException` may not be reported to the user. Rethrow Error to make sure at least
                // The user can see Error in UncaughtExceptionHandler.
                throw (Error) ex;
            }
        } finally {
            stateChangeLock.lock();
            readAheadBuffer.limit(off);
            if (read < 0 || (exception instanceof EOFException)) {
                endOfStream = true;
            } else if (exception != null) {
                readAborted = true;
                readException = exception;
            }
            readInProgress = false;
            signalAsyncReadComplete();
            stateChangeLock.unlock();
            closeUnderlyingInputStreamIfNecessary();
        }
    });
} 
{
    ByteBuffer temp = activeBuffer;
    activeBuffer = readAheadBuffer;
    readAheadBuffer = temp;
} 
{
    assert (stateChangeLock.isLocked());
    waitForAsyncReadComplete();
    if (isEndOfStream()) {
        return 0;
    }
    if (available() >= n) {
        // we can skip from the internal buffers
        int toSkip = (int) n;
        // We need to skip from both active buffer and read ahead buffer
        toSkip -= activeBuffer.remaining();
        // skipping from activeBuffer already handled.
        assert (toSkip > 0);
        activeBuffer.position(0);
        activeBuffer.flip();
        readAheadBuffer.position(toSkip + readAheadBuffer.position());
        swapBuffers();
        // Trigger async read to emptied read ahead buffer.
        readAsync();
        return n;
    } else {
        int skippedBytes = available();
        long toSkip = n - skippedBytes;
        activeBuffer.position(0);
        activeBuffer.flip();
        readAheadBuffer.position(0);
        readAheadBuffer.flip();
        long skippedFromInputStream = underlyingInputStream.skip(toSkip);
        readAsync();
        return skippedBytes + skippedFromInputStream;
    }
} 
","/**
 * Creates a <code>ReadAheadInputStream</code> with the specified buffer size and read-ahead
 * threshold
 *
 * @param inputStream The underlying input stream.
 * @param bufferSizeInBytes The buffer size.
 */
 
","{
    Preconditions.checkArgument(bufferSizeInBytes > 0, ""bufferSizeInBytes should be greater than 0, but the value is "" + bufferSizeInBytes);
    activeBuffer = ByteBuffer.allocate(bufferSizeInBytes);
    readAheadBuffer = ByteBuffer.allocate(bufferSizeInBytes);
    this.underlyingInputStream = inputStream;
    activeBuffer.flip();
    readAheadBuffer.flip();
} 
",,,7,588
NioBufferedFileInputStream.java,27,94,0.2872340425531915,"
 * {@link InputStream} implementation which uses direct buffer
 * to read a file to avoid extra copy of data between Java and
 * native memory which happens when using {@link java.io.BufferedInputStream}.
 * Unfortunately, this is not something already available in JDK,
 * {@code sun.nio.ch.ChannelInputStream} supports reading a file using nio,
 * but does not support buffering.
 ","/**
 * Checks weather data is left to be read from the input stream.
 * @return true if data is left, false otherwise
 */
 
","{
    if (!byteBuffer.hasRemaining()) {
        byteBuffer.clear();
        int nRead = 0;
        while (nRead == 0) {
            nRead = fileChannel.read(byteBuffer);
        }
        if (nRead < 0) {
            return false;
        }
        byteBuffer.flip();
    }
    return true;
} 
",,,,,6,371
SparkStageInfo.java,22,12,1.8333333333333333,"
 * Exposes information about Spark Stages.
 *
 * This interface is not designed to be implemented outside of Spark.  We may add additional methods
 * which may break binary compatibility with outside implementations.
 ",,,,,,,4,210
ApplicationStatus.java,16,9,1.7777777777777777,,,,,,,,1,0
TaskSorting.java,16,25,0.64,,,,,,,,1,0
StageStatus.java,16,12,1.3333333333333333,,,,,,,,1,0
TimeTrackingOutputStream.java,20,45,0.4444444444444444,"
 * Intercepts write calls and tracks total time spent writing in order to update shuffle write
 * metrics. Not thread safe.
 ",,,,,,,2,121
BytesToBytesMap.java,280,553,0.5063291139240507,"
 * An append-only hash map where keys and values are contiguous regions of bytes.
 *
 * This is backed by a power-of-2-sized hash table, using quadratic probing with triangular numbers,
 * which is guaranteed to exhaust the space.
 *
 * The map can support up to 2^29 keys. If the key cardinality is higher than this, you should
 * probably be using sorting instead of hashing for better cache locality.
 *
 * The key and values under the hood are stored together, in the following format:
 *   Bytes 0 to 4: len(k) (key length in bytes) + len(v) (value length in bytes) + 4
 *   Bytes 4 to 8: len(k)
 *   Bytes 8 to 8 + len(k): key data
 *   Bytes 8 + len(k) to 8 + len(k) + len(v): value data
 *   Bytes 8 + len(k) + len(v) to 8 + len(k) + len(v) + 8: pointer to next pair
 *
 * This means that the first four bytes store the entire record (key + value) length. This format
 * is compatible with {@link org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter},
 * so we can pass records from this map directly into the sorter to sort records in place.
 | This choice of page table size and page size means that we can address up to 500 gigabytes| of memory.| TODO: we're wasting 32 bits of space here; we can probably store fewer bits of the hashcode| and exploit word-alignment to use fewer bits to hold the address.  This might let us store| only one long per map entry, increasing the chance that this array will fit in cache at the| expense of maybe performing more lookups if we have hash collisions.  Say that we stored only| 27 bits of the hashcode and 37 bits of the address.  37 bits is enough to address 1 terabyte| of RAM given word-alignment.  If we use 13 bits of this for our page table, that gives us a| maximum page size of 2^24 * 8 = ~134 megabytes per page. This change will require us to store| full base addresses in the page table for off-heap mode so that we can reconstruct the full| absolute memory addresses.|
   * Handle returned by {@link BytesToBytesMap#lookup(Object, long, int)} function.
   ","/**
 * Returns the number of keys defined in the map.
 */
 
/**
 * Returns the number of values defined in the map. A key could have multiple values.
 */
 
/**
 * Returns an iterator for iterating over the entries of this map.
 *
 * For efficiency, all calls to `next()` will return the same {@link Location} object.
 *
 * If any other lookups or operations are performed on this map while iterating over it, including
 * `lookup()`, the behavior of the returned iterator is undefined.
 */
 
/**
 * Returns a thread safe iterator that iterates of the entries of this map.
 */
 
/**
 * Returns a destructive iterator for iterating over the entries of this map. It frees each page
 * as it moves onto next one. Notice: it is illegal to call any method on the map after
 * `destructiveIterator()` has been called.
 *
 * For efficiency, all calls to `next()` will return the same {@link Location} object.
 *
 * If any other lookups or operations are performed on this map while iterating over it, including
 * `lookup()`, the behavior of the returned iterator is undefined.
 */
 
/**
 * Looks up a key, and return a {@link Location} handle that can be used to test existence
 * and read/write values.
 *
 * This function always return the same {@link Location} instance to avoid object allocation.
 */
 
/**
 * Looks up a key, and return a {@link Location} handle that can be used to test existence
 * and read/write values.
 *
 * This function always return the same {@link Location} instance to avoid object allocation.
 */
 
/**
 * Looks up a key, and saves the result in provided `loc`.
 *
 * This is a thread-safe version of `lookup`, could be used by multiple threads.
 */
 
/**
 * This is only used for spilling
 */
 
/**
 * Find the next pair that has the same key as current one.
 */
 
/**
 * Returns the memory page that contains the current record.
 * This is only valid if this is returned by {@link BytesToBytesMap#iterator()}.
 */
 
/**
 * Returns true if the key is defined at this position, and false otherwise.
 */
 
/**
 * Returns the base object for key.
 */
 
/**
 * Returns the offset for key.
 */
 
/**
 * Returns the base object for value.
 */
 
/**
 * Returns the offset for value.
 */
 
/**
 * Returns the length of the key defined at this position.
 * Unspecified behavior if the key is not defined.
 */
 
/**
 * Returns the length of the value defined at this position.
 * Unspecified behavior if the key is not defined.
 */
 
/**
 * Append a new value for the key. This method could be called multiple times for a given key.
 * The return value indicates whether the put succeeded or whether it failed because additional
 * memory could not be acquired.
 * <p>
 * It is only valid to call this method immediately after calling `lookup()` using the same key.
 * </p>
 * <p>
 * The key and value must be word-aligned (that is, their sizes must be a multiple of 8).
 * </p>
 * <p>
 * After calling this method, calls to `get[Key|Value]Address()` and `get[Key|Value]Length`
 * will return information on the data stored by this `append` call.
 * </p>
 * <p>
 * As an example usage, here's the proper way to store a new key:
 * </p>
 * <pre>
 *   Location loc = map.lookup(keyBase, keyOffset, keyLength);
 *   if (!loc.isDefined()) {
 *     if (!loc.append(keyBase, keyOffset, keyLength, ...)) {
 *       // handle failure to grow map (by spilling, for example)
 *     }
 *   }
 * </pre>
 * <p>
 * Unspecified behavior if the key is not defined.
 * </p>
 *
 * @return true if the put() was successful and false if the put() failed because memory could
 *         not be acquired.
 */
 
/**
 * Acquire a new page from the memory manager.
 * @return whether there is enough space to allocate the new page.
 */
 
/**
 * Allocate new data structures for this map. When calling this outside of the constructor,
 * make sure to keep references to the old data structures so that you can free them.
 *
 * @param capacity the new map capacity
 */
 
/**
 * Free all allocated memory associated with this map, including the storage for keys and values
 * as well as the hash map array itself.
 *
 * This method is idempotent and can be called multiple times.
 */
 
/**
 * Returns the total amount of memory, in bytes, consumed by this map's managed structures.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * Returns the average number of probes per key lookup.
 */
 
/**
 * Returns the underline long[] of longArray.
 */
 
/**
 * Reset this map to initialized state.
 */
 
/**
 * Grows the size of the hash table and re-hash everything.
 */
 
","{
    return numKeys;
} 
{
    return numValues;
} 
{
    return new MapIterator(numValues, loc, false);
} 
{
    return new MapIterator(numValues, new Location(), false);
} 
{
    updatePeakMemoryUsed();
    return new MapIterator(numValues, loc, true);
} 
{
    safeLookup(keyBase, keyOffset, keyLength, loc, Murmur3_x86_32.hashUnsafeWords(keyBase, keyOffset, keyLength, 42));
    return loc;
} 
{
    safeLookup(keyBase, keyOffset, keyLength, loc, hash);
    return loc;
} 
{
    assert (longArray != null);
    numKeyLookups++;
    int pos = hash & mask;
    int step = 1;
    while (true) {
        numProbes++;
        if (longArray.get(pos * 2) == 0) {
            // This is a new key.
            loc.with(pos, hash, false);
            return;
        } else {
            long stored = longArray.get(pos * 2 + 1);
            if ((int) (stored) == hash) {
                // Full hash code matches.  Let's compare the keys for equality.
                loc.with(pos, hash, true);
                if (loc.getKeyLength() == keyLength) {
                    final boolean areEqual = ByteArrayMethods.arrayEquals(keyBase, keyOffset, loc.getKeyBase(), loc.getKeyOffset(), keyLength);
                    if (areEqual) {
                        return;
                    }
                }
            }
        }
        pos = (pos + step) & mask;
        step++;
    }
} 
{
    this.isDefined = true;
    this.memoryPage = null;
    baseObject = base;
    int uaoSize = UnsafeAlignedOffset.getUaoSize();
    keyOffset = offset + uaoSize;
    keyLength = UnsafeAlignedOffset.getSize(base, offset);
    valueOffset = offset + uaoSize + keyLength;
    valueLength = length - uaoSize - keyLength;
    return this;
} 
{
    assert isDefined;
    long nextAddr = Platform.getLong(baseObject, valueOffset + valueLength);
    if (nextAddr == 0) {
        return false;
    } else {
        updateAddressesAndSizes(nextAddr);
        return true;
    }
} 
{
    return this.memoryPage;
} 
{
    return isDefined;
} 
{
    assert (isDefined);
    return baseObject;
} 
{
    assert (isDefined);
    return keyOffset;
} 
{
    assert (isDefined);
    return baseObject;
} 
{
    assert (isDefined);
    return valueOffset;
} 
{
    assert (isDefined);
    return keyLength;
} 
{
    assert (isDefined);
    return valueLength;
} 
{
    assert (klen % 8 == 0);
    assert (vlen % 8 == 0);
    assert (longArray != null);
    if (numKeys == MAX_CAPACITY || // The map could be reused from last spill (because of no enough memory to grow),
    // then we don't try to grow again if hit the `growthThreshold`.
    !canGrowArray && numKeys >= growthThreshold) {
        return false;
    }
    // Here, we'll copy the data into our data pages. Because we only store a relative offset from
    // the key address instead of storing the absolute address of the value, the key and value
    // must be stored in the same memory page.
    // (8 byte key length) (key) (value) (8 byte pointer to next value)
    int uaoSize = UnsafeAlignedOffset.getUaoSize();
    final long recordLength = (2L * uaoSize) + klen + vlen + 8;
    if (currentPage == null || currentPage.size() - pageCursor < recordLength) {
        if (!acquireNewPage(recordLength + uaoSize)) {
            return false;
        }
    }
    // --- Append the key and value data to the current data page --------------------------------
    final Object base = currentPage.getBaseObject();
    long offset = currentPage.getBaseOffset() + pageCursor;
    final long recordOffset = offset;
    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);
    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);
    offset += (2L * uaoSize);
    Platform.copyMemory(kbase, koff, base, offset, klen);
    offset += klen;
    Platform.copyMemory(vbase, voff, base, offset, vlen);
    offset += vlen;
    // put this value at the beginning of the list
    Platform.putLong(base, offset, isDefined ? longArray.get(pos * 2) : 0);
    // --- Update bookkeeping data structures ----------------------------------------------------
    offset = currentPage.getBaseOffset();
    UnsafeAlignedOffset.putSize(base, offset, UnsafeAlignedOffset.getSize(base, offset) + 1);
    pageCursor += recordLength;
    final long storedKeyAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, recordOffset);
    longArray.set(pos * 2, storedKeyAddress);
    updateAddressesAndSizes(storedKeyAddress);
    numValues++;
    if (!isDefined) {
        numKeys++;
        longArray.set(pos * 2 + 1, keyHashcode);
        isDefined = true;
        if (numKeys >= growthThreshold && longArray.size() < MAX_CAPACITY) {
            try {
                growAndRehash();
            } catch (SparkOutOfMemoryError oom) {
                canGrowArray = false;
            }
        }
    }
    return true;
} 
{
    try {
        currentPage = allocatePage(required);
    } catch (SparkOutOfMemoryError e) {
        return false;
    }
    dataPages.add(currentPage);
    UnsafeAlignedOffset.putSize(currentPage.getBaseObject(), currentPage.getBaseOffset(), 0);
    pageCursor = UnsafeAlignedOffset.getUaoSize();
    return true;
} 
{
    assert (capacity >= 0);
    capacity = Math.max((int) Math.min(MAX_CAPACITY, ByteArrayMethods.nextPowerOf2(capacity)), 64);
    assert (capacity <= MAX_CAPACITY);
    longArray = allocateArray(capacity * 2L);
    longArray.zeroOut();
    this.growthThreshold = (int) (capacity * loadFactor);
    this.mask = capacity - 1;
} 
{
    updatePeakMemoryUsed();
    if (longArray != null) {
        freeArray(longArray);
        longArray = null;
    }
    Iterator<MemoryBlock> dataPagesIterator = dataPages.iterator();
    while (dataPagesIterator.hasNext()) {
        MemoryBlock dataPage = dataPagesIterator.next();
        dataPagesIterator.remove();
        freePage(dataPage);
    }
    assert (dataPages.isEmpty());
    while (!spillWriters.isEmpty()) {
        File file = spillWriters.removeFirst().getFile();
        if (file != null && file.exists()) {
            if (!file.delete()) {
                logger.error(""Was unable to delete spill file {}"", file.getAbsolutePath());
            }
        }
    }
} 
{
    long totalDataPagesSize = 0L;
    for (MemoryBlock dataPage : dataPages) {
        totalDataPagesSize += dataPage.size();
    }
    return totalDataPagesSize + ((longArray != null) ? longArray.memoryBlock().size() : 0L);
} 
{
    updatePeakMemoryUsed();
    return peakMemoryUsedBytes;
} 
{
    return (1.0 * numProbes) / numKeyLookups;
} 
{
    assert (longArray != null);
    return longArray;
} 
{
    updatePeakMemoryUsed();
    numKeys = 0;
    numValues = 0;
    freeArray(longArray);
    longArray = null;
    while (dataPages.size() > 0) {
        MemoryBlock dataPage = dataPages.removeLast();
        freePage(dataPage);
    }
    allocate(initialCapacity);
    canGrowArray = true;
    currentPage = null;
    pageCursor = 0;
} 
{
    assert (longArray != null);
    // Store references to the old data structures to be used when we re-hash
    final LongArray oldLongArray = longArray;
    final int oldCapacity = (int) oldLongArray.size() / 2;
    // Allocate the new data structures
    allocate(Math.min(growthStrategy.nextCapacity(oldCapacity), MAX_CAPACITY));
    // Re-mask (we don't recompute the hashcode because we stored all 32 bits of it)
    for (int i = 0; i < oldLongArray.size(); i += 2) {
        final long keyPointer = oldLongArray.get(i);
        if (keyPointer == 0) {
            continue;
        }
        final int hashcode = (int) oldLongArray.get(i + 1);
        int newPos = hashcode & mask;
        int step = 1;
        while (longArray.get(newPos * 2) != 0) {
            newPos = (newPos + step) & mask;
            step++;
        }
        longArray.set(newPos * 2, keyPointer);
        longArray.set(newPos * 2 + 1, hashcode);
    }
    freeArray(oldLongArray);
} 
",,,"/**
 * A linked list for tracking all allocated data pages so that we can free all of our memory.
 */
 
/**
 * The data page that will be used to store keys and values for new hashtable entries. When this
 * page becomes full, a new page will be allocated and this pointer will change to point to that
 * new page.
 */
 
/**
 * Offset into `currentPage` that points to the location where new data can be inserted into
 * the page. This does not incorporate the page's base offset.
 */
 
/**
 * The maximum number of keys that BytesToBytesMap supports. The hash table has to be
 * power-of-2-sized and its backing Java array can contain at most (1 &lt;&lt; 30) elements,
 * since that's the largest power-of-2 that's less than Integer.MAX_VALUE. We need two long array
 * entries per key, giving us a maximum capacity of (1 &lt;&lt; 29).
 */
 
/**
 * A single array to store the key and value.
 *
 * Position {@code 2 * i} in the array is used to track a pointer to the key at index {@code i},
 * while position {@code 2 * i + 1} in the array holds key's full 32-bit hashcode.
 */
 
/**
 * Whether or not the longArray can grow. We will not insert more elements if it's false.
 */
 
/**
 * The size of the data pages that hold key and value data. Map entries cannot span multiple
 * pages, so this limits the maximum entry size.
 */
 
/**
 * Number of keys defined in the map.
 */
 
/**
 * Number of values defined in the map. A key could have multiple values.
 */
 
/**
 * The map will be expanded once the number of keys exceeds this threshold.
 */
 
/**
 * Mask for truncating hashcodes so that they do not exceed the long array's size.
 * This is a strength reduction optimization; we're essentially performing a modulus operation,
 * but doing so with a bitmask because this is a power-of-2-sized hash map.
 */
 
/**
 * Return value of {@link BytesToBytesMap#lookup(Object, long, int)}.
 */
 
/**
 * An index into the hash map's Long array
 */
 
/**
 * True if this location points to a position where a key is defined, false otherwise
 */
 
/**
 * The hashcode of the most recent key passed to
 * {@link BytesToBytesMap#lookup(Object, long, int, int)}. Caching this hashcode here allows us
 * to avoid re-hashing the key when storing a value for that key.
 */
 
/**
 * Memory page containing the record. Only set if created by {@link BytesToBytesMap#iterator()}.
 */
 
","Field dataPages
Field currentPage
Field pageCursor
Field MAX_CAPACITY
Field longArray
Field canGrowArray
Field pageSizeBytes
Field numKeys
Field numValues
Field growthThreshold
Field mask
Field loc
Field pos
Field isDefined
Field keyHashcode
Field memoryPage
",20,1994
HashMapGrowthStrategy.java,23,15,1.5333333333333334,"
 * Interface that defines how we can grow the size of a hash map when it is over a threshold.
 ",,,,,"/**
 * Double the size of the hash map every time.
 */
 
","Field DOUBLING
",1,93
Optional.java,95,68,1.3970588235294117,"
 * <p>Like {@code java.util.Optional} in Java 8, {@code scala.Option} in Scala, and
 * {@code com.google.common.base.Optional} in Google Guava, this class represents a
 * value of a given type that may or may not exist. It is used in methods that wish
 * to optionally return a value, in preference to returning {@code null}.</p>
 *
 * <p>In fact, the class here is a reimplementation of the essential API of both
 * {@code java.util.Optional} and {@code com.google.common.base.Optional}. From
 * {@code java.util.Optional}, it implements:</p>
 *
 * <ul>
 *   <li>{@link #empty()}</li>
 *   <li>{@link #of(Object)}</li>
 *   <li>{@link #ofNullable(Object)}</li>
 *   <li>{@link #get()}</li>
 *   <li>{@link #orElse(Object)}</li>
 *   <li>{@link #isPresent()}</li>
 * </ul>
 *
 * <p>From {@code com.google.common.base.Optional} it implements:</p>
 *
 * <ul>
 *   <li>{@link #absent()}</li>
 *   <li>{@link #of(Object)}</li>
 *   <li>{@link #fromNullable(Object)}</li>
 *   <li>{@link #get()}</li>
 *   <li>{@link #or(Object)}</li>
 *   <li>{@link #orNull()}</li>
 *   <li>{@link #isPresent()}</li>
 * </ul>
 *
 * <p>{@code java.util.Optional} itself was not used because at the time, the
 * project did not require Java 8. Using {@code com.google.common.base.Optional}
 * has in the past caused serious library version conflicts with Guava that can't
 * be resolved by shading. Hence this work-alike clone.</p>
 *
 * @param <T> type of value held inside
 ","/**
 * @return an empty {@code Optional}
 */
 
/**
 * @param value non-null value to wrap
 * @return {@code Optional} wrapping this value
 * @throws NullPointerException if value is null
 */
 
/**
 * @param value value to wrap, which may be null
 * @return {@code Optional} wrapping this value, which may be empty
 */
 
/**
 * @return the value wrapped by this {@code Optional}
 * @throws NullPointerException if this is empty (contains no value)
 */
 
/**
 * @param other value to return if this is empty
 * @return this {@code Optional}'s value if present, or else the given value
 */
 
/**
 * @return true iff this {@code Optional} contains a value (non-empty)
 */
 
/**
 * @return an empty {@code Optional}
 */
 
/**
 * @param value value to wrap, which may be null
 * @return {@code Optional} wrapping this value, which may be empty
 */
 
/**
 * @param other value to return if this is empty
 * @return this {@code Optional}'s value if present, or else the given value
 */
 
/**
 * @return this {@code Optional}'s value if present, or else null
 */
 
","{
    @SuppressWarnings(""unchecked"")
    Optional<T> t = (Optional<T>) EMPTY;
    return t;
} 
{
    return new Optional<>(value);
} 
{
    if (value == null) {
        return empty();
    } else {
        return of(value);
    }
} 
{
    Preconditions.checkNotNull(value);
    return value;
} 
{
    return value != null ? value : other;
} 
{
    return value != null;
} 
{
    return empty();
} 
{
    return ofNullable(value);
} 
{
    return value != null ? value : other;
} 
{
    return value;
} 
",,,,,36,1382
JavaFutureAction.java,22,6,3.6666666666666665,,"/**
 * Returns the job IDs run by the underlying async operation.
 *
 * This returns the current snapshot of the job list. Certain operations may run multiple
 * jobs, so multiple calls to this method may return different lists.
 */
 
","jobIds 
",,,,,1,0
Function0.java,19,6,3.1666666666666665,"
 * A zero-argument function that returns an R.
 ",,,,,,,1,46
MapGroupsFunction.java,19,7,2.7142857142857144,"
 * Base interface for a map function used in GroupedDataset's mapGroup function.
 ",,,,,,,1,80
ForeachFunction.java,21,6,3.5,"
 * Base interface for a function used in Dataset's foreach function.
 *
 * Spark will invoke the call function on each element in the input Dataset.
 ",,,,,,,3,144
VoidFunction2.java,19,6,3.1666666666666665,"
 * A two-argument function that takes arguments of type T1 and T2 with no return value.
 ",,,,,,,1,87
PairFlatMapFunction.java,20,8,2.5,"
 * A function that returns zero or more key-value pair records from each input record. The
 * key-value pairs are represented as scala.Tuple2 objects.
 ",,,,,,,2,148
PairFunction.java,20,7,2.857142857142857,"
 * A function that returns key-value pairs (Tuple2&lt;K, V&gt;), and can be used to
 * construct PairRDDs.
 ",,,,,,,2,104
MapPartitionsFunction.java,19,7,2.7142857142857144,"
 * Base interface for function used in Dataset's mapPartitions.
 ",,,,,,,1,63
DoubleFunction.java,19,6,3.1666666666666665,"
 *  A function that returns Doubles, and can be used to construct DoubleRDDs.
 ",,,,,,,1,77
FlatMapFunction.java,20,8,2.5,"
 * A function that returns zero or more output records from each input record.
 ",,,,,,,1,78
FilterFunction.java,21,6,3.5,"
 * Base interface for a function used in Dataset's filter function.
 *
 * If the function returns true, the element is included in the returned Dataset.
 ",,,,,,,3,148
Function4.java,19,6,3.1666666666666665,"
 * A four-argument function that takes arguments of type T1, T2, T3 and T4 and returns an R.
 ",,,,,,,1,92
VoidFunction.java,19,6,3.1666666666666665,"
 * A function with no return value.
 ",,,,,,,1,35
ReduceFunction.java,19,6,3.1666666666666665,"
 * Base interface for function used in Dataset's reduce.
 ",,,,,,,1,56
CoGroupFunction.java,20,7,2.857142857142857,"
 * A function that returns zero or more output records from each grouping key and its values from 2
 * Datasets.
 ",,,,,,,2,110
FlatMapFunction2.java,19,7,2.7142857142857144,"
 * A function that takes two inputs and returns zero or more output records.
 ",,,,,,,1,76
DoubleFlatMapFunction.java,19,7,2.7142857142857144,"
 * A function that returns zero or more records of type Double from each input record.
 ",,,,,,,1,86
ForeachPartitionFunction.java,19,7,2.7142857142857144,"
 * Base interface for a function used in Dataset's foreachPartition function.
 ",,,,,,,1,77
Function2.java,19,7,2.7142857142857144,"
 * A two-argument function that takes arguments of type T1 and T2 and returns an R.
 ",,,,,,,1,83
MapFunction.java,20,8,2.5,"
 * Base interface for a map function used in Dataset's map function.
 ",,,,,,,1,68
Function.java,23,11,2.090909090909091,"
 * Base interface for functions whose return types do not create special RDDs. PairFunction and
 * DoubleFunction are handled separately, to allow PairRDDs and DoubleRDDs to be constructed
 * when mapping RDDs of other types.
 ",,,,,,,3,221
Function3.java,19,6,3.1666666666666665,"
 * A three-argument function that takes arguments of type T1, T2 and T3 and returns an R.
 ",,,,,,,1,89
FlatMapGroupsFunction.java,19,7,2.7142857142857144,"
 * A function that returns zero or more output records from each grouping key and its values.
 ",,,,,,,1,93
StorageLevels.java,27,24,1.125,"
 * Expose some commonly useful storage level constants.
 ","/**
 * Create a new StorageLevel object.
 * @param useDisk saved to disk, if true
 * @param useMemory saved to on-heap memory, if true
 * @param useOffHeap saved to off-heap memory, if true
 * @param deserialized saved as deserialized objects, if true
 * @param replication replication factor
 */
 
","{
    return StorageLevel.apply(useDisk, useMemory, useOffHeap, deserialized, replication);
} 
",,,,,1,55
SparkJobInfo.java,22,7,3.142857142857143,"
 * Exposes information about Spark Jobs.
 *
 * This interface is not designed to be implemented outside of Spark.  We may add additional methods
 * which may break binary compatibility with outside implementations.
 ",,,,,,,4,208
JobExecutionStatus.java,13,28,0.4642857142857143,,,,,,,,1,0
SparkFirehoseListener.java,25,114,0.21929824561403508,"
 * Class that allows users to receive all SparkListener events.
 * Users should override the onEvent method.
 *
 * This is a concrete Java class in order to ensure that we don't forget to update it when adding
 * new methods to SparkListener: forgetting to add a method will result in a compilation error (if
 * this was a concrete Scala class, default implementations of new event handlers would be inherited
 * from the SparkListener trait).
 ",,,,,,,7,431
EdgeActiveness.java,24,8,3.0,"
 * Criteria for filtering edges based on activeness. For internal use only.
 ",,,,,,,1,75
TripletFields.java,39,20,1.95,"
 * Represents a subset of the fields of an [[EdgeTriplet]] or [[EdgeContext]]. This allows the
 * system to populate only those fields for efficiency.
 ",,,"/**
 * Constructs a default TripletFields in which all fields are included.
 */
 
","{
    this(true, true, true);
} 
","/**
 * Indicates whether the source vertex attribute is included.
 */
 
/**
 * Indicates whether the destination vertex attribute is included.
 */
 
/**
 * Indicates whether the edge attribute is included.
 */
 
/**
 * None of the triplet fields are exposed.
 */
 
/**
 * Expose only the edge field and not the source or destination field.
 */
 
/**
 * Expose the source and edge fields but not the destination field. (Same as Src)
 */
 
/**
 * Expose the destination and edge fields but not the source field. (Same as Dst)
 */
 
/**
 * Expose all the fields (source, edge, and destination).
 */
 
","Field useSrc
Field useDst
Field useEdge
Field None
Field EdgeOnly
Field Src
Field Dst
Field All
",2,148
JavaLogisticRegressionSuite.java,21,110,0.19090909090909092,,,,,,,,1,0
JavaStreamingLogisticRegressionSuite.java,16,54,0.2962962962962963,,,,,,,,1,0
JavaNaiveBayesSuite.java,17,61,0.2786885245901639,,,,,,,,1,0
JavaSVMSuite.java,16,50,0.32,,,,,,,,1,0
JavaALSSuite.java,18,146,0.1232876712328767,,,,,,,,1,0
JavaDecisionTreeSuite.java,17,57,0.2982456140350877,,,,,,,,1,0
JavaRowMatrixSuite.java,16,20,0.8,,,,,,,,1,0
JavaMatricesSuite.java,16,123,0.13008130081300814,,,,,,,,1,0
JavaVectorsSuite.java,16,20,0.8,,,,,,,,1,0
JavaMLUtilsSuite.java,16,50,0.32,,,,,,,,1,0
JavaKMeansSuite.java,17,57,0.2982456140350877,,,,,,,,1,0
JavaLDASuite.java,33,128,0.2578125,,,,,,,,1,0
JavaStreamingKMeansSuite.java,16,54,0.2962962962962963,,,,,,,,1,0
JavaGaussianMixtureSuite.java,16,25,0.64,,,,,,,,1,0
JavaBisectingKMeansSuite.java,16,35,0.45714285714285713,,,,,,,,1,0
JavaFPGrowthSuite.java,18,65,0.27692307692307694,,,,,,,,1,0
JavaAssociationRulesSuite.java,16,18,0.8888888888888888,,,,,,,,1,0
JavaPrefixSpanSuite.java,18,63,0.2857142857142857,,,,,,,,1,0
JavaStatisticsSuite.java,17,85,0.2,,,,,,,,1,0
JavaRankingMetricsSuite.java,16,31,0.5161290322580645,,,,,,,,1,0
JavaTfIdfSuite.java,18,46,0.391304347826087,,,,,,,,1,0
JavaWord2VecSuite.java,16,35,0.45714285714285713,,,,,,,,1,0
JavaRandomRDDsSuite.java,17,229,0.07423580786026202," This is just a test generator, it always returns a string of 42",,,,,,,1,64
JavaRidgeRegressionSuite.java,17,60,0.2833333333333333,,,,,,,,1,0
JavaStreamingLinearRegressionSuite.java,16,53,0.3018867924528302,,,,,,,,1,0
JavaLassoSuite.java,17,46,0.3695652173913043,,,,,,,,1,0
JavaLinearRegressionSuite.java,18,63,0.2857142857142857,,,,,,,,1,0
JavaIsotonicRegressionSuite.java,16,43,0.37209302325581395,,,,,,,,1,0
SharedSparkSession.java,16,29,0.5517241379310345,,,,,,,,1,0
JavaGBTClassifierSuite.java,29,44,0.6590909090909091,,,,,,,,1,0
JavaLogisticRegressionSuite.java,21,110,0.19090909090909092,,,,,,,,1,0
JavaDecisionTreeClassifierSuite.java,30,39,0.7692307692307693,,,,,,,,1,0
JavaNaiveBayesSuite.java,17,61,0.2786885245901639,,,,,,,,1,0
JavaOneVsRestSuite.java,19,43,0.4418604651162791,,,,,,,,1,0
JavaMultilayerPerceptronClassifierSuite.java,16,33,0.48484848484848486,,,,,,,,1,0
JavaRandomForestClassifierSuite.java,30,66,0.45454545454545453,,,,,,,,1,0
JavaSQLDataTypesSuite.java,16,11,1.4545454545454546,,,,,,,,1,0
JavaAttributeGroupSuite.java,16,25,0.64,,,,,,,,1,0
JavaAttributeSuite.java,16,32,0.5,,,,,,,,1,0
JavaDefaultReadWriteSuite.java,17,40,0.425,,,,,,,,1,0
JavaLibSVMRelationSuite.java,19,42,0.4523809523809524,"
 * Test LibSVMRelation in Java.
 ",,,,,,,1,31
JavaKMeansSuite.java,17,57,0.2982456140350877,,,,,,,,1,0
JavaParamsSuite.java,19,26,0.7307692307692307,"
 * Test Param and related classes in Java
 ",,,,,,,1,41
JavaTestParams.java,19,80,0.2375,"
 * A subclass of Params for testing.
 ",,,,,,,1,36
JavaSummarizerSuite.java,16,38,0.42105263157894735,,,,,,,,1,0
JavaKolmogorovSmirnovTestSuite.java,22,40,0.55,,,,,,,,1,0
JavaCrossValidatorSuite.java,16,40,0.4,,,,,,,,1,0
JavaStandardScalerSuite.java,19,28,0.6785714285714286,,,,,,,,1,0
JavaNormalizerSuite.java,19,27,0.7037037037037037,,,,,,,,1,0
JavaPolynomialExpansionSuite.java,16,48,0.3333333333333333,,,,,,,,1,0
JavaHashingTFSuite.java,16,46,0.34782608695652173,,,,,,,,1,0
JavaPCASuite.java,16,68,0.23529411764705882,,,,,,,,1,0
JavaTokenizerSuite.java,16,32,0.5,,,,,,,,1,0
JavaVectorSlicerSuite.java,16,40,0.4,,,,,,,,1,0
JavaDCTSuite.java,16,35,0.45714285714285713,,,,,,,,1,0
JavaVectorIndexerSuite.java,17,31,0.5483870967741935,,,,,,,,1,0
JavaStringIndexerSuite.java,19,34,0.5588235294117647,,"/**
 * An alias for RowFactory.create.
 */
 
","{
    return RowFactory.create(values);
} 
",,,,,1,0
JavaStopWordsRemoverSuite.java,16,30,0.5333333333333333,,,,,,,,1,0
JavaWord2VecSuite.java,16,35,0.45714285714285713,,,,,,,,1,0
JavaVectorAssemblerSuite.java,16,38,0.42105263157894735,,,,,,,,1,0
JavaBucketizerSuite.java,16,67,0.23880597014925373,,,,,,,,1,0
JavaDecisionTreeRegressorSuite.java,29,40,0.725,,,,,,,,1,0
JavaLinearRegressionSuite.java,18,63,0.2857142857142857,,,,,,,,1,0
JavaRandomForestRegressorSuite.java,29,67,0.43283582089552236,,,,,,,,1,0
JavaGBTRegressorSuite.java,28,45,0.6222222222222222,,,,,,,,1,0
JavaPipelineSuite.java,19,35,0.5428571428571428,"
 * Test Pipeline construction and fitting in Java.
 ",,,,,,,1,50
JavaPackage.java,22,6,3.6666666666666665,"
 * A dummy class as a workaround to show the package doc of <code>spark.mllib</code> in generated
 * Java API docs.
 * @see <a href=""http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4492654"" target=""_blank"">
 *      JDK-4492654</a>
 ",,,,,,,4,228
JavaAPISuite.java,63,1363,0.046221570066030816," The test suite itself is Serializable so that anonymous Function implementations can be| serialized, as an alternative to converting these anonymous classes to static inner classes;| see http://stackoverflow.com/questions/758570/.",,,,,,,1,231
Java8APISuite.java,31,714,0.04341736694677871,"
 * Most of these tests replicate org.apache.spark.streaming.JavaAPISuite using java 8
 * lambda syntax.
 ","/**
 * This test is only for testing the APIs. It's not necessary to run it.
 */
 
","{
    JavaPairRDD<String, Boolean> initialRDD = null;
    JavaPairDStream<String, Integer> wordsDstream = null;
    Function4<Time, String, Optional<Integer>, State<Boolean>, Optional<Double>> mapFn = (time, key, value, state) -> {
        // Use all State's methods here
        state.exists();
        state.get();
        state.isTimingOut();
        state.remove();
        state.update(true);
        return Optional.of(2.0);
    };
    JavaMapWithStateDStream<String, Integer, Boolean, Double> stateDstream = wordsDstream.mapWithState(StateSpec.function(mapFn).initialState(initialRDD).numPartitions(10).partitioner(new HashPartitioner(10)).timeout(Durations.seconds(10)));
    JavaPairDStream<String, Boolean> emittedRecords = stateDstream.stateSnapshots();
    Function3<String, Optional<Integer>, State<Boolean>, Double> mapFn2 = (key, value, state) -> {
        state.exists();
        state.get();
        state.isTimingOut();
        state.remove();
        state.update(true);
        return 2.0;
    };
    JavaMapWithStateDStream<String, Integer, Boolean, Double> stateDstream2 = wordsDstream.mapWithState(StateSpec.function(mapFn2).initialState(initialRDD).numPartitions(10).partitioner(new HashPartitioner(10)).timeout(Durations.seconds(10)));
    JavaPairDStream<String, Boolean> mappedDStream = stateDstream2.stateSnapshots();
} 
",,,,,2,101
LocalJavaStreamingContext.java,16,22,0.7272727272727273,,,,,,,,1,0
JavaMapWithStateSuite.java,21,126,0.16666666666666666,,"/**
 * This test is only for testing the APIs. It's not necessary to run it.
 */
 
","{
    JavaPairRDD<String, Boolean> initialRDD = null;
    JavaPairDStream<String, Integer> wordsDstream = null;
    Function4<Time, String, Optional<Integer>, State<Boolean>, Optional<Double>> mappingFunc = (time, word, one, state) -> {
        // Use all State's methods here
        state.exists();
        state.get();
        state.isTimingOut();
        state.remove();
        state.update(true);
        return Optional.of(2.0);
    };
    JavaMapWithStateDStream<String, Integer, Boolean, Double> stateDstream = wordsDstream.mapWithState(StateSpec.function(mappingFunc).initialState(initialRDD).numPartitions(10).partitioner(new HashPartitioner(10)).timeout(Durations.seconds(10)));
    stateDstream.stateSnapshots();
    Function3<String, Optional<Integer>, State<Boolean>, Double> mappingFunc2 = (key, one, state) -> {
        // Use all State's methods here
        state.exists();
        state.get();
        state.isTimingOut();
        state.remove();
        state.update(true);
        return 2.0;
    };
    JavaMapWithStateDStream<String, Integer, Boolean, Double> stateDstream2 = wordsDstream.mapWithState(StateSpec.function(mappingFunc2).initialState(initialRDD).numPartitions(10).partitioner(new HashPartitioner(10)).timeout(Durations.seconds(10)));
    stateDstream2.stateSnapshots();
} 
",,,,,1,0
JavaStreamingListenerAPISuite.java,16,66,0.24242424242424243,,,,,,,,1,0
JavaDurationSuite.java,18,49,0.3673469387755102,,,,,,,,1,0
JavaTimeSuite.java,18,33,0.5454545454545454,,,,,,,,1,0
JavaWriteAheadLogSuite.java,17,90,0.18888888888888888,,,,,,,,1,0
JavaReceiverAPISuite.java,16,103,0.1553398058252427,,,,,,,,1,0
BatchStatus.java,16,10,1.6,,,,,,,,1,0
StreamingContextState.java,32,8,4.0,"
 * :: DeveloperApi ::
 *
 * Represents the state of a StreamingContext.
 ",,,,,,,3,67
WriteAheadLog.java,44,11,4.0,"
 * :: DeveloperApi ::
 *
 * This abstract class represents a write ahead log (aka journal) that is used by Spark Streaming
 * to save the received data (by receivers) and associated metadata to a reliable storage, so that
 * they can be recovered after driver failures. See the Spark documentation for more information
 * on how to plug in your own custom implementation of a write ahead log.
 ","/**
 * Write the record to the log and return a record handle, which contains all the information
 * necessary to read back the written record. The time is used to the index the record,
 * such that it can be cleaned later. Note that implementations of this abstract class must
 * ensure that the written data is durable and readable (using the record handle) by the
 * time this function returns.
 */
 
/**
 * Read a written record based on the given record handle.
 */
 
/**
 * Read and return an iterator of all the records that have been written but not yet cleaned up.
 */
 
/**
 * Clean all the records that are older than the threshold time. It can wait for
 * the completion of the deletion.
 */
 
/**
 * Close this log and release any resources. It must be idempotent.
 */
 
","write 
read 
readAll 
clean 
close 
",,,,,6,382
WriteAheadLogRecordHandle.java,26,4,6.5,"
 * :: DeveloperApi ::
 *
 * This abstract class represents a handle that refers to a record written in a
 * {@link org.apache.spark.streaming.util.WriteAheadLog WriteAheadLog}.
 * It must contain all the information necessary for the record to be read and returned by
 * an implementation of the WriteAheadLog class.
 *
 * @see org.apache.spark.streaming.util.WriteAheadLog
 ",,,,,,,8,359
HadoopConfigProvider.java,17,28,0.6071428571428571," Use the Hadoop configuration to obtain config values. ",,,,,,,1,55
YarnShuffleService.java,92,288,0.3194444444444444,"
 * An external shuffle service used by Spark on Yarn.
 *
 * This is intended to be a long-running auxiliary service that runs in the NodeManager process.
 * A Spark application may connect to this service by setting `spark.shuffle.service.enabled`.
 * The application also automatically derives the service port through `spark.shuffle.service.port`
 * specified in the Yarn configuration. This is so that both the clients and the server agree on
 * the same port to communicate on.
 *
 * The service also optionally supports authentication. This ensures that executors from one
 * application cannot read the shuffle files written by those from another. This feature can be
 * enabled by setting `spark.authenticate` in the Yarn configuration before starting the NM.
 * Note that the Spark application must also set `spark.authenticate` manually and, unlike in
 * the case of the service port, will not inherit this setting from the Yarn configuration. This
 * is because an application running on the same Yarn cluster may choose to not use the external
 * shuffle service, in which case its setting of `spark.authenticate` should be independent of
 * the service's.
 | just for integration tests that want to look at this file -- in general not sensible as| An entity that manages the shuffle secret per application|
   * Simply encodes an application ID.
   ","/**
 * Return whether authentication is enabled as specified by the configuration.
 * If so, fetch requests will fail unless the appropriate authentication secret
 * for the application is provided.
 */
 
/**
 * Start the shuffle server with the given configuration.
 */
 
/**
 * Close the shuffle server to clean up any associated state.
 */
 
/**
 * Set the recovery path for shuffle service recovery when NM is restarted. This will be call
 * by NM if NM recovery is enabled.
 */
 
/**
 * Get the path specific to this auxiliary service to use for recovery.
 */
 
/**
 * Figure out the recovery path and handle moving the DB if YARN NM recovery gets enabled
 * and DB exists in the local dir of NM by old version of shuffle service.
 */
 
","{
    return secretManager != null;
} 
{
    _conf = conf;
    boolean stopOnFailure = conf.getBoolean(STOP_ON_FAILURE_KEY, DEFAULT_STOP_ON_FAILURE);
    try {
        // In case this NM was killed while there were running spark applications, we need to restore
        // lost state for the existing executors. We look for an existing file in the NM's local dirs.
        // If we don't find one, then we choose a file to use to save the state next time.  Even if
        // an application was stopped while the NM was down, we expect yarn to call stopApplication()
        // when it comes back
        if (_recoveryPath != null) {
            registeredExecutorFile = initRecoveryDb(RECOVERY_FILE_NAME);
        }
        TransportConf transportConf = new TransportConf(""shuffle"", new HadoopConfigProvider(conf));
        blockHandler = new ExternalBlockHandler(transportConf, registeredExecutorFile);
        // If authentication is enabled, set up the shuffle server to use a
        // special RPC handler that filters out unauthenticated fetch requests
        List<TransportServerBootstrap> bootstraps = Lists.newArrayList();
        boolean authEnabled = conf.getBoolean(SPARK_AUTHENTICATE_KEY, DEFAULT_SPARK_AUTHENTICATE);
        if (authEnabled) {
            secretManager = new ShuffleSecretManager();
            if (_recoveryPath != null) {
                loadSecretsFromDb();
            }
            bootstraps.add(new AuthServerBootstrap(transportConf, secretManager));
        }
        int port = conf.getInt(SPARK_SHUFFLE_SERVICE_PORT_KEY, DEFAULT_SPARK_SHUFFLE_SERVICE_PORT);
        transportContext = new TransportContext(transportConf, blockHandler);
        shuffleServer = transportContext.createServer(port, bootstraps);
        // the port should normally be fixed, but for tests its useful to find an open port
        port = shuffleServer.getPort();
        boundPort = port;
        String authEnabledString = authEnabled ? ""enabled"" : ""not enabled"";
        // register metrics on the block handler into the Node Manager's metrics system.
        blockHandler.getAllMetrics().getMetrics().put(""numRegisteredConnections"", shuffleServer.getRegisteredConnections());
        YarnShuffleServiceMetrics serviceMetrics = new YarnShuffleServiceMetrics(blockHandler.getAllMetrics());
        MetricsSystemImpl metricsSystem = (MetricsSystemImpl) DefaultMetricsSystem.instance();
        metricsSystem.register(""sparkShuffleService"", ""Metrics on the Spark Shuffle Service"", serviceMetrics);
        logger.info(""Registered metrics with Hadoop's DefaultMetricsSystem"");
        logger.info(""Started YARN shuffle service for Spark on port {}. "" + ""Authentication is {}.  Registered executor file is {}"", port, authEnabledString, registeredExecutorFile);
    } catch (Exception e) {
        if (stopOnFailure) {
            throw e;
        } else {
            noteFailure(e);
        }
    }
} 
{
    try {
        if (shuffleServer != null) {
            shuffleServer.close();
        }
        if (transportContext != null) {
            transportContext.close();
        }
        if (blockHandler != null) {
            blockHandler.close();
        }
        if (db != null) {
            db.close();
        }
    } catch (Exception e) {
        logger.error(""Exception when stopping service"", e);
    }
} 
{
    _recoveryPath = recoveryPath;
} 
{
    return _recoveryPath;
} 
{
    Preconditions.checkNotNull(_recoveryPath, ""recovery path should not be null if NM recovery is enabled"");
    File recoveryFile = new File(_recoveryPath.toUri().getPath(), dbName);
    if (recoveryFile.exists()) {
        return recoveryFile;
    }
    // db doesn't exist in recovery path go check local dirs for it
    String[] localDirs = _conf.getTrimmedStrings(""yarn.nodemanager.local-dirs"");
    for (String dir : localDirs) {
        File f = new File(new Path(dir).toUri().getPath(), dbName);
        if (f.exists()) {
            // If the recovery path is set then either NM recovery is enabled or another recovery
            // DB has been initialized. If NM recovery is enabled and had set the recovery path
            // make sure to move all DBs to the recovery path from the old NM local dirs.
            // If another DB was initialized first just make sure all the DBs are in the same
            // location.
            Path newLoc = new Path(_recoveryPath, dbName);
            Path copyFrom = new Path(f.toURI());
            if (!newLoc.equals(copyFrom)) {
                logger.info(""Moving "" + copyFrom + "" to: "" + newLoc);
                try {
                    // The move here needs to handle moving non-empty directories across NFS mounts
                    FileSystem fs = FileSystem.getLocal(_conf);
                    fs.rename(copyFrom, newLoc);
                } catch (Exception e) {
                    // Fail to move recovery file to new path, just continue on with new DB location
                    logger.error(""Failed to move recovery file {} to the path {}"", dbName, _recoveryPath.toString(), e);
                }
            }
            return new File(newLoc.toUri().getPath());
        }
    }
    return new File(_recoveryPath.toUri().getPath(), dbName);
} 
",,,,,18,1326
YarnShuffleServiceMetrics.java,32,100,0.32,"
 * Forward {@link org.apache.spark.network.shuffle.ExternalBlockHandler.ShuffleMetrics}
 * to hadoop metrics system.
 * NodeManager by default exposes JMX endpoint where can be collected.
 ","/**
 * Get metrics from the source
 *
 * @param collector to contain the resulting metrics snapshot
 * @param all       if true, return all metrics even if unchanged.
 */
 
/**
 * The metric types used in
 * {@link org.apache.spark.network.shuffle.ExternalBlockHandler.ShuffleMetrics}.
 * Visible for testing.
 */
 
","{
    MetricsRecordBuilder metricsRecordBuilder = collector.addRecord(""sparkShuffleService"");
    for (Map.Entry<String, Metric> entry : metricSet.getMetrics().entrySet()) {
        collectMetric(metricsRecordBuilder, entry.getKey(), entry.getValue());
    }
} 
{
    if (metric instanceof Timer) {
        Timer t = (Timer) metric;
        metricsRecordBuilder.addCounter(new ShuffleServiceMetricsInfo(name + ""_count"", ""Count of timer "" + name), t.getCount()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate15"", ""15 minute rate of timer "" + name), t.getFifteenMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate5"", ""5 minute rate of timer "" + name), t.getFiveMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate1"", ""1 minute rate of timer "" + name), t.getOneMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rateMean"", ""Mean rate of timer "" + name), t.getMeanRate());
    } else if (metric instanceof Meter) {
        Meter m = (Meter) metric;
        metricsRecordBuilder.addCounter(new ShuffleServiceMetricsInfo(name + ""_count"", ""Count of meter "" + name), m.getCount()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate15"", ""15 minute rate of meter "" + name), m.getFifteenMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate5"", ""5 minute rate of meter "" + name), m.getFiveMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rate1"", ""1 minute rate of meter "" + name), m.getOneMinuteRate()).addGauge(new ShuffleServiceMetricsInfo(name + ""_rateMean"", ""Mean rate of meter "" + name), m.getMeanRate());
    } else if (metric instanceof Gauge) {
        final Object gaugeValue = ((Gauge) metric).getValue();
        if (gaugeValue instanceof Integer) {
            metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfoForGauge(name), (Integer) gaugeValue);
        } else if (gaugeValue instanceof Long) {
            metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfoForGauge(name), (Long) gaugeValue);
        } else if (gaugeValue instanceof Float) {
            metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfoForGauge(name), (Float) gaugeValue);
        } else if (gaugeValue instanceof Double) {
            metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfoForGauge(name), (Double) gaugeValue);
        } else {
            throw new IllegalStateException(""Not supported class type of metric["" + name + ""] for value "" + gaugeValue);
        }
    } else if (metric instanceof Counter) {
        Counter c = (Counter) metric;
        long counterValue = c.getCount();
        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfoForCounter(name), counterValue);
    }
} 
",,,,,3,183
AuthIntegrationSuite.java,22,193,0.11398963730569948,,,,,,,,1,0
AuthMessagesSuite.java,16,50,0.32,,,,,,,,1,0
AuthEngineSuite.java,20,155,0.12903225806451613,,,,,,,,1,0
TestUtils.java,16,23,0.6956521739130435,,,,,,,,1,0
TestManagedBuffer.java,21,72,0.2916666666666667,"
 * A ManagedBuffer implementation that contains 0, 1, 2, 3, ..., (len-1).
 *
 * Used for testing.
 ",,,,,,,3,93
TransportFrameDecoderSuite.java,24,248,0.0967741935483871,,"/**
 * Creates a number of randomly sized frames and feed them to the given decoder, verifying
 * that the frames were read.
 */
 
","{
    ByteBuf data = Unpooled.buffer();
    for (int i = 0; i < frameCount; i++) {
        byte[] frame = new byte[1024 * (RND.nextInt(31) + 1)];
        data.writeLong(frame.length + 8);
        data.writeBytes(frame);
    }
    try {
        while (data.isReadable()) {
            int size = RND.nextInt(4 * 1024) + 256;
            decoder.channelRead(ctx, data.readSlice(Math.min(data.readableBytes(), size)).retain());
        }
        verify(ctx, times(frameCount)).fireChannelRead(any(ByteBuf.class));
    } catch (Exception e) {
        release(data);
        throw e;
    }
    return data;
} 
",,,,,1,0
CryptoUtilsSuite.java,16,24,0.6666666666666666,,,,,,,,1,0
NettyMemoryMetricsSuite.java,18,131,0.13740458015267176,,,,,,,,1,0
ChunkFetchRequestHandlerSuite.java,18,78,0.23076923076923078,,,,,,,,1,0
ChunkFetchIntegrationSuite.java,16,201,0.07960199004975124,,,,,,,,1,0
SparkSaslSuite.java,35,366,0.09562841530054644,"
 * Jointly tests SparkSaslClient and SparkSaslServer, as both are black boxes.
 ",,,,,"/**
 * Provides a secret key holder which returns secret key == appId
 */
 
","Field secretKeyHolder
",1,78
OneForOneStreamManagerSuite.java,20,64,0.3125,,,,,,,,1,0
MessageWithHeaderSuite.java,22,135,0.16296296296296298,,"/**
 * Test writing a {@link MessageWithHeader} using the given {@link ByteBuf} as header.
 *
 * @param header the header to use.
 * @throws Exception thrown on error.
 */
 
","{
    long expectedHeaderValue = header.getLong(header.readerIndex());
    ByteBuf bodyPassedToNettyManagedBuffer = Unpooled.copyLong(84);
    assertEquals(1, header.refCnt());
    assertEquals(1, bodyPassedToNettyManagedBuffer.refCnt());
    ManagedBuffer managedBuf = new NettyManagedBuffer(bodyPassedToNettyManagedBuffer);
    Object body = managedBuf.convertToNetty();
    assertEquals(2, bodyPassedToNettyManagedBuffer.refCnt());
    assertEquals(1, header.refCnt());
    MessageWithHeader msg = new MessageWithHeader(managedBuf, header, body, managedBuf.size());
    ByteBuf result = doWrite(msg, 1);
    assertEquals(msg.count(), result.readableBytes());
    assertEquals(expectedHeaderValue, result.readLong());
    assertEquals(84, result.readLong());
    assertTrue(msg.release());
    assertEquals(0, bodyPassedToNettyManagedBuffer.refCnt());
    assertEquals(0, header.refCnt());
} 
",,,,,1,0
RpcIntegrationSuite.java,22,397,0.055415617128463476,,,,,,,,1,0
RequestTimeoutIntegrationSuite.java,45,198,0.22727272727272727,"
 * Suite which ensures that requests that go without a response for the network timeout period are
 * failed, and the connection closed.
 *
 * In this suite, we use 10 seconds as the connection timeout, with some slack given in the tests,
 * to ensure stability in different test environments.
 | A timeout will cause the connection to be closed, invalidating the current TransportClient.| The timeout is relative to the LAST request sent, which is kinda weird, but still.|
   * Callback which sets 'success' or 'failure' on completion.
   * Additionally notifies all waiters on this callback when invoked.
   ",,,,,,,8,595
ProtocolSuite.java,23,84,0.27380952380952384,"
   * Handler to transform a FileRegion into a byte buffer. EmbeddedChannel doesn't actually transfer
   * bytes, but messages, so this is needed so that the frame decoder on the receiving side can
   * understand what MessageWithHeader actually contains.
   ",,,,,,,3,252
TransportClientFactorySuite.java,25,173,0.14450867052023122,,"/**
 * Request a bunch of clients to a single server to test
 * we create up to maxConnections of clients.
 *
 * If concurrent is true, create multiple threads to create clients in parallel.
 */
 
","{
    Map<String, String> configMap = new HashMap<>();
    configMap.put(""spark.shuffle.io.numConnectionsPerPeer"", Integer.toString(maxConnections));
    TransportConf conf = new TransportConf(""shuffle"", new MapConfigProvider(configMap));
    RpcHandler rpcHandler = new NoOpRpcHandler();
    try (TransportContext context = new TransportContext(conf, rpcHandler)) {
        TransportClientFactory factory = context.createClientFactory();
        Set<TransportClient> clients = Collections.synchronizedSet(new HashSet<>());
        AtomicInteger failed = new AtomicInteger();
        Thread[] attempts = new Thread[maxConnections * 10];
        // Launch a bunch of threads to create new clients.
        for (int i = 0; i < attempts.length; i++) {
            attempts[i] = new Thread(() -> {
                try {
                    TransportClient client = factory.createClient(TestUtils.getLocalHost(), server1.getPort());
                    assertTrue(client.isActive());
                    clients.add(client);
                } catch (IOException e) {
                    failed.incrementAndGet();
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            });
            if (concurrent) {
                attempts[i].start();
            } else {
                attempts[i].run();
            }
        }
        // Wait until all the threads complete.
        for (Thread attempt : attempts) {
            attempt.join();
        }
        Assert.assertEquals(0, failed.get());
        Assert.assertTrue(clients.size() <= maxConnections);
        for (TransportClient client : clients) {
            client.close();
        }
        factory.close();
    }
} 
",,,,,1,0
TransportResponseHandlerSuite.java,18,124,0.14516129032258066,,,,,,,,1,0
ExtendedChannelPromise.java,17,42,0.40476190476190477,,,,,,,,1,0
StreamTestHelper.java,16,75,0.21333333333333335,,,,,,,,1,0
TransportRequestHandlerSuite.java,20,75,0.26666666666666666,,,,,,,,1,0
StreamSuite.java,17,262,0.0648854961832061,,,,,,,,1,0
TransportCipher.java,39,275,0.14181818181818182,"
 * Cipher for encryption and decryption.
 ","/**
 * The IV for the input channel (i.e. output channel of the remote side).
 */
 
/**
 * The IV for the output channel (i.e. input channel of the remote side).
 */
 
/**
 * Add handlers to channel.
 *
 * @param ch the channel for adding handlers
 * @throws IOException
 */
 
/**
 * SPARK-25535. Workaround for CRYPTO-141. Avoid further interaction with the underlying cipher
 * after an error occurs.
 */
 
","{
    return inIv;
} 
{
    return outIv;
} 
{
    ch.pipeline().addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(this)).addFirst(DECRYPTION_HANDLER_NAME, new DecryptionHandler(this));
} 
{
    this.isCipherValid = false;
} 
",,,,,1,40
AuthRpcHandler.java,35,123,0.2845528455284553,"
 * RPC Handler which performs authentication using Spark's auth protocol before delegating to a
 * child RPC handler. If the configuration allows, this handler will delegate messages to a SASL
 * RPC handler for further authentication, to support for clients that do not support Spark's
 * protocol.
 *
 * The delegate will only receive messages if the given connection has been successfully
 * authenticated. A connection may be authenticated at most once.
 ",,,,,"/**
 * Transport configuration.
 */
 
/**
 * The client channel.
 */
 
/**
 * RpcHandler we will delegate to for authenticated connections. When falling back to SASL
 * this will be replaced with the SASL RPC handler.
 */
 
/**
 * Class which provides secret keys which are shared by server and client on a per-app basis.
 */
 
/**
 * Whether auth is done and future calls should be delegated.
 */
 
","Field conf
Field channel
Field delegate
Field secretKeyHolder
Field doDelegate
",7,445
AuthClientBootstrap.java,36,72,0.5,"
 * Bootstraps a {@link TransportClient} by performing authentication using Spark's auth protocol.
 *
 * This bootstrap falls back to using the SASL bootstrap if the server throws an error during
 * authentication, and the configuration allows it. This is used for backwards compatibility
 * with external shuffle services that do not support the new protocol.
 *
 * It also automatically falls back to SASL if the new encryption backend is disabled, so that
 * callers only need to install this bootstrap when authentication is enabled.
 ",,,,,,,8,522
AuthEngine.java,52,216,0.24074074074074073,"
 * A helper class for abstracting authentication and key negotiation details. This is used by
 * both client and server sides, since the operations are basically the same.
 ","/**
 * Create the client challenge.
 *
 * @return A challenge to be sent the remote side.
 */
 
/**
 * Validates the client challenge, and create the encryption backend for the channel from the
 * parameters sent by the client.
 *
 * @param clientChallenge The challenge from the client.
 * @return A response to be sent to the client.
 */
 
/**
 * Validates the server response and initializes the cipher to use for the session.
 *
 * @param serverResponse The response from the server.
 */
 
/**
 * Validates an encrypted challenge as defined in the protocol, and returns the byte array
 * that corresponds to the actual challenge data.
 */
 
/**
 * Checks that the ""test"" array is in the data array starting at the given offset.
 */
 
","{
    this.authNonce = randomBytes(conf.encryptionKeyLength() / Byte.SIZE);
    SecretKeySpec authKey = generateKey(conf.keyFactoryAlgorithm(), conf.keyFactoryIterations(), authNonce, conf.encryptionKeyLength());
    initializeForAuth(conf.cipherTransformation(), authNonce, authKey);
    this.challenge = randomBytes(conf.encryptionKeyLength() / Byte.SIZE);
    return new ClientChallenge(new String(appId, UTF_8), conf.keyFactoryAlgorithm(), conf.keyFactoryIterations(), conf.cipherTransformation(), conf.encryptionKeyLength(), authNonce, challenge(appId, authNonce, challenge));
} 
{
    SecretKeySpec authKey = generateKey(clientChallenge.kdf, clientChallenge.iterations, clientChallenge.nonce, clientChallenge.keyLength);
    initializeForAuth(clientChallenge.cipher, clientChallenge.nonce, authKey);
    byte[] challenge = validateChallenge(clientChallenge.nonce, clientChallenge.challenge);
    byte[] response = challenge(appId, clientChallenge.nonce, rawResponse(challenge));
    byte[] sessionNonce = randomBytes(conf.encryptionKeyLength() / Byte.SIZE);
    byte[] inputIv = randomBytes(conf.ivLength());
    byte[] outputIv = randomBytes(conf.ivLength());
    SecretKeySpec sessionKey = generateKey(clientChallenge.kdf, clientChallenge.iterations, sessionNonce, clientChallenge.keyLength);
    this.sessionCipher = new TransportCipher(cryptoConf, clientChallenge.cipher, sessionKey, inputIv, outputIv);
    // Note the IVs are swapped in the response.
    return new ServerResponse(response, encrypt(sessionNonce), encrypt(outputIv), encrypt(inputIv));
} 
{
    byte[] response = validateChallenge(authNonce, serverResponse.response);
    byte[] expected = rawResponse(challenge);
    Preconditions.checkArgument(Arrays.equals(expected, response));
    byte[] nonce = decrypt(serverResponse.nonce);
    byte[] inputIv = decrypt(serverResponse.inputIv);
    byte[] outputIv = decrypt(serverResponse.outputIv);
    SecretKeySpec sessionKey = generateKey(conf.keyFactoryAlgorithm(), conf.keyFactoryIterations(), nonce, conf.encryptionKeyLength());
    this.sessionCipher = new TransportCipher(cryptoConf, conf.cipherTransformation(), sessionKey, inputIv, outputIv);
} 
{
    byte[] challenge = decrypt(encryptedChallenge);
    checkSubArray(appId, challenge, 0);
    checkSubArray(nonce, challenge, appId.length);
    return Arrays.copyOfRange(challenge, appId.length + nonce.length, challenge.length);
} 
{
    Preconditions.checkArgument(data.length >= test.length + offset);
    for (int i = 0; i < test.length; i++) {
        Preconditions.checkArgument(test[i] == data[i + offset]);
    }
} 
",,,,,2,169
AuthServerBootstrap.java,24,22,1.0909090909090908,"
 * A bootstrap which is executed on a TransportServer's client channel once a client connects
 * to the server, enabling authentication using Spark's auth protocol (and optionally SASL for
 * clients that don't support the new protocol).
 *
 * It also automatically falls back to SASL if the new encryption backend is disabled, so that
 * callers only need to install this bootstrap when authentication is enabled.
 ",,,,,,,6,404
ClientChallenge.java,22,66,0.3333333333333333,"
 * The client challenge message, used to initiate authentication.
 *
 * Please see crypto/README.md for more details of implementation.
 ",,,,,"/**
 * Serialization tag used to catch incorrect payloads.
 */
 
","Field TAG_BYTE
",3,131
ServerResponse.java,22,50,0.44,"
 * Server's response to client's challenge.
 *
 * Please see crypto/README.md for more details.
 ",,,,,"/**
 * Serialization tag used to catch incorrect payloads.
 */
 
","Field TAG_BYTE
",3,91
IOMode.java,21,4,5.25,"
 * Selector for which form of low-level IO we should use.
 * NIO is always available, while EPOLL is only available on Linux.
 * AUTO is used to select EPOLL if it's available, or NIO otherwise.
 ",,,,,,,3,190
CryptoUtils.java,26,17,1.5294117647058822,"
 * Utility methods related to the commons-crypto library.
 ","/**
 * Extract the commons-crypto configuration embedded in a list of config values.
 *
 * @param prefix Prefix in the given configuration that identifies the commons-crypto configs.
 * @param conf List of configuration values.
 */
 
","{
    Properties props = new Properties();
    for (Map.Entry<String, String> e : conf) {
        String key = e.getKey();
        if (key.startsWith(prefix)) {
            props.setProperty(COMMONS_CRYPTO_CONFIG_PREFIX + key.substring(prefix.length()), e.getValue());
        }
    }
    return props;
} 
",,,,,1,57
MapConfigProvider.java,17,29,0.5862068965517241," ConfigProvider based on a Map (copied in the constructor). ",,,,,,,1,60
TransportConf.java,147,159,0.9245283018867925,"
 * A central location that tracks all the settings we expose to users.
 ","/**
 * IO mode: nio or epoll
 */
 
/**
 * If true, we will prefer allocating off-heap byte buffers within Netty.
 */
 
/**
 * Connect timeout in milliseconds. Default 120 secs.
 */
 
/**
 * Number of concurrent connections between two nodes for fetching data.
 */
 
/**
 * Requested maximum length of the queue of incoming connections. Default is 64.
 */
 
/**
 * Number of threads used in the server thread pool. Default to 0, which is 2x#cores.
 */
 
/**
 * Number of threads used in the client thread pool. Default to 0, which is 2x#cores.
 */
 
/**
 * Receive buffer size (SO_RCVBUF).
 * Note: the optimal size for receive buffer and send buffer should be
 *  latency * network_bandwidth.
 * Assuming latency = 1ms, network_bandwidth = 10Gbps
 *  buffer size should be ~ 1.25MB
 */
 
/**
 * Send buffer size (SO_SNDBUF).
 */
 
/**
 * Timeout for a single round trip of auth message exchange, in milliseconds.
 */
 
/**
 * Max number of times we will try IO exceptions (such as connection timeouts) per request.
 * If set to 0, we will not do any retries.
 */
 
/**
 * Time (in milliseconds) that we will wait in order to perform a retry after an IOException.
 * Only relevant if maxIORetries &gt; 0.
 */
 
/**
 * Minimum size of a block that we should start using memory map rather than reading in through
 * normal IO operations. This prevents Spark from memory mapping very small blocks. In general,
 * memory mapping has high overhead for blocks close to or below the page size of the OS.
 */
 
/**
 * Whether to initialize FileDescriptor lazily or not. If true, file descriptors are
 * created only when data is going to be transferred. This can reduce the number of open files.
 */
 
/**
 * Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty
 * PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked.
 */
 
/**
 * Whether to enable TCP keep-alive. If true, the TCP keep-alives are enabled, which removes
 * connections that are idle for too long.
 */
 
/**
 * Maximum number of retries when binding to a port before giving up.
 */
 
/**
 * Enables strong encryption. Also enables the new auth protocol, used to negotiate keys.
 */
 
/**
 * The cipher transformation to use for encrypting session data.
 */
 
/**
 * The key generation algorithm. This should be an algorithm that accepts a ""PBEKeySpec""
 * as input. The default value (PBKDF2WithHmacSHA1) is available in Java 7.
 */
 
/**
 * How many iterations to run when generating keys.
 *
 * See some discussion about this at: http://security.stackexchange.com/q/3959
 * The default value was picked for speed, since it assumes that the secret has good entropy
 * (128 bits by default), which is not generally the case with user passwords.
 */
 
/**
 * Encryption key length, in bits.
 */
 
/**
 * Initial vector length, in bytes.
 */
 
/**
 * The algorithm for generated secret keys. Nobody should really need to change this,
 * but configurable just in case.
 */
 
/**
 * Whether to fall back to SASL if the new auth protocol fails. Enabled by default for
 * backwards compatibility.
 */
 
/**
 * Whether to enable SASL-based encryption when authenticating using SASL.
 */
 
/**
 * Maximum number of bytes to be encrypted at a time when SASL encryption is used.
 */
 
/**
 * Whether the server should enforce encryption on SASL-authenticated connections.
 */
 
/**
 * Flag indicating whether to share the pooled ByteBuf allocators between the different Netty
 * channels. If enabled then only two pooled ByteBuf allocators are created: one where caching
 * is allowed (for transport servers) and one where not (for transport clients).
 * When disabled a new allocator is created for each transport servers and clients.
 */
 
/**
 * If enabled then off-heap byte buffers will be prefered for the shared ByteBuf allocators.
 */
 
/**
 * The commons-crypto configuration for the module.
 */
 
/**
 * The max number of chunks allowed to be transferred at the same time on shuffle service.
 * Note that new incoming connections will be closed when the max number is hit. The client will
 * retry according to the shuffle retry configs (see `spark.shuffle.io.maxRetries` and
 * `spark.shuffle.io.retryWait`), if those limits are reached the task will fail with fetch
 * failure.
 */
 
/**
 * Percentage of io.serverThreads used by netty to process ChunkFetchRequest.
 * Shuffle server will use a separate EventLoopGroup to process ChunkFetchRequest messages.
 * Although when calling the async writeAndFlush on the underlying channel to send
 * response back to client, the I/O on the channel is still being handled by
 * {@link org.apache.spark.network.server.TransportServer}'s default EventLoopGroup
 * that's registered with the Channel, by waiting inside the ChunkFetchRequest handler
 * threads for the completion of sending back responses, we are able to put a limit on
 * the max number of threads from TransportServer's default EventLoopGroup that are
 * going to be consumed by writing response to ChunkFetchRequest, which are I/O intensive
 * and could take long time to process due to disk contentions. By configuring a slightly
 * higher number of shuffler server threads, we are able to reserve some threads for
 * handling other RPC messages, thus making the Client less likely to experience timeout
 * when sending RPC messages to the shuffle server. The number of threads used for handling
 * chunked fetch requests are percentage of io.serverThreads (if defined) else it is a percentage
 * of 2 * #cores. However, a percentage of 0 means netty default number of threads which
 * is 2 * #cores ignoring io.serverThreads. The percentage here is configured via
 * spark.shuffle.server.chunkFetchHandlerThreadsPercent. The returned value is rounded off to
 * ceiling of the nearest integer.
 */
 
/**
 * Whether to use the old protocol while doing the shuffle block fetching.
 * It is only enabled while we need the compatibility in the scenario of new spark version
 * job fetching blocks from old version external shuffle service.
 */
 
","{
    return conf.get(SPARK_NETWORK_IO_MODE_KEY, ""NIO"").toUpperCase(Locale.ROOT);
} 
{
    return conf.getBoolean(SPARK_NETWORK_IO_PREFERDIRECTBUFS_KEY, true);
} 
{
    long defaultNetworkTimeoutS = JavaUtils.timeStringAsSec(conf.get(""spark.network.timeout"", ""120s""));
    long defaultTimeoutMs = JavaUtils.timeStringAsSec(conf.get(SPARK_NETWORK_IO_CONNECTIONTIMEOUT_KEY, defaultNetworkTimeoutS + ""s"")) * 1000;
    return (int) defaultTimeoutMs;
} 
{
    return conf.getInt(SPARK_NETWORK_IO_NUMCONNECTIONSPERPEER_KEY, 1);
} 
{
    return conf.getInt(SPARK_NETWORK_IO_BACKLOG_KEY, 64);
} 
{
    return conf.getInt(SPARK_NETWORK_IO_SERVERTHREADS_KEY, 0);
} 
{
    return conf.getInt(SPARK_NETWORK_IO_CLIENTTHREADS_KEY, 0);
} 
{
    return conf.getInt(SPARK_NETWORK_IO_RECEIVEBUFFER_KEY, -1);
} 
{
    return conf.getInt(SPARK_NETWORK_IO_SENDBUFFER_KEY, -1);
} 
{
    return (int) JavaUtils.timeStringAsSec(conf.get(""spark.network.auth.rpcTimeout"", conf.get(SPARK_NETWORK_SASL_TIMEOUT_KEY, ""30s""))) * 1000;
} 
{
    return conf.getInt(SPARK_NETWORK_IO_MAXRETRIES_KEY, 3);
} 
{
    return (int) JavaUtils.timeStringAsSec(conf.get(SPARK_NETWORK_IO_RETRYWAIT_KEY, ""5s"")) * 1000;
} 
{
    return Ints.checkedCast(JavaUtils.byteStringAsBytes(conf.get(""spark.storage.memoryMapThreshold"", ""2m"")));
} 
{
    return conf.getBoolean(SPARK_NETWORK_IO_LAZYFD_KEY, true);
} 
{
    return conf.getBoolean(SPARK_NETWORK_VERBOSE_METRICS, false);
} 
{
    return conf.getBoolean(SPARK_NETWORK_IO_ENABLETCPKEEPALIVE_KEY, false);
} 
{
    return conf.getInt(""spark.port.maxRetries"", 16);
} 
{
    return conf.getBoolean(""spark.network.crypto.enabled"", false);
} 
{
    return conf.get(""spark.network.crypto.cipher"", ""AES/CTR/NoPadding"");
} 
{
    return conf.get(""spark.network.crypto.keyFactoryAlgorithm"", ""PBKDF2WithHmacSHA1"");
} 
{
    return conf.getInt(""spark.network.crypto.keyFactoryIterations"", 1024);
} 
{
    return conf.getInt(""spark.network.crypto.keyLength"", 128);
} 
{
    return conf.getInt(""spark.network.crypto.ivLength"", 16);
} 
{
    return conf.get(""spark.network.crypto.keyAlgorithm"", ""AES"");
} 
{
    return conf.getBoolean(""spark.network.crypto.saslFallback"", true);
} 
{
    return conf.getBoolean(""spark.authenticate.enableSaslEncryption"", false);
} 
{
    return Ints.checkedCast(JavaUtils.byteStringAsBytes(conf.get(""spark.network.sasl.maxEncryptedBlockSize"", ""64k"")));
} 
{
    return conf.getBoolean(""spark.network.sasl.serverAlwaysEncrypt"", false);
} 
{
    return conf.getBoolean(""spark.network.sharedByteBufAllocators.enabled"", true);
} 
{
    return conf.getBoolean(""spark.network.io.preferDirectBufs"", true);
} 
{
    return CryptoUtils.toCryptoConf(""spark.network.crypto.config."", conf.getAll());
} 
{
    return conf.getLong(""spark.shuffle.maxChunksBeingTransferred"", Long.MAX_VALUE);
} 
{
    if (!this.getModuleName().equalsIgnoreCase(""shuffle"")) {
        return 0;
    }
    int chunkFetchHandlerThreadsPercent = conf.getInt(""spark.shuffle.server.chunkFetchHandlerThreadsPercent"", 100);
    int threads = this.serverThreads() > 0 ? this.serverThreads() : 2 * NettyRuntime.availableProcessors();
    return (int) Math.ceil(threads * (chunkFetchHandlerThreadsPercent / 100.0));
} 
{
    return conf.getBoolean(""spark.shuffle.useOldFetchProtocol"", false);
} 
",,,,,1,70
ByteUnit.java,21,39,0.5384615384615384,,,,,,,,1,0
ConfigProvider.java,17,29,0.5862068965517241,"
 * Provides a mechanism for constructing a {@link TransportConf} using some sort of configuration.
 ","/**
 * Obtains the value of the given config, throws NoSuchElementException if it doesn't exist.
 */
 
/**
 * Returns all the config values in the provider.
 */
 
","get 
getAll 
",,,,,1,98
ByteArrayWritableChannel.java,23,33,0.696969696969697,"
 * A writable channel that stores the written data in a byte array in memory.
 ","/**
 * Resets the channel so that writing to it will overwrite the existing buffer.
 */
 
/**
 * Reads from the given buffer into the internal byte array.
 */
 
","{
    offset = 0;
} 
{
    int toTransfer = Math.min(src.remaining(), data.length - offset);
    src.get(data, offset, toTransfer);
    offset += toTransfer;
    return toTransfer;
} 
",,,,,1,77
LimitedInputStream.java,50,70,0.7142857142857143,"
 * Based on LimitedInputStream.java from Google Guava
 *
 * Copyright (C) 2007 The Guava Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the ""License"");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an ""AS IS"" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 |
 * Wraps a {@link InputStream}, limiting the number of bytes which can be read.
 *
 * This code is from Guava's 14.0 source code, because there is no compatible way to
 * use this functionality in both a Guava 11 environment and a Guava &gt;14 environment.
 ",,,"/**
 * Create a LimitedInputStream that will read {@code limit} bytes from {@code in}.
 * <p>
 * If {@code closeWrappedStream} is true, this will close {@code in} when it is closed.
 * Otherwise, the stream is left open for reading its remaining content.
 *
 * @param in a {@link InputStream} to read from
 * @param limit the number of bytes to read
 * @param closeWrappedStream whether to close {@code in} when {@link #close} is called
 */
 
","{
    super(in);
    this.closeWrappedStream = closeWrappedStream;
    Preconditions.checkNotNull(in);
    Preconditions.checkArgument(limit >= 0, ""limit must be non-negative"");
    left = limit;
} 
",,,20,900
AbstractFileRegion.java,16,29,0.5517241379310345,,,,,,,,1,0
JavaUtils.java,107,225,0.47555555555555556,"
 * General utilities available in the network package. Many of these are sourced from Spark's
 * own Utils, just accessible within this package.
 ","/**
 * Closes the given object, ignoring IOExceptions.
 */
 
/**
 * Returns a hash consistent with Spark's Utils.nonNegativeHash().
 */
 
/**
 * Convert the given string to a byte buffer. The resulting buffer can be
 * converted back to the same string through {@link #bytesToString(ByteBuffer)}.
 */
 
/**
 * Convert the given byte buffer to a string. The resulting string can be
 * converted back to the same byte buffer through {@link #stringToBytes(String)}.
 */
 
/**
 * Delete a file or directory and its contents recursively.
 * Don't follow directories if they are symlinks.
 *
 * @param file Input file / dir to be deleted
 * @throws IOException if deletion is unsuccessful
 */
 
/**
 * Delete a file or directory and its contents recursively.
 * Don't follow directories if they are symlinks.
 *
 * @param file Input file / dir to be deleted
 * @param filter A filename filter that make sure only files / dirs with the satisfied filenames
 *               are deleted.
 * @throws IOException if deletion is unsuccessful
 */
 
/**
 * Convert a passed time string (e.g. 50s, 100ms, or 250us) to a time count in the given unit.
 * The unit is also considered the default if the given string does not specify a unit.
 */
 
/**
 * Convert a time parameter such as (50s, 100ms, or 250us) to milliseconds for internal use. If
 * no suffix is provided, the passed number is assumed to be in ms.
 */
 
/**
 * Convert a time parameter such as (50s, 100ms, or 250us) to seconds for internal use. If
 * no suffix is provided, the passed number is assumed to be in seconds.
 */
 
/**
 * Convert a passed byte string (e.g. 50b, 100kb, or 250mb) to the given. If no suffix is
 * provided, a direct conversion to the provided unit is attempted.
 */
 
/**
 * Convert a passed byte string (e.g. 50b, 100k, or 250m) to bytes for
 * internal use.
 *
 * If no suffix is provided, the passed number is assumed to be in bytes.
 */
 
/**
 * Convert a passed byte string (e.g. 50b, 100k, or 250m) to kibibytes for
 * internal use.
 *
 * If no suffix is provided, the passed number is assumed to be in kibibytes.
 */
 
/**
 * Convert a passed byte string (e.g. 50b, 100k, or 250m) to mebibytes for
 * internal use.
 *
 * If no suffix is provided, the passed number is assumed to be in mebibytes.
 */
 
/**
 * Convert a passed byte string (e.g. 50b, 100k, or 250m) to gibibytes for
 * internal use.
 *
 * If no suffix is provided, the passed number is assumed to be in gibibytes.
 */
 
/**
 * Returns a byte array with the buffer's contents, trying to avoid copying the data if
 * possible.
 */
 
/**
 * Fills a buffer with data read from the channel.
 */
 
","{
    try {
        if (closeable != null) {
            closeable.close();
        }
    } catch (IOException e) {
        logger.error(""IOException should not have been thrown."", e);
    }
} 
{
    if (obj == null) {
        return 0;
    }
    int hash = obj.hashCode();
    return hash != Integer.MIN_VALUE ? Math.abs(hash) : 0;
} 
{
    return Unpooled.wrappedBuffer(s.getBytes(StandardCharsets.UTF_8)).nioBuffer();
} 
{
    return Unpooled.wrappedBuffer(b).toString(StandardCharsets.UTF_8);
} 
{
    deleteRecursively(file, null);
} 
{
    if (file == null) {
        return;
    }
    // On Unix systems, use operating system command to run faster
    // If that does not work out, fallback to the Java IO way
    if (SystemUtils.IS_OS_UNIX && filter == null) {
        try {
            deleteRecursivelyUsingUnixNative(file);
            return;
        } catch (IOException e) {
            logger.warn(""Attempt to delete using native Unix OS command failed for path = {}. "" + ""Falling back to Java IO way"", file.getAbsolutePath(), e);
        }
    }
    deleteRecursivelyUsingJavaIO(file, filter);
} 
{
    String lower = str.toLowerCase(Locale.ROOT).trim();
    try {
        Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(lower);
        if (!m.matches()) {
            throw new NumberFormatException(""Failed to parse time string: "" + str);
        }
        long val = Long.parseLong(m.group(1));
        String suffix = m.group(2);
        // Check for invalid suffixes
        if (suffix != null && !timeSuffixes.containsKey(suffix)) {
            throw new NumberFormatException(""Invalid suffix: \"""" + suffix + ""\"""");
        }
        // If suffix is valid use that, otherwise none was provided and use the default passed
        return unit.convert(val, suffix != null ? timeSuffixes.get(suffix) : unit);
    } catch (NumberFormatException e) {
        String timeError = ""Time must be specified as seconds (s), "" + ""milliseconds (ms), microseconds (us), minutes (m or min), hour (h), or day (d). "" + ""E.g. 50s, 100ms, or 250us."";
        throw new NumberFormatException(timeError + ""\n"" + e.getMessage());
    }
} 
{
    return timeStringAs(str, TimeUnit.MILLISECONDS);
} 
{
    return timeStringAs(str, TimeUnit.SECONDS);
} 
{
    String lower = str.toLowerCase(Locale.ROOT).trim();
    try {
        Matcher m = Pattern.compile(""([0-9]+)([a-z]+)?"").matcher(lower);
        Matcher fractionMatcher = Pattern.compile(""([0-9]+\\.[0-9]+)([a-z]+)?"").matcher(lower);
        if (m.matches()) {
            long val = Long.parseLong(m.group(1));
            String suffix = m.group(2);
            // Check for invalid suffixes
            if (suffix != null && !byteSuffixes.containsKey(suffix)) {
                throw new NumberFormatException(""Invalid suffix: \"""" + suffix + ""\"""");
            }
            // If suffix is valid use that, otherwise none was provided and use the default passed
            return unit.convertFrom(val, suffix != null ? byteSuffixes.get(suffix) : unit);
        } else if (fractionMatcher.matches()) {
            throw new NumberFormatException(""Fractional values are not supported. Input was: "" + fractionMatcher.group(1));
        } else {
            throw new NumberFormatException(""Failed to parse byte string: "" + str);
        }
    } catch (NumberFormatException e) {
        String byteError = ""Size must be specified as bytes (b), "" + ""kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). "" + ""E.g. 50b, 100k, or 250m."";
        throw new NumberFormatException(byteError + ""\n"" + e.getMessage());
    }
} 
{
    return byteStringAs(str, ByteUnit.BYTE);
} 
{
    return byteStringAs(str, ByteUnit.KiB);
} 
{
    return byteStringAs(str, ByteUnit.MiB);
} 
{
    return byteStringAs(str, ByteUnit.GiB);
} 
{
    if (buffer.hasArray() && buffer.arrayOffset() == 0 && buffer.array().length == buffer.remaining()) {
        return buffer.array();
    } else {
        byte[] bytes = new byte[buffer.remaining()];
        buffer.get(bytes);
        return bytes;
    }
} 
{
    int expected = dst.remaining();
    while (dst.hasRemaining()) {
        if (channel.read(dst) < 0) {
            throw new EOFException(String.format(""Not enough bytes in channel (expected %d)."", expected));
        }
    }
} 
",,,"/**
 * Define a default value for driver memory here since this value is referenced across the code
 * base and nearly all files already use Utils.scala
 */
 
","Field DEFAULT_DRIVER_MEM_MB
",2,142
ByteArrayReadableChannel.java,16,34,0.47058823529411764,,,,,,,,1,0
NettyMemoryMetrics.java,22,103,0.21359223300970873,"
 * A Netty memory metrics class to collect metrics from Netty PooledByteBufAllocator.
 ",,,,,,,1,85
NettyUtils.java,52,103,0.5048543689320388,"
 * Utilities for creating various Netty constructs based on whether we're using EPOLL or NIO.
 ","/**
 * Creates a new ThreadFactory which prefixes each thread with the given name.
 */
 
/**
 * Creates a Netty EventLoopGroup based on the IOMode.
 */
 
/**
 * Returns the correct (client) SocketChannel class based on IOMode.
 */
 
/**
 * Returns the correct ServerSocketChannel class based on IOMode.
 */
 
/**
 * Creates a LengthFieldBasedFrameDecoder where the first 8 bytes are the length of the frame.
 * This is used before all decoders.
 */
 
/**
 * Returns the remote address on the channel or ""&lt;unknown remote&gt;"" if none exists.
 */
 
/**
 * Returns the default number of threads for both the Netty client and server thread pools.
 * If numUsableCores is 0, we will use Runtime get an approximate number of available cores.
 */
 
/**
 * Returns the lazily created shared pooled ByteBuf allocator for the specified allowCache
 * parameter value.
 */
 
/**
 * Create a pooled ByteBuf allocator but disables the thread-local cache. Thread-local caches
 * are disabled for TransportClients because the ByteBufs are allocated by the event loop thread,
 * but released by the executor thread rather than the event loop thread. Those thread-local
 * caches actually delay the recycling of buffers, leading to larger memory usage.
 */
 
","{
    return new DefaultThreadFactory(threadPoolPrefix, true);
} 
{
    ThreadFactory threadFactory = createThreadFactory(threadPrefix);
    switch(mode) {
        case NIO:
            return new NioEventLoopGroup(numThreads, threadFactory);
        case EPOLL:
            return new EpollEventLoopGroup(numThreads, threadFactory);
        default:
            throw new IllegalArgumentException(""Unknown io mode: "" + mode);
    }
} 
{
    switch(mode) {
        case NIO:
            return NioSocketChannel.class;
        case EPOLL:
            return EpollSocketChannel.class;
        default:
            throw new IllegalArgumentException(""Unknown io mode: "" + mode);
    }
} 
{
    switch(mode) {
        case NIO:
            return NioServerSocketChannel.class;
        case EPOLL:
            return EpollServerSocketChannel.class;
        default:
            throw new IllegalArgumentException(""Unknown io mode: "" + mode);
    }
} 
{
    return new TransportFrameDecoder();
} 
{
    if (channel != null && channel.remoteAddress() != null) {
        return channel.remoteAddress().toString();
    }
    return ""<unknown remote>"";
} 
{
    final int availableCores;
    if (numUsableCores > 0) {
        availableCores = numUsableCores;
    } else {
        availableCores = Runtime.getRuntime().availableProcessors();
    }
    return Math.min(availableCores, MAX_DEFAULT_NETTY_THREADS);
} 
{
    final int index = allowCache ? 0 : 1;
    if (_sharedPooledByteBufAllocator[index] == null) {
        _sharedPooledByteBufAllocator[index] = createPooledByteBufAllocator(allowDirectBufs, allowCache, defaultNumThreads(0));
    }
    return _sharedPooledByteBufAllocator[index];
} 
{
    if (numCores == 0) {
        numCores = Runtime.getRuntime().availableProcessors();
    }
    return new PooledByteBufAllocator(allowDirectBufs && PlatformDependent.directBufferPreferred(), Math.min(PooledByteBufAllocator.defaultNumHeapArena(), numCores), Math.min(PooledByteBufAllocator.defaultNumDirectArena(), allowDirectBufs ? numCores : 0), PooledByteBufAllocator.defaultPageSize(), PooledByteBufAllocator.defaultMaxOrder(), allowCache ? PooledByteBufAllocator.defaultTinyCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultSmallCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultNormalCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultUseCacheForAllThreads() : false);
} 
",,,"/**
 * Specifies an upper bound on the number of Netty threads that Spark requires by default.
 * In practice, only 2-4 cores should be required to transfer roughly 10 Gb/s, and each core
 * that we use will have an initial overhead of roughly 32 MB of off-heap memory, which comes
 * at a premium.
 *
 * Thus, this value should still retain maximum throughput and reduce wasted off-heap memory
 * allocation. It can be overridden by setting the number of serverThreads and clientThreads
 * manually in Spark's configuration.
 */
 
","Field MAX_DEFAULT_NETTY_THREADS
",1,93
TransportFrameDecoder.java,61,173,0.35260115606936415,"
 * A customized frame decoder that allows intercepting raw data.
 * <p>
 * This behaves like Netty's frame decoder (with hard coded parameters that match this library's
 * needs), except it allows an interceptor to be installed to read data directly before it's
 * framed.
 * <p>
 * Unlike Netty's frame decoder, each frame is dispatched to child handlers as soon as it's
 * decoded, instead of building as many frames as the current buffer allows and dispatching
 * all of them. This allows a child handler to install an interceptor if needed.
 * <p>
 * If an interceptor is installed, framing stops, and data is instead fed directly to the
 * interceptor. When the interceptor indicates that it doesn't need to read any more data,
 * framing resumes. Interceptors should not hold references to the data buffers provided
 * to their handle() method.
 ","/**
 * Takes the first buffer in the internal list, and either adjust it to fit in the frame
 * (by taking a slice out of it) or remove it from the internal list.
 */
 
/**
 * @return Whether the interceptor is still active after processing the data.
 */
 
/**
 * Handles data received from the remote end.
 *
 * @param data Buffer containing data.
 * @return ""true"" if the interceptor expects more data, ""false"" to uninstall the interceptor.
 */
 
/**
 * Called if an exception is thrown in the channel pipeline.
 */
 
/**
 * Called if the channel is closed and the interceptor is still installed.
 */
 
","{
    ByteBuf buf = buffers.getFirst();
    ByteBuf frame;
    if (buf.readableBytes() > bytesToRead) {
        frame = buf.retain().readSlice(bytesToRead);
        totalSize -= bytesToRead;
    } else {
        frame = buf;
        buffers.removeFirst();
        totalSize -= frame.readableBytes();
    }
    return frame;
} 
{
    if (interceptor != null && !interceptor.handle(buf)) {
        interceptor = null;
    }
    return interceptor != null;
} 
handle 
exceptionCaught 
channelInactive 
",,,,,14,824
LevelDBProvider.java,27,107,0.2523364485981308,"
 * LevelDB utility class available in the network package.
 ","/**
 * Simple major.minor versioning scheme.  Any incompatible changes should be across major
 * versions.  Minor version differences are allowed -- meaning we should be able to read
 * dbs that are either earlier *or* later on the minor version.
 */
 
","{
    byte[] bytes = db.get(StoreVersion.KEY);
    if (bytes == null) {
        storeVersion(db, newversion, mapper);
    } else {
        StoreVersion version = mapper.readValue(bytes, StoreVersion.class);
        if (version.major != newversion.major) {
            throw new IOException(""cannot read state DB with version "" + version + "", incompatible "" + ""with current version "" + newversion);
        }
        storeVersion(db, newversion, mapper);
    }
} 
",,,,,1,58
SecretKeyHolder.java,27,5,5.4,"
 * Interface for getting a secret key associated with some application.
 ","/**
 * Gets an appropriate SASL User for the given appId.
 * @throws IllegalArgumentException if the given appId is not associated with a SASL user.
 */
 
/**
 * Gets an appropriate SASL secret key for the given appId.
 * @throws IllegalArgumentException if the given appId is not associated with a SASL secret key.
 */
 
","getSaslUser 
getSecretKey 
",,,,,1,71
SaslServerBootstrap.java,25,16,1.5625,"
 * A bootstrap which is executed on a TransportServer's client channel once a client connects
 * to the server. This allows customizing the client channel to allow for things such as SASL
 * authentication.
 ","/**
 * Wrap the given application handler in a SaslRpcHandler that will handle the initial SASL
 * negotiation.
 */
 
","{
    return new SaslRpcHandler(conf, channel, rpcHandler, secretKeyHolder);
} 
",,,,,3,202
SparkSaslServer.java,60,136,0.4411764705882353,"
 * A SASL Server for Spark which simply keeps track of the state of a single SASL session, from the
 * initial state to the ""authenticated"" state. (It is not a server in the sense of accepting
 * connections on some socket.)
 |
   * Implementation of javax.security.auth.callback.CallbackHandler for SASL DIGEST-MD5 mechanism.
   ","/**
 * Determines whether the authentication exchange has completed successfully.
 */
 
/**
 * Returns the value of a negotiated property.
 */
 
/**
 * Used to respond to server SASL tokens.
 * @param token Server's SASL token
 * @return response to send back to the server.
 */
 
/**
 * Disposes of any system resources or security-sensitive information the
 * SaslServer might be using.
 */
 
/**
 * Encode a password as a base64-encoded char[] array.
 */
 
/**
 * Return a Base64-encoded string.
 */
 
","{
    return saslServer != null && saslServer.isComplete();
} 
{
    return saslServer.getNegotiatedProperty(name);
} 
{
    try {
        return saslServer != null ? saslServer.evaluateResponse(token) : new byte[0];
    } catch (SaslException e) {
        throw Throwables.propagate(e);
    }
} 
{
    if (saslServer != null) {
        try {
            saslServer.dispose();
        } catch (SaslException e) {
        // ignore
        } finally {
            saslServer = null;
        }
    }
} 
{
    Preconditions.checkNotNull(password, ""Password cannot be null if SASL is enabled"");
    return getBase64EncodedString(password).toCharArray();
} 
{
    ByteBuf byteBuf = null;
    ByteBuf encodedByteBuf = null;
    try {
        byteBuf = Unpooled.wrappedBuffer(str.getBytes(StandardCharsets.UTF_8));
        encodedByteBuf = Base64.encode(byteBuf);
        return encodedByteBuf.toString(StandardCharsets.UTF_8);
    } finally {
        // The release is called to suppress the memory leak error messages raised by netty.
        if (byteBuf != null) {
            byteBuf.release();
            if (encodedByteBuf != null) {
                encodedByteBuf.release();
            }
        }
    }
} 
",,,"/**
 * This is passed as the server name when creating the sasl client/server.
 * This could be changed to be configurable in the future.
 */
 
/**
 * The authentication mechanism used here is DIGEST-MD5. This could be changed to be
 * configurable in the future.
 */
 
/**
 * Quality of protection value that includes encryption.
 */
 
/**
 * Quality of protection value that does not include encryption.
 */
 
/**
 * Identifier for a certain secret key within the secretKeyHolder.
 */
 
","Field DEFAULT_REALM
Field DIGEST
Field QOP_AUTH_CONF
Field QOP_AUTH
Field secretKeyId
",5,321
SaslMessage.java,27,39,0.6923076923076923,"
 * Encodes a Sasl-related message which is attempting to authenticate using some credentials tagged
 * with the given appId. This appId allows a single SaslRpcHandler to multiplex different
 * applications which may be using different sets of credentials.
 ",,,,,"/**
 * Serialization tag used to catch incorrect payloads.
 */
 
","Field TAG_BYTE
",3,251
SparkSaslClient.java,39,104,0.375,"
 * A SASL Client for Spark which simply keeps track of the state of a single SASL session, from the
 * initial state to the ""authenticated"" state. This client initializes the protocol via a
 * firstToken, which is then followed by a set of challenges and responses.
 |
   * Implementation of javax.security.auth.callback.CallbackHandler
   * that works with share secrets.
   ","/**
 * Used to initiate SASL handshake with server.
 */
 
/**
 * Determines whether the authentication exchange has completed.
 */
 
/**
 * Returns the value of a negotiated property.
 */
 
/**
 * Respond to server's SASL token.
 * @param token contains server's SASL token
 * @return client's response SASL token
 */
 
/**
 * Disposes of any system resources or security-sensitive information the
 * SaslClient might be using.
 */
 
","{
    if (saslClient != null && saslClient.hasInitialResponse()) {
        try {
            return saslClient.evaluateChallenge(new byte[0]);
        } catch (SaslException e) {
            throw Throwables.propagate(e);
        }
    } else {
        return new byte[0];
    }
} 
{
    return saslClient != null && saslClient.isComplete();
} 
{
    return saslClient.getNegotiatedProperty(name);
} 
{
    try {
        return saslClient != null ? saslClient.evaluateChallenge(token) : new byte[0];
    } catch (SaslException e) {
        throw Throwables.propagate(e);
    }
} 
{
    if (saslClient != null) {
        try {
            saslClient.dispose();
        } catch (SaslException e) {
        // ignore
        } finally {
            saslClient = null;
        }
    }
} 
",,,,,6,365
SaslEncryptionBackend.java,19,7,2.7142857142857144,,"/**
 * Disposes of resources used by the backend.
 */
 
/**
 * Encrypt data.
 */
 
/**
 * Decrypt data.
 */
 
","dispose 
wrap 
unwrap 
",,,,,1,0
SaslRpcHandler.java,35,123,0.2845528455284553,"
 * RPC Handler which performs SASL authentication before delegating to a child RPC handler.
 * The delegate will only receive messages if the given connection has been successfully
 * authenticated. A connection may be authenticated at most once.
 *
 * Note that the authentication process consists of multiple challenge-response pairs, each of
 * which are individual RPCs.
 ",,,,,"/**
 * Transport configuration.
 */
 
/**
 * The client channel.
 */
 
/**
 * RpcHandler we will delegate to for authenticated connections.
 */
 
/**
 * Class which provides secret keys which are shared by server and client on a per-app basis.
 */
 
","Field conf
Field channel
Field delegate
Field secretKeyHolder
",6,364
SaslClientBootstrap.java,26,60,0.43333333333333335,"
 * Bootstraps a {@link TransportClient} by performing SASL authentication on the connection. The
 * server should be setup with a {@link SaslRpcHandler} with matching keys for the given appId.
 ","/**
 * Performs SASL authentication by sending a token, and then proceeding with the SASL
 * challenge-response tokens until we either successfully authenticate or throw an exception
 * due to mismatch.
 */
 
","{
    SparkSaslClient saslClient = new SparkSaslClient(appId, secretKeyHolder, conf.saslEncryption());
    try {
        byte[] payload = saslClient.firstToken();
        while (!saslClient.isComplete()) {
            SaslMessage msg = new SaslMessage(appId, payload);
            ByteBuf buf = Unpooled.buffer(msg.encodedLength() + (int) msg.body().size());
            msg.encode(buf);
            buf.writeBytes(msg.body().nioByteBuffer());
            ByteBuffer response = client.sendRpcSync(buf.nioBuffer(), conf.authRTTimeoutMs());
            payload = saslClient.response(JavaUtils.bufferToArray(response));
        }
        client.setClientId(appId);
        if (conf.saslEncryption()) {
            if (!SparkSaslServer.QOP_AUTH_CONF.equals(saslClient.getNegotiatedProperty(Sasl.QOP))) {
                throw new RuntimeException(new SaslException(""Encryption requests by negotiated non-encrypted connection.""));
            }
            SaslEncryption.addToChannel(channel, saslClient, conf.maxSaslEncryptedBlockSize());
            saslClient = null;
            logger.debug(""Channel {} configured for encryption."", client);
        }
    } catch (IOException ioe) {
        throw new RuntimeException(ioe);
    } finally {
        if (saslClient != null) {
            try {
                // Once authentication is complete, the server will trust all remaining communication.
                saslClient.dispose();
            } catch (RuntimeException e) {
                logger.error(""Error while disposing SASL client"", e);
            }
        }
    }
} 
",,,,,2,190
SaslEncryption.java,72,212,0.33962264150943394,"
 * Provides SASL-based encryption for transport channels. The single method exposed by this
 * class installs the needed channel handlers on a connected channel.
 ","/**
 * Adds channel handlers that perform encryption / decryption of data using SASL.
 *
 * @param channel The channel.
 * @param backend The SASL backend.
 * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control
 *                             memory usage.
 */
 
/**
 * Wrap the incoming message in an implementation that will perform encryption lazily. This is
 * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in
 * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it
 * does not guarantee any ordering.
 */
 
/**
 * Returns the size of the original (unencrypted) message.
 *
 * This makes assumptions about how netty treats FileRegion instances, because there's no way
 * to know beforehand what will be the size of the encrypted message. Namely, it assumes
 * that netty will try to transfer data from this message while
 * <code>transferred() < count()</code>. So these two methods return, technically, wrong data,
 * but netty doesn't know better.
 */
 
/**
 * Returns an approximation of the amount of data transferred. See {@link #count()}.
 */
 
/**
 * Transfers data from the original message to the channel, encrypting it in the process.
 *
 * This method also breaks down the original message into smaller chunks when needed. This
 * is done to keep memory usage under control. This avoids having to copy the whole message
 * data into memory at once, and can avoid ballooning memory usage when transferring large
 * messages such as shuffle blocks.
 *
 * The {@link #transferred()} counter also behaves a little funny, in that it won't go forward
 * until a whole chunk has been written. This is done because the code can't use the actual
 * number of bytes written to the channel as the transferred count (see {@link #count()}).
 * Instead, once an encrypted chunk is written to the output (including its header), the
 * size of the original block will be added to the {@link #transferred()} amount.
 */
 
","{
    channel.pipeline().addFirst(ENCRYPTION_HANDLER_NAME, new EncryptionHandler(backend, maxOutboundBlockSize)).addFirst(""saslDecryption"", new DecryptionHandler(backend)).addFirst(""saslFrameDecoder"", NettyUtils.createFrameDecoder());
} 
{
    ctx.write(new EncryptedMessage(backend, msg, maxOutboundBlockSize), promise);
} 
{
    return isByteBuf ? buf.readableBytes() : region.count();
} 
{
    return transferred;
} 
{
    Preconditions.checkArgument(position == transferred(), ""Invalid position."");
    long reportedWritten = 0L;
    long actuallyWritten = 0L;
    do {
        if (currentChunk == null) {
            nextChunk();
        }
        if (currentHeader.readableBytes() > 0) {
            int bytesWritten = target.write(currentHeader.nioBuffer());
            currentHeader.skipBytes(bytesWritten);
            actuallyWritten += bytesWritten;
            if (currentHeader.readableBytes() > 0) {
                // Break out of loop if there are still header bytes left to write.
                break;
            }
        }
        actuallyWritten += target.write(currentChunk);
        if (!currentChunk.hasRemaining()) {
            // Only update the count of written bytes once a full chunk has been written.
            // See method javadoc.
            long chunkBytesRemaining = unencryptedChunkSize - currentReportedBytes;
            reportedWritten += chunkBytesRemaining;
            transferred += chunkBytesRemaining;
            currentHeader.release();
            currentHeader = null;
            currentChunk = null;
            currentChunkSize = 0;
            currentReportedBytes = 0;
        }
    } while (currentChunk == null && transferred() + reportedWritten < count());
    // Returning 0 triggers a backoff mechanism in netty which may harm performance. Instead,
    // we return 1 until we can (i.e. until the reported count would actually match the size
    // of the current chunk), at which point we resort to returning 0 so that the counts still
    // match, at the cost of some performance. That situation should be rare, though.
    if (reportedWritten != 0L) {
        return reportedWritten;
    }
    if (actuallyWritten > 0 && currentReportedBytes < currentChunkSize - 1) {
        transferred += 1L;
        currentReportedBytes += 1L;
        return 1L;
    }
    return 0L;
} 
",,,"/**
 * A channel used to buffer input data for encryption. The channel has an upper size bound
 * so that if the input is larger than the allowed buffer, it will be broken into multiple
 * chunks. Made non-final to enable lazy initialization, which saves memory.
 */
 
","Field byteChannel
",2,159
TransportContext.java,88,133,0.6616541353383458,"
 * Contains the context to create a {@link TransportServer}, {@link TransportClientFactory}, and to
 * setup Netty Channel pipelines with a
 * {@link org.apache.spark.network.server.TransportChannelHandler}.
 *
 * There are two communication protocols that the TransportClient provides, control-plane RPCs and
 * data-plane ""chunk fetching"". The handling of the RPCs is performed outside of the scope of the
 * TransportContext (i.e., by a user-provided handler), and it is responsible for setting up streams
 * which can be streamed through the data plane in chunks using zero-copy IO.
 *
 * The TransportServer and TransportClientFactory both create a TransportChannelHandler for each
 * channel. As each TransportChannelHandler contains a TransportClient, this enables server
 * processes to send messages back to the client on an existing channel.
 ","/**
 * Initializes a ClientFactory which runs the given TransportClientBootstraps prior to returning
 * a new Client. Bootstraps will be executed synchronously, and must run successfully in order
 * to create a Client.
 */
 
/**
 * Create a server which will attempt to bind to a specific port.
 */
 
/**
 * Create a server which will attempt to bind to a specific host and port.
 */
 
/**
 * Creates a new server, binding to any available ephemeral port.
 */
 
/**
 * Initializes a client or server Netty Channel Pipeline which encodes/decodes messages and
 * has a {@link org.apache.spark.network.server.TransportChannelHandler} to handle request or
 * response messages.
 *
 * @param channel The channel to initialize.
 * @param channelRpcHandler The RPC handler to use for the channel.
 *
 * @return Returns the created TransportChannelHandler, which includes a TransportClient that can
 * be used to communicate on this channel. The TransportClient is directly associated with a
 * ChannelHandler to ensure all users of the same channel get the same TransportClient object.
 */
 
/**
 * Creates the server- and client-side handler which is used to handle both RequestMessages and
 * ResponseMessages. The channel is expected to have been successfully created, though certain
 * properties (such as the remoteAddress()) may not be available yet.
 */
 
/**
 * Creates the dedicated ChannelHandler for ChunkFetchRequest messages.
 */
 
","{
    return new TransportClientFactory(this, bootstraps);
} 
{
    return new TransportServer(this, null, port, rpcHandler, bootstraps);
} 
{
    return new TransportServer(this, host, port, rpcHandler, bootstraps);
} 
{
    return createServer(0, bootstraps);
} 
{
    try {
        TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler);
        ChunkFetchRequestHandler chunkFetchHandler = createChunkFetchHandler(channelHandler, channelRpcHandler);
        ChannelPipeline pipeline = channel.pipeline().addLast(""encoder"", ENCODER).addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder()).addLast(""decoder"", DECODER).addLast(""idleStateHandler"", new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000)).addLast(""handler"", channelHandler);
        // Use a separate EventLoopGroup to handle ChunkFetchRequest messages for shuffle rpcs.
        if (chunkFetchWorkers != null) {
            pipeline.addLast(chunkFetchWorkers, ""chunkFetchHandler"", chunkFetchHandler);
        }
        return channelHandler;
    } catch (RuntimeException e) {
        logger.error(""Error while initializing Netty pipeline"", e);
        throw e;
    }
} 
{
    TransportResponseHandler responseHandler = new TransportResponseHandler(channel);
    TransportClient client = new TransportClient(channel, responseHandler);
    TransportRequestHandler requestHandler = new TransportRequestHandler(channel, client, rpcHandler, conf.maxChunksBeingTransferred());
    return new TransportChannelHandler(client, responseHandler, requestHandler, conf.connectionTimeoutMs(), closeIdleConnections, this);
} 
{
    return new ChunkFetchRequestHandler(channelHandler.getClient(), rpcHandler.getStreamManager(), conf.maxChunksBeingTransferred());
} 
","/**
 * Enables TransportContext initialization for underlying client and server.
 *
 * @param conf TransportConf
 * @param rpcHandler RpcHandler responsible for handling requests and responses.
 * @param closeIdleConnections Close idle connections if it is set to true.
 * @param isClientOnly This config indicates the TransportContext is only used by a client.
 *                     This config is more important when external shuffle is enabled.
 *                     It stops creating extra event loop and subsequent thread pool
 *                     for shuffle clients to handle chunked fetch requests.
 */
 
","{
    this.conf = conf;
    this.rpcHandler = rpcHandler;
    this.closeIdleConnections = closeIdleConnections;
    if (conf.getModuleName() != null && conf.getModuleName().equalsIgnoreCase(""shuffle"") && !isClientOnly) {
        chunkFetchWorkers = NettyUtils.createEventLoop(IOMode.valueOf(conf.ioMode()), conf.chunkFetchHandlerThreads(), ""shuffle-chunk-fetch-handler"");
    } else {
        chunkFetchWorkers = null;
    }
} 
","/**
 * Force to create MessageEncoder and MessageDecoder so that we can make sure they will be created
 * before switching the current context class loader to ExecutorClassLoader.
 *
 * Netty's MessageToMessageEncoder uses Javassist to generate a matcher class and the
 * implementation calls ""Class.forName"" to check if this calls is already generated. If the
 * following two objects are created in ""ExecutorClassLoader.findClass"", it will cause
 * ""ClassCircularityError"". This is because loading this Netty generated class will call
 * ""ExecutorClassLoader.findClass"" to search this class, and ""ExecutorClassLoader"" will try to use
 * RPC to load it and cause to load the non-exist matcher class again. JVM will report
 * `ClassCircularityError` to prevent such infinite recursion. (See SPARK-17714)
 */
 
","Field ENCODER
",12,829
RpcHandler.java,35,123,0.2845528455284553,"
 * Handler for sendRPC() messages sent by {@link org.apache.spark.network.client.TransportClient}s.
 ","/**
 * Receive a single RPC message. Any exception thrown while in this method will be sent back to
 * the client in string form as a standard RPC failure.
 *
 * Neither this method nor #receiveStream will be called in parallel for a single
 * TransportClient (i.e., channel).
 *
 * @param client A channel client which enables the handler to make requests back to the sender
 *               of this RPC. This will always be the exact same object for a particular channel.
 * @param message The serialized bytes of the RPC.
 * @param callback Callback which should be invoked exactly once upon success or failure of the
 *                 RPC.
 */
 
/**
 * Receive a single RPC message which includes data that is to be received as a stream. Any
 * exception thrown while in this method will be sent back to the client in string form as a
 * standard RPC failure.
 *
 * Neither this method nor #receive will be called in parallel for a single TransportClient
 * (i.e., channel).
 *
 * An error while reading data from the stream
 * ({@link org.apache.spark.network.client.StreamCallback#onData(String, ByteBuffer)})
 * will fail the entire channel.  A failure in ""post-processing"" the stream in
 * {@link org.apache.spark.network.client.StreamCallback#onComplete(String)} will result in an
 * rpcFailure, but the channel will remain active.
 *
 * @param client A channel client which enables the handler to make requests back to the sender
 *               of this RPC. This will always be the exact same object for a particular channel.
 * @param messageHeader The serialized bytes of the header portion of the RPC.  This is in meant
 *                      to be relatively small, and will be buffered entirely in memory, to
 *                      facilitate how the streaming portion should be received.
 * @param callback Callback which should be invoked exactly once upon success or failure of the
 *                 RPC.
 * @return a StreamCallback for handling the accompanying streaming data
 */
 
/**
 * Returns the StreamManager which contains the state about which streams are currently being
 * fetched by a TransportClient.
 */
 
/**
 * Receives an RPC message that does not expect a reply. The default implementation will
 * call ""{@link #receive(TransportClient, ByteBuffer, RpcResponseCallback)}"" and log a warning if
 * any of the callback methods are called.
 *
 * @param client A channel client which enables the handler to make requests back to the sender
 *               of this RPC. This will always be the exact same object for a particular channel.
 * @param message The serialized bytes of the RPC.
 */
 
/**
 * Invoked when the channel associated with the given client is active.
 */
 
/**
 * Invoked when the channel associated with the given client is inactive.
 * No further requests will come from this client.
 */
 
","receive 
{
    throw new UnsupportedOperationException();
} 
getStreamManager 
{
    receive(client, message, ONE_WAY_CALLBACK);
} 
{
} 
{
} 
",,,,,1,99
StreamManager.java,44,138,0.3188405797101449,"
 * The StreamManager is used to fetch individual chunks from a stream. This is used in
 * {@link TransportRequestHandler} in order to respond to fetchChunk() requests. Creation of the
 * stream is outside the scope of the transport layer, but a given stream is guaranteed to be read
 * by only one client connection, meaning that getChunk() for a particular stream will be called
 * serially and that once the connection associated with the stream is closed, that stream will
 * never be used again.
 ","/**
 * Called in response to a fetchChunk() request. The returned buffer will be passed as-is to the
 * client. A single stream will be associated with a single TCP connection, so this method
 * will not be called in parallel for a particular stream.
 *
 * Chunks may be requested in any order, and requests may be repeated, but it is not required
 * that implementations support this behavior.
 *
 * The returned ManagedBuffer will be release()'d after being written to the network.
 *
 * @param streamId id of a stream that has been previously registered with the StreamManager.
 * @param chunkIndex 0-indexed chunk of the stream that's requested
 */
 
/**
 * Called in response to a stream() request. The returned data is streamed to the client
 * through a single TCP connection.
 *
 * Note the <code>streamId</code> argument is not related to the similarly named argument in the
 * {@link #getChunk(long, int)} method.
 *
 * @param streamId id of a stream that has been previously registered with the StreamManager.
 * @return A managed buffer for the stream, or null if the stream was not found.
 */
 
/**
 * Indicates that the given channel has been terminated. After this occurs, we are guaranteed not
 * to read from the associated streams again, so any state can be cleaned up.
 */
 
/**
 * Verify that the client is authorized to read from the given stream.
 *
 * @throws SecurityException If client is not authorized.
 */
 
/**
 * Return the number of chunks being transferred and not finished yet in this StreamManager.
 */
 
/**
 * Called when start sending a chunk.
 */
 
/**
 * Called when start sending a stream.
 */
 
/**
 * Called when a chunk is successfully sent.
 */
 
/**
 * Called when a stream is successfully sent.
 */
 
","getChunk 
{
    throw new UnsupportedOperationException();
} 
{
} 
{
} 
{
    return 0;
} 
{
} 
{
} 
{
} 
{
} 
",,,,,6,489
MessageHandler.java,25,8,3.125,"
 * Handles either request or response messages coming off of Netty. A MessageHandler instance
 * is associated with a single Netty Channel (though it may have multiple clients on the same
 * Channel.)
 ","/**
 * Handles the receipt of a single message.
 */
 
/**
 * Invoked when the channel this MessageHandler is on is active.
 */
 
/**
 * Invoked when an exception was caught on the Channel.
 */
 
/**
 * Invoked when the channel this MessageHandler is on is inactive.
 */
 
","handle 
channelActive 
exceptionCaught 
channelInactive 
",,,,,3,196
NoOpRpcHandler.java,17,16,1.0625," An RpcHandler suitable for a client-only TransportContext, which cannot receive RPCs. ",,,,,,,1,87
TransportRequestHandler.java,40,200,0.2,"
 * A handler that processes requests from clients and writes chunk data back. Each handler is
 * attached to a single Netty channel, and keeps track of which streams have been fetched via this
 * channel, in order to clean them up if the channel is terminated (see #channelUnregistered).
 *
 * The messages should have been processed by the pipeline setup by {@link TransportServer}.
 ","/**
 * Handle a request from the client to upload a stream of data.
 */
 
/**
 * Responds to a single message with some Encodable object. If a failure occurs while sending,
 * it will be logged and the channel closed.
 */
 
","{
    assert (req.body() == null);
    try {
        RpcResponseCallback callback = new RpcResponseCallback() {

            @Override
            public void onSuccess(ByteBuffer response) {
                respond(new RpcResponse(req.requestId, new NioManagedBuffer(response)));
            }

            @Override
            public void onFailure(Throwable e) {
                respond(new RpcFailure(req.requestId, Throwables.getStackTraceAsString(e)));
            }
        };
        TransportFrameDecoder frameDecoder = (TransportFrameDecoder) channel.pipeline().get(TransportFrameDecoder.HANDLER_NAME);
        ByteBuffer meta = req.meta.nioByteBuffer();
        StreamCallbackWithID streamHandler = rpcHandler.receiveStream(reverseClient, meta, callback);
        if (streamHandler == null) {
            throw new NullPointerException(""rpcHandler returned a null streamHandler"");
        }
        StreamCallbackWithID wrappedCallback = new StreamCallbackWithID() {

            @Override
            public void onData(String streamId, ByteBuffer buf) throws IOException {
                streamHandler.onData(streamId, buf);
            }

            @Override
            public void onComplete(String streamId) throws IOException {
                try {
                    streamHandler.onComplete(streamId);
                    callback.onSuccess(ByteBuffer.allocate(0));
                } catch (Exception ex) {
                    IOException ioExc = new IOException(""Failure post-processing complete stream;"" + "" failing this rpc and leaving channel active"", ex);
                    callback.onFailure(ioExc);
                    streamHandler.onFailure(streamId, ioExc);
                }
            }

            @Override
            public void onFailure(String streamId, Throwable cause) throws IOException {
                callback.onFailure(new IOException(""Destination failed while reading stream"", cause));
                streamHandler.onFailure(streamId, cause);
            }

            @Override
            public String getID() {
                return streamHandler.getID();
            }
        };
        if (req.bodyByteCount > 0) {
            StreamInterceptor<RequestMessage> interceptor = new StreamInterceptor<>(this, wrappedCallback.getID(), req.bodyByteCount, wrappedCallback);
            frameDecoder.setInterceptor(interceptor);
        } else {
            wrappedCallback.onComplete(wrappedCallback.getID());
        }
    } catch (Exception e) {
        logger.error(""Error while invoking RpcHandler#receive() on RPC id "" + req.requestId, e);
        respond(new RpcFailure(req.requestId, Throwables.getStackTraceAsString(e)));
        // We choose to totally fail the channel, rather than trying to recover as we do in other
        // cases.  We don't know how many bytes of the stream the client has already sent for the
        // stream, it's not worth trying to recover.
        channel.pipeline().fireExceptionCaught(e);
    } finally {
        req.meta.release();
    }
} 
{
    SocketAddress remoteAddress = channel.remoteAddress();
    return channel.writeAndFlush(result).addListener(future -> {
        if (future.isSuccess()) {
            logger.trace(""Sent result {} to client {}"", result, remoteAddress);
        } else {
            logger.error(String.format(""Error sending result %s to %s; closing connection"", result, remoteAddress), future.cause());
            channel.close();
        }
    });
} 
",,,"/**
 * The Netty channel that this handler is associated with.
 */
 
/**
 * Client on the same channel allowing us to talk back to the requester.
 */
 
/**
 * Handles all RPC messages.
 */
 
/**
 * Returns each chunk part of a stream.
 */
 
/**
 * The max number of chunks being transferred and not finished yet.
 */
 
","Field channel
Field reverseClient
Field rpcHandler
Field streamManager
Field maxChunksBeingTransferred
",5,375
TransportServerBootstrap.java,28,5,5.6,"
 * A bootstrap which is executed on a TransportServer's client channel once a client connects
 * to the server. This allows customizing the client channel to allow for things such as SASL
 * authentication.
 ","/**
 * Customizes the channel to include new features, if needed.
 *
 * @param channel The connected channel opened by the client.
 * @param rpcHandler The RPC handler for the server.
 * @return The RPC handler to use for the channel.
 */
 
","doBootstrap 
",,,,,3,202
TransportChannelHandler.java,46,133,0.3458646616541353,"
 * The single Transport-level Channel handler which is used for delegating requests to the
 * {@link TransportRequestHandler} and responses to the {@link TransportResponseHandler}.
 *
 * All channels created in the transport layer are bidirectional. When the Client initiates a Netty
 * Channel with a RequestMessage (which gets handled by the Server's RequestHandler), the Server
 * will produce a ResponseMessage (handled by the Client's ResponseHandler). However, the Server
 * also gets a handle on the same Channel, so it may then begin to send RequestMessages to the
 * Client.
 * This means that the Client also needs a RequestHandler and the Server needs a ResponseHandler,
 * for the Client's responses to the Server's requests.
 *
 * This class also handles timeouts from a {@link io.netty.handler.timeout.IdleStateHandler}.
 * We consider a connection timed out if there are outstanding fetch or RPC requests but no traffic
 * on the channel for at least `requestTimeoutMs`. Note that this is duplex traffic; we will not
 * timeout if the client is continuously sending but getting no responses, for simplicity.
 ","/**
 * Overwrite acceptInboundMessage to properly delegate ChunkFetchRequest messages
 * to ChunkFetchRequestHandler.
 */
 
/**
 * Triggered based on events from an {@link io.netty.handler.timeout.IdleStateHandler}.
 */
 
","{
    if (msg instanceof ChunkFetchRequest) {
        return false;
    } else {
        return super.acceptInboundMessage(msg);
    }
} 
{
    if (evt instanceof IdleStateEvent) {
        IdleStateEvent e = (IdleStateEvent) evt;
        // See class comment for timeout semantics. In addition to ensuring we only timeout while
        // there are outstanding requests, we also do a secondary consistency check to ensure
        // there's no race between the idle timeout and incrementing the numOutstandingRequests
        // (see SPARK-7003).
        // 
        // To avoid a race between TransportClientFactory.createClient() and this code which could
        // result in an inactive client being returned, this needs to run in a synchronized block.
        synchronized (this) {
            boolean hasInFlightRequests = responseHandler.numOutstandingRequests() > 0;
            boolean isActuallyOverdue = System.nanoTime() - responseHandler.getTimeOfLastRequestNs() > requestTimeoutNs;
            if (e.state() == IdleState.ALL_IDLE && isActuallyOverdue) {
                if (hasInFlightRequests) {
                    String address = getRemoteAddress(ctx.channel());
                    logger.error(""Connection to {} has been quiet for {} ms while there are outstanding "" + ""requests. Assuming connection is dead; please adjust spark.network.timeout if "" + ""this is wrong."", address, requestTimeoutNs / 1000 / 1000);
                    client.timeOut();
                    ctx.close();
                } else if (closeIdleConnections) {
                    // While CloseIdleConnections is enable, we also close idle connection
                    client.timeOut();
                    ctx.close();
                }
            }
        }
    }
    ctx.fireUserEventTriggered(evt);
} 
",,,,,15,1094
TransportServer.java,24,129,0.18604651162790697,"
 * Server for the efficient, low-level streaming service.
 ",,,"/**
 * Creates a TransportServer that binds to the given host and the given port, or to any available
 * if 0. If you don't want to bind to any special host, set ""hostToBind"" to null.
 */
 
","{
    this.context = context;
    this.conf = context.getConf();
    this.appRpcHandler = appRpcHandler;
    if (conf.sharedByteBufAllocators()) {
        this.pooledAllocator = NettyUtils.getSharedPooledByteBufAllocator(conf.preferDirectBufsForSharedByteBufAllocators(), true);
    } else {
        this.pooledAllocator = NettyUtils.createPooledByteBufAllocator(conf.preferDirectBufs(), true, /* allowCache */
        conf.serverThreads());
    }
    this.bootstraps = Lists.newArrayList(Preconditions.checkNotNull(bootstraps));
    boolean shouldClose = true;
    try {
        init(hostToBind, portToBind);
        shouldClose = false;
    } finally {
        if (shouldClose) {
            JavaUtils.closeQuietly(this);
        }
    }
} 
",,,1,57
OneForOneStreamManager.java,44,138,0.3188405797101449,"
 * StreamManager which allows registration of an Iterator&lt;ManagedBuffer&gt;, which are
 * individually fetched as chunks by the client. Each registered buffer is one chunk.
 | Parse streamChunkId to be stream id and chunk id. This is used when fetch remote chunk as a| State of a single stream. ","/**
 * Registers a stream of ManagedBuffers which are served as individual chunks one at a time to
 * callers. Each ManagedBuffer will be release()'d after it is transferred on the wire. If a
 * client connection is closed before the iterator is fully drained, then the remaining buffers
 * will all be release()'d.
 *
 * If an app ID is provided, only callers who've authenticated with the given app ID will be
 * allowed to fetch from this stream.
 *
 * This method also associates the stream with a single client connection, which is guaranteed
 * to be the only reader of the stream. Once the connection is closed, the stream will never
 * be used again, enabling cleanup by `connectionTerminated`.
 */
 
","{
    long myStreamId = nextStreamId.getAndIncrement();
    streams.put(myStreamId, new StreamState(appId, buffers, channel));
    return myStreamId;
} 
",,,,,3,294
ChunkFetchRequestHandler.java,41,84,0.4880952380952381,"
 * A dedicated ChannelHandler for processing ChunkFetchRequest messages. When sending response
 * of ChunkFetchRequest messages to the clients, the thread performing the I/O on the underlying
 * channel could potentially be blocked due to disk contentions. If several hundreds of clients
 * send ChunkFetchRequest to the server at the same time, it could potentially occupying all
 * threads from TransportServer's default EventLoopGroup for waiting for disk reads before it
 * can send the block data back to the client as part of the ChunkFetchSuccess messages. As a
 * result, it would leave no threads left to process other RPC messages, which takes much less
 * time to process, and could lead to client timing out on either performing SASL authentication,
 * registering executors, or waiting for response for an OpenBlocks messages.
 ","/**
 * The invocation to channel.writeAndFlush is async, and the actual I/O on the
 * channel will be handled by the EventLoop the channel is registered to. So even
 * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O,
 * which is the potentially blocking call that could deplete server handler threads, is still
 * being processed by TransportServer's default EventLoopGroup. In order to throttle the max
 * number of threads that channel I/O for sending response to ChunkFetchRequest, the thread
 * calling channel.writeAndFlush will wait for the completion of sending response back to
 * client by invoking await(). This will throttle the rate at which threads from
 * ChunkFetchRequest dedicated EventLoopGroup submit channel I/O requests to TransportServer's
 * default EventLoopGroup, thus making sure that we can reserve some threads in
 * TransportServer's default EventLoopGroup for handling other RPC messages.
 */
 
","{
    final SocketAddress remoteAddress = channel.remoteAddress();
    return channel.writeAndFlush(result).await().addListener((ChannelFutureListener) future -> {
        if (future.isSuccess()) {
            logger.trace(""Sent result {} to client {}"", result, remoteAddress);
        } else {
            logger.error(String.format(""Error sending result %s to %s; closing connection"", result, remoteAddress), future.cause());
            channel.close();
        }
    });
} 
",,,"/**
 * The max number of chunks being transferred and not finished yet.
 */
 
","Field maxChunksBeingTransferred
",9,823
Message.java,34,48,0.7083333333333334," An on-the-wire transmittable message. | Preceding every serialized Message is its type, which allows us to deserialize it. ","/**
 * Used to identify this request type.
 */
 
/**
 * An optional body for the message.
 */
 
/**
 * Whether to include the body of the message in the same frame as the message.
 */
 
","type 
body 
isBodyInFrame 
",,,,,1,124
RequestMessage.java,18,3,6.0," Messages from the client to the server. ",,,,,,,1,41
ChunkFetchSuccess.java,25,51,0.49019607843137253,"
 * Response to {@link ChunkFetchRequest} when a chunk exists and has been successfully fetched.
 *
 * Note that the server-side encoding of this messages does NOT include the buffer itself, as this
 * may be written by Netty in a more efficient manner (i.e., zero-copy write).
 * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.
 ","/**
 * Encoding does NOT include 'buffer' itself. See {@link MessageEncoder}.
 */
 
/**
 * Decoding uses the given ByteBuf as our data, and will retain() it.
 */
 
","{
    streamChunkId.encode(buf);
} 
{
    StreamChunkId streamChunkId = StreamChunkId.decode(buf);
    buf.retain();
    NettyManagedBuffer managedBuf = new NettyManagedBuffer(buf.duplicate());
    return new ChunkFetchSuccess(streamChunkId, managedBuf);
} 
",,,,,5,351
MessageDecoder.java,20,50,0.4,"
 * Decoder used by the client side to encode server-to-client responses.
 * This encoder is stateless so it is safe to be shared by multiple threads.
 ",,,,,,,2,147
AbstractMessage.java,19,25,0.76,"
 * Abstract class for messages which optionally contain a body kept in a separate buffer.
 ",,,,,,,1,89
ChunkFetchRequest.java,20,40,0.5,"
 * Request to fetch a sequence of a single chunk of a stream. This will correspond to a single
 * {@link org.apache.spark.network.protocol.ResponseMessage} (either success or failure).
 ",,,,,,,2,182
StreamChunkId.java,19,44,0.4318181818181818,"
* Encapsulates a request for a particular chunk of a stream.
",,,,,,,1,59
StreamFailure.java,19,46,0.41304347826086957,"
 * Message indicating an error when transferring a stream.
 ",,,,,,,1,58
RpcFailure.java,17,46,0.3695652173913043," Response to {@link RpcRequest} for a failed RPC. ",,,,,,,1,50
UploadStream.java,25,68,0.36764705882352944,"
 * An RPC with data that is sent outside of the frame, so it can be read as a stream.
 ",,,,,"/**
 * Used to link an RPC request with its response.
 */
 
","Field requestId
",1,85
ResponseMessage.java,19,8,2.375," Messages from the server to the client. ",,,,,,,1,41
StreamResponse.java,24,53,0.4528301886792453,"
 * Response to {@link StreamRequest} when the stream has been successfully opened.
 * <p>
 * Note the message itself does not contain the stream data. That is written separately by the
 * sender. The receiver is expected to set a temporary channel handler that will consume the
 * number of bytes this message says the stream has.
 ","/**
 * Encoding does NOT include 'buffer' itself. See {@link MessageEncoder}.
 */
 
","{
    Encoders.Strings.encode(buf, streamId);
    buf.writeLong(byteCount);
} 
",,,,,5,322
StreamRequest.java,22,41,0.5365853658536586,"
 * Request to stream data from the remote end.
 * <p>
 * The stream ID is an arbitrary string that needs to be negotiated between the two endpoints before
 * the data can be streamed.
 ",,,,,,,4,177
AbstractResponseMessage.java,19,8,2.375,"
 * Abstract class for response messages.
 ",,,,,,,1,40
RpcRequest.java,27,47,0.574468085106383,"
 * A generic RPC which is handled by a remote {@link org.apache.spark.network.server.RpcHandler}.
 * This will correspond to a single
 * {@link org.apache.spark.network.protocol.ResponseMessage} (either success or failure).
 ",,,,,"/**
 * Used to link an RPC request with its response.
 */
 
","Field requestId
",3,219
Encodable.java,31,6,5.166666666666667,"
 * Interface for an object which can be encoded into a ByteBuf. Multiple Encodable objects are
 * stored in a single, pre-allocated ByteBuf, so Encodables must also provide their length.
 *
 * Encodable objects should provide a static ""decode(ByteBuf)"" method which is invoked by
 * {@link MessageDecoder}. During decoding, if the object uses the ByteBuf as its data (rather than
 * just copying data from it), then you must retain() the ByteBuf.
 *
 * Additionally, when adding a new Encodable Message, add it to {@link Message.Type}.
 ","/**
 * Number of bytes of the encoded form of this object.
 */
 
/**
 * Serializes this object by writing into the given ByteBuf.
 * This method must write exactly encodedLength() bytes.
 */
 
","encodedLength 
encode 
",,,,,8,521
Encoders.java,22,97,0.2268041237113402," Provides a canonical set of Encoders for simple types. | Strings are encoded with their length followed by UTF-8 bytes. | Byte arrays are encoded with their length followed by bytes. | String arrays are encoded with the number of strings followed by per-String encoding. | Integer arrays are encoded with their length followed by integers. | Long integer arrays are encoded with their length followed by long integers. ",,,,,,,1,420
MessageEncoder.java,33,52,0.6346153846153846,"
 * Encoder used by the server side to encode server-to-client responses.
 * This encoder is stateless so it is safe to be shared by multiple threads.
 ","/**
 * Encodes a Message by invoking its encode() method. For non-data messages, we will add one
 * ByteBuf to 'out' containing the total frame length, the message type, and the message itself.
 * In the case of a ChunkFetchSuccess, we will also add the ManagedBuffer corresponding to the
 * data to 'out', in order to enable zero-copy transfer.
 */
 
","{
    Object body = null;
    long bodyLength = 0;
    boolean isBodyInFrame = false;
    // If the message has a body, take it out to enable zero-copy transfer for the payload.
    if (in.body() != null) {
        try {
            bodyLength = in.body().size();
            body = in.body().convertToNetty();
            isBodyInFrame = in.isBodyInFrame();
        } catch (Exception e) {
            in.body().release();
            if (in instanceof AbstractResponseMessage) {
                AbstractResponseMessage resp = (AbstractResponseMessage) in;
                // Re-encode this message as a failure response.
                String error = e.getMessage() != null ? e.getMessage() : ""null"";
                logger.error(String.format(""Error processing %s for client %s"", in, ctx.channel().remoteAddress()), e);
                encode(ctx, resp.createFailureResponse(error), out);
            } else {
                throw e;
            }
            return;
        }
    }
    Message.Type msgType = in.type();
    // All messages have the frame length, message type, and message itself. The frame length
    // may optionally include the length of the body data, depending on what message is being
    // sent.
    int headerLength = 8 + msgType.encodedLength() + in.encodedLength();
    long frameLength = headerLength + (isBodyInFrame ? bodyLength : 0);
    ByteBuf header = ctx.alloc().buffer(headerLength);
    header.writeLong(frameLength);
    msgType.encode(header);
    in.encode(header);
    assert header.writableBytes() == 0;
    if (body != null) {
        // We transfer ownership of the reference on in.body() to MessageWithHeader.
        // This reference will be freed when MessageWithHeader.deallocate() is called.
        out.add(new MessageWithHeader(in.body(), header, body, bodyLength));
    } else {
        out.add(header);
    }
} 
",,,,,2,147
RpcResponse.java,22,51,0.43137254901960786," Response to {@link RpcRequest} for a successful RPC. ",,,,,,,1,54
ChunkFetchFailure.java,19,46,0.41304347826086957,"
 * Response to {@link ChunkFetchRequest} when there is an error fetching the chunk.
 ",,,,,,,1,83
MessageWithHeader.java,55,123,0.44715447154471544,"
 * A wrapper message that holds two separate pieces (a header and a body).
 *
 * The header must be a ByteBuf, while the body can be a ByteBuf or a FileRegion.
 ","/**
 * This code is more complicated than you would think because we might require multiple
 * transferTo invocations in order to transfer a single MessageWithHeader to avoid busy waiting.
 *
 * The contract is that the caller will ensure position is properly set to the total number
 * of bytes transferred so far (i.e. value returned by transferred()).
 */
 
","{
    Preconditions.checkArgument(position == totalBytesTransferred, ""Invalid position."");
    // Bytes written for header in this call.
    long writtenHeader = 0;
    if (header.readableBytes() > 0) {
        writtenHeader = copyByteBuf(header, target);
        totalBytesTransferred += writtenHeader;
        if (header.readableBytes() > 0) {
            return writtenHeader;
        }
    }
    // Bytes written for body in this call.
    long writtenBody = 0;
    if (body instanceof FileRegion) {
        writtenBody = ((FileRegion) body).transferTo(target, totalBytesTransferred - headerLength);
    } else if (body instanceof ByteBuf) {
        writtenBody = copyByteBuf((ByteBuf) body, target);
    }
    totalBytesTransferred += writtenBody;
    return writtenHeader + writtenBody;
} 
","/**
 * Construct a new MessageWithHeader.
 *
 * @param managedBuffer the {@link ManagedBuffer} that the message body came from. This needs to
 *                      be passed in so that the buffer can be freed when this message is
 *                      deallocated. Ownership of the caller's reference to this buffer is
 *                      transferred to this class, so if the caller wants to continue to use the
 *                      ManagedBuffer in other messages then they will need to call retain() on
 *                      it before passing it to this constructor. This may be null if and only if
 *                      `body` is a {@link FileRegion}.
 * @param header the message header.
 * @param body the message body. Must be either a {@link ByteBuf} or a {@link FileRegion}.
 * @param bodyLength the length of the message body, in bytes.
 */
 
","{
    Preconditions.checkArgument(body instanceof ByteBuf || body instanceof FileRegion, ""Body must be a ByteBuf or a FileRegion."");
    this.managedBuffer = managedBuffer;
    this.header = header;
    this.headerLength = header.readableBytes();
    this.body = body;
    this.bodyLength = bodyLength;
} 
","/**
 * When the write buffer size is larger than this limit, I/O will be done in chunks of this size.
 * The size should not be too large as it will waste underlying memory copy. e.g. If network
 * available buffer is smaller than this limit, the data cannot be sent within one single write
 * operation while it still will make memory copy with this size.
 */
 
","Field NIO_BUFFER_LIMIT
",3,155
OneWayMessage.java,25,42,0.5952380952380952,"
 * A RPC that does not expect a reply, which is handled by a remote
 * {@link org.apache.spark.network.server.RpcHandler}.
 ",,,,,,,2,120
ChunkFetchFailureException.java,19,9,2.111111111111111,"
 * General exception caused by a remote exception while fetching a chunk.
 ",,,,,,,1,73
RpcResponseCallback.java,27,6,4.5,"
 * Callback for the result of a single RPC. This will be invoked once with either success or
 * failure.
 ","/**
 * Successful serialized result from server.
 *
 * After `onSuccess` returns, `response` will be recycled and its content will become invalid.
 * Please copy the content of `response` if you want to use it after `onSuccess` returns.
 */
 
/**
 * Exception either propagated from server or raised on client side.
 */
 
","onSuccess 
onFailure 
",,,,,2,102
StreamInterceptor.java,22,59,0.3728813559322034,"
 * An interceptor that is registered with the frame decoder to feed stream data to a
 * callback.
 ",,,,,,,2,95
TransportClient.java,113,208,0.5432692307692307,"
 * Client for fetching consecutive chunks of a pre-negotiated stream. This API is intended to allow
 * efficient transfer of a large amount of data, broken up into chunks with size ranging from
 * hundreds of KB to a few MB.
 *
 * Note that while this client deals with the fetching of chunks from a stream (i.e., data plane),
 * the actual setup of the streams is done outside the scope of the transport layer. The convenience
 * method ""sendRPC"" is provided to enable control plane communication between the client and server
 * to perform this setup.
 *
 * For example, a typical workflow might be:
 * client.sendRPC(new OpenFile(""/foo"")) --&gt; returns StreamId = 100
 * client.fetchChunk(streamId = 100, chunkIndex = 0, callback)
 * client.fetchChunk(streamId = 100, chunkIndex = 1, callback)
 * ...
 * client.sendRPC(new CloseStream(100))
 *
 * Construct an instance of TransportClient using {@link TransportClientFactory}. A single
 * TransportClient may be used for multiple streams, but any given stream must be restricted to a
 * single client, in order to avoid out-of-order responses.
 *
 * NB: This class is used to make requests to the server, while {@link TransportResponseHandler} is
 * responsible for handling responses from the server.
 *
 * Concurrency: thread safe and can be called from multiple threads.
 ","/**
 * Returns the ID used by the client to authenticate itself when authentication is enabled.
 *
 * @return The client ID, or null if authentication is disabled.
 */
 
/**
 * Sets the authenticated client ID. This is meant to be used by the authentication layer.
 *
 * Trying to set a different client ID after it's been set will result in an exception.
 */
 
/**
 * Requests a single chunk from the remote side, from the pre-negotiated streamId.
 *
 * Chunk indices go from 0 onwards. It is valid to request the same chunk multiple times, though
 * some streams may not support this.
 *
 * Multiple fetchChunk requests may be outstanding simultaneously, and the chunks are guaranteed
 * to be returned in the same order that they were requested, assuming only a single
 * TransportClient is used to fetch the chunks.
 *
 * @param streamId Identifier that refers to a stream in the remote StreamManager. This should
 *                 be agreed upon by client and server beforehand.
 * @param chunkIndex 0-based index of the chunk to fetch
 * @param callback Callback invoked upon successful receipt of chunk, or upon any failure.
 */
 
/**
 * Request to stream the data with the given stream ID from the remote end.
 *
 * @param streamId The stream to fetch.
 * @param callback Object to call with the stream data.
 */
 
/**
 * Sends an opaque message to the RpcHandler on the server-side. The callback will be invoked
 * with the server's response or upon any failure.
 *
 * @param message The message to send.
 * @param callback Callback to handle the RPC's reply.
 * @return The RPC's id.
 */
 
/**
 * Send data to the remote end as a stream.  This differs from stream() in that this is a request
 * to *send* data to the remote end, not to receive it from the remote.
 *
 * @param meta meta data associated with the stream, which will be read completely on the
 *             receiving end before the stream itself.
 * @param data this will be streamed to the remote end to allow for transferring large amounts
 *             of data without reading into memory.
 * @param callback handles the reply -- onSuccess will only be called when both message and data
 *                 are received successfully.
 */
 
/**
 * Synchronously sends an opaque message to the RpcHandler on the server-side, waiting for up to
 * a specified timeout for a response.
 */
 
/**
 * Sends an opaque message to the RpcHandler on the server-side. No reply is expected for the
 * message, and no delivery guarantees are made.
 *
 * @param message The message to send.
 */
 
/**
 * Removes any state associated with the given RPC.
 *
 * @param requestId The RPC id returned by {@link #sendRpc(ByteBuffer, RpcResponseCallback)}.
 */
 
/**
 * Mark this channel as having timed out.
 */
 
","{
    return clientId;
} 
{
    Preconditions.checkState(clientId == null, ""Client ID has already been set."");
    this.clientId = id;
} 
{
    if (logger.isDebugEnabled()) {
        logger.debug(""Sending fetch chunk request {} to {}"", chunkIndex, getRemoteAddress(channel));
    }
    StreamChunkId streamChunkId = new StreamChunkId(streamId, chunkIndex);
    StdChannelListener listener = new StdChannelListener(streamChunkId) {

        @Override
        void handleFailure(String errorMsg, Throwable cause) {
            handler.removeFetchRequest(streamChunkId);
            callback.onFailure(chunkIndex, new IOException(errorMsg, cause));
        }
    };
    handler.addFetchRequest(streamChunkId, callback);
    channel.writeAndFlush(new ChunkFetchRequest(streamChunkId)).addListener(listener);
} 
{
    StdChannelListener listener = new StdChannelListener(streamId) {

        @Override
        void handleFailure(String errorMsg, Throwable cause) throws Exception {
            callback.onFailure(streamId, new IOException(errorMsg, cause));
        }
    };
    if (logger.isDebugEnabled()) {
        logger.debug(""Sending stream request for {} to {}"", streamId, getRemoteAddress(channel));
    }
    // Need to synchronize here so that the callback is added to the queue and the RPC is
    // written to the socket atomically, so that callbacks are called in the right order
    // when responses arrive.
    synchronized (this) {
        handler.addStreamCallback(streamId, callback);
        channel.writeAndFlush(new StreamRequest(streamId)).addListener(listener);
    }
} 
{
    if (logger.isTraceEnabled()) {
        logger.trace(""Sending RPC to {}"", getRemoteAddress(channel));
    }
    long requestId = requestId();
    handler.addRpcRequest(requestId, callback);
    RpcChannelListener listener = new RpcChannelListener(requestId, callback);
    channel.writeAndFlush(new RpcRequest(requestId, new NioManagedBuffer(message))).addListener(listener);
    return requestId;
} 
{
    if (logger.isTraceEnabled()) {
        logger.trace(""Sending RPC to {}"", getRemoteAddress(channel));
    }
    long requestId = requestId();
    handler.addRpcRequest(requestId, callback);
    RpcChannelListener listener = new RpcChannelListener(requestId, callback);
    channel.writeAndFlush(new UploadStream(requestId, meta, data)).addListener(listener);
    return requestId;
} 
{
    final SettableFuture<ByteBuffer> result = SettableFuture.create();
    sendRpc(message, new RpcResponseCallback() {

        @Override
        public void onSuccess(ByteBuffer response) {
            try {
                ByteBuffer copy = ByteBuffer.allocate(response.remaining());
                copy.put(response);
                // flip ""copy"" to make it readable
                copy.flip();
                result.set(copy);
            } catch (Throwable t) {
                logger.warn(""Error in responding PRC callback"", t);
                result.setException(t);
            }
        }

        @Override
        public void onFailure(Throwable e) {
            result.setException(e);
        }
    });
    try {
        return result.get(timeoutMs, TimeUnit.MILLISECONDS);
    } catch (ExecutionException e) {
        throw Throwables.propagate(e.getCause());
    } catch (Exception e) {
        throw Throwables.propagate(e);
    }
} 
{
    channel.writeAndFlush(new OneWayMessage(new NioManagedBuffer(message)));
} 
{
    handler.removeRpcRequest(requestId);
} 
{
    this.timedOut = true;
} 
",,,,,24,1280
ChunkReceivedCallback.java,37,6,6.166666666666667,"
 * Callback for the result of a single chunk result. For a single stream, the callbacks are
 * guaranteed to be called by the same thread in the same order as the requests for chunks were
 * made.
 *
 * Note that if a general stream failure occurs, all outstanding chunk requests may be failed.
 ","/**
 * Called upon receipt of a particular chunk.
 *
 * The given buffer will initially have a refcount of 1, but will be release()'d as soon as this
 * call returns. You must therefore either retain() the buffer or copy its contents before
 * returning.
 */
 
/**
 * Called upon failure to fetch a particular chunk. Note that this may actually be called due
 * to failure to fetch a prior chunk in this stream.
 *
 * After receiving a failure, the stream may or may not be valid. The client should not assume
 * that the server's side of the stream has been closed.
 */
 
","onSuccess 
onFailure 
",,,,,5,286
TransportClientFactory.java,63,201,0.31343283582089554,"
 * Factory for creating {@link TransportClient}s by using createClient.
 *
 * The factory maintains a connection pool to other hosts and should return the same
 * TransportClient for the same remote host. It also shares a single worker thread pool for
 * all TransportClients.
 *
 * TransportClients will be reused whenever possible. Prior to completing the creation of a new
 * TransportClient, all given {@link TransportClientBootstrap}s will be run.
 | A simple data structure to track the pool of clients between two peer nodes. ","/**
 * Create a {@link TransportClient} connecting to the given remote host / port.
 *
 * We maintains an array of clients (size determined by spark.shuffle.io.numConnectionsPerPeer)
 * and randomly picks one to use. If no client was previously created in the randomly selected
 * spot, this function creates a new client and places it there.
 *
 * Prior to the creation of a new TransportClient, we will execute all
 * {@link TransportClientBootstrap}s that are registered with this factory.
 *
 * This blocks until a connection is successfully established and fully bootstrapped.
 *
 * Concurrency: This method is safe to call from multiple threads.
 */
 
/**
 * Create a completely new {@link TransportClient} to the given remote host / port.
 * This connection is not pooled.
 *
 * As with {@link #createClient(String, int)}, this method is blocking.
 */
 
/**
 * Create a completely new {@link TransportClient} to the remote address.
 */
 
/**
 * Close all connections in the connection pool, and shutdown the worker thread pool.
 */
 
","{
    // Get connection from the connection pool first.
    // If it is not found or not active, create a new one.
    // Use unresolved address here to avoid DNS resolution each time we creates a client.
    final InetSocketAddress unresolvedAddress = InetSocketAddress.createUnresolved(remoteHost, remotePort);
    // Create the ClientPool if we don't have it yet.
    ClientPool clientPool = connectionPool.get(unresolvedAddress);
    if (clientPool == null) {
        connectionPool.putIfAbsent(unresolvedAddress, new ClientPool(numConnectionsPerPeer));
        clientPool = connectionPool.get(unresolvedAddress);
    }
    int clientIndex = rand.nextInt(numConnectionsPerPeer);
    TransportClient cachedClient = clientPool.clients[clientIndex];
    if (cachedClient != null && cachedClient.isActive()) {
        // Make sure that the channel will not timeout by updating the last use time of the
        // handler. Then check that the client is still alive, in case it timed out before
        // this code was able to update things.
        TransportChannelHandler handler = cachedClient.getChannel().pipeline().get(TransportChannelHandler.class);
        synchronized (handler) {
            handler.getResponseHandler().updateTimeOfLastRequest();
        }
        if (cachedClient.isActive()) {
            logger.trace(""Returning cached connection to {}: {}"", cachedClient.getSocketAddress(), cachedClient);
            return cachedClient;
        }
    }
    // If we reach here, we don't have an existing connection open. Let's create a new one.
    // Multiple threads might race here to create new connections. Keep only one of them active.
    final long preResolveHost = System.nanoTime();
    final InetSocketAddress resolvedAddress = new InetSocketAddress(remoteHost, remotePort);
    final long hostResolveTimeMs = (System.nanoTime() - preResolveHost) / 1000000;
    final String resolvMsg = resolvedAddress.isUnresolved() ? ""failed"" : ""succeed"";
    if (hostResolveTimeMs > 2000) {
        logger.warn(""DNS resolution {} for {} took {} ms"", resolvMsg, resolvedAddress, hostResolveTimeMs);
    } else {
        logger.trace(""DNS resolution {} for {} took {} ms"", resolvMsg, resolvedAddress, hostResolveTimeMs);
    }
    synchronized (clientPool.locks[clientIndex]) {
        cachedClient = clientPool.clients[clientIndex];
        if (cachedClient != null) {
            if (cachedClient.isActive()) {
                logger.trace(""Returning cached connection to {}: {}"", resolvedAddress, cachedClient);
                return cachedClient;
            } else {
                logger.info(""Found inactive connection to {}, creating a new one."", resolvedAddress);
            }
        }
        clientPool.clients[clientIndex] = createClient(resolvedAddress);
        return clientPool.clients[clientIndex];
    }
} 
{
    final InetSocketAddress address = new InetSocketAddress(remoteHost, remotePort);
    return createClient(address);
} 
{
    logger.debug(""Creating new connection to {}"", address);
    Bootstrap bootstrap = new Bootstrap();
    bootstrap.group(workerGroup).channel(socketChannelClass).option(ChannelOption.TCP_NODELAY, true).option(ChannelOption.SO_KEEPALIVE, true).option(ChannelOption.CONNECT_TIMEOUT_MILLIS, conf.connectionTimeoutMs()).option(ChannelOption.ALLOCATOR, pooledAllocator);
    if (conf.receiveBuf() > 0) {
        bootstrap.option(ChannelOption.SO_RCVBUF, conf.receiveBuf());
    }
    if (conf.sendBuf() > 0) {
        bootstrap.option(ChannelOption.SO_SNDBUF, conf.sendBuf());
    }
    final AtomicReference<TransportClient> clientRef = new AtomicReference<>();
    final AtomicReference<Channel> channelRef = new AtomicReference<>();
    bootstrap.handler(new ChannelInitializer<SocketChannel>() {

        @Override
        public void initChannel(SocketChannel ch) {
            TransportChannelHandler clientHandler = context.initializePipeline(ch);
            clientRef.set(clientHandler.getClient());
            channelRef.set(ch);
        }
    });
    // Connect to the remote server
    long preConnect = System.nanoTime();
    ChannelFuture cf = bootstrap.connect(address);
    if (!cf.await(conf.connectionTimeoutMs())) {
        throw new IOException(String.format(""Connecting to %s timed out (%s ms)"", address, conf.connectionTimeoutMs()));
    } else if (cf.cause() != null) {
        throw new IOException(String.format(""Failed to connect to %s"", address), cf.cause());
    }
    TransportClient client = clientRef.get();
    Channel channel = channelRef.get();
    assert client != null : ""Channel future completed successfully with null client"";
    // Execute any client bootstraps synchronously before marking the Client as successful.
    long preBootstrap = System.nanoTime();
    logger.debug(""Connection to {} successful, running bootstraps..."", address);
    try {
        for (TransportClientBootstrap clientBootstrap : clientBootstraps) {
            clientBootstrap.doBootstrap(client, channel);
        }
    } catch (Exception e) {
        // catch non-RuntimeExceptions too as bootstrap may be written in Scala
        long bootstrapTimeMs = (System.nanoTime() - preBootstrap) / 1000000;
        logger.error(""Exception while bootstrapping client after "" + bootstrapTimeMs + "" ms"", e);
        client.close();
        throw Throwables.propagate(e);
    }
    long postBootstrap = System.nanoTime();
    logger.info(""Successfully created connection to {} after {} ms ({} ms spent in bootstraps)"", address, (postBootstrap - preConnect) / 1000000, (postBootstrap - preBootstrap) / 1000000);
    return client;
} 
{
    // Go through all clients and close them if they are active.
    for (ClientPool clientPool : connectionPool.values()) {
        for (int i = 0; i < clientPool.clients.length; i++) {
            TransportClient client = clientPool.clients[i];
            if (client != null) {
                clientPool.clients[i] = null;
                JavaUtils.closeQuietly(client);
            }
        }
    }
    connectionPool.clear();
    if (workerGroup != null && !workerGroup.isShuttingDown()) {
        workerGroup.shutdownGracefully();
    }
} 
",,,"/**
 * Random number generator for picking connections between peers.
 */
 
","Field rand
",9,517
TransportResponseHandler.java,31,212,0.14622641509433962,"
 * Handler that processes server responses, in response to requests issued from a
 * [[TransportClient]]. It works by tracking the list of outstanding requests (and their callbacks).
 *
 * Concurrency: thread safe and can be called from multiple threads.
 ","/**
 * Fire the failure callback for all outstanding requests. This is called when we have an
 * uncaught exception or pre-mature connection termination.
 */
 
/**
 * Returns total number of outstanding requests (fetch requests + rpcs)
 */
 
/**
 * Returns the time in nanoseconds of when the last request was sent out.
 */
 
/**
 * Updates the time of the last request to the current system time.
 */
 
","{
    for (Map.Entry<StreamChunkId, ChunkReceivedCallback> entry : outstandingFetches.entrySet()) {
        try {
            entry.getValue().onFailure(entry.getKey().chunkIndex, cause);
        } catch (Exception e) {
            logger.warn(""ChunkReceivedCallback.onFailure throws exception"", e);
        }
    }
    for (Map.Entry<Long, RpcResponseCallback> entry : outstandingRpcs.entrySet()) {
        try {
            entry.getValue().onFailure(cause);
        } catch (Exception e) {
            logger.warn(""RpcResponseCallback.onFailure throws exception"", e);
        }
    }
    for (Pair<String, StreamCallback> entry : streamCallbacks) {
        try {
            entry.getValue().onFailure(entry.getKey(), cause);
        } catch (Exception e) {
            logger.warn(""StreamCallback.onFailure throws exception"", e);
        }
    }
    // It's OK if new fetches appear, as they will fail immediately.
    outstandingFetches.clear();
    outstandingRpcs.clear();
    streamCallbacks.clear();
} 
{
    return outstandingFetches.size() + outstandingRpcs.size() + streamCallbacks.size() + (streamActive ? 1 : 0);
} 
{
    return timeOfLastRequestNs.get();
} 
{
    timeOfLastRequestNs.set(System.nanoTime());
} 
",,,"/**
 * Records the time (in system nanoseconds) that the last fetch or RPC request was sent.
 */
 
","Field timeOfLastRequestNs
",4,248
StreamCallbackWithID.java,16,4,4.0,,,,,,,,1,0
StreamCallback.java,27,8,3.375,"
 * Callback for streaming data. Stream data will be offered to the
 * {@link #onData(String, ByteBuffer)} method as it arrives. Once all the stream data is received,
 * {@link #onComplete(String)} will be called.
 * <p>
 * The network library guarantees that a single thread will call these methods at a time, but
 * different call may be made by different threads.
 ","/**
 * Called upon receipt of stream data.
 */
 
/**
 * Called when all data from the stream has been received.
 */
 
/**
 * Called if there's an error reading data from the stream.
 */
 
","onData 
onComplete 
onFailure 
",,,,,6,355
TransportClientBootstrap.java,26,5,5.2,"
 * A bootstrap which is executed on a TransportClient before it is returned to the user.
 * This enables an initial exchange of information (e.g., SASL authentication tokens) on a once-per-
 * connection basis.
 *
 * Since connections (and TransportClients) are reused as much as possible, it is generally
 * reasonable to perform an expensive bootstrapping operation, as they often share a lifespan with
 * the JVM itself.
 ","/**
 * Performs the bootstrapping operation, throwing an exception on failure.
 */
 
","doBootstrap 
",,,,,7,411
NioManagedBuffer.java,19,43,0.4418604651162791,"
 * A {@link ManagedBuffer} backed by {@link ByteBuffer}.
 ",,,,,,,1,56
NettyManagedBuffer.java,19,45,0.4222222222222222,"
 * A {@link ManagedBuffer} backed by a Netty {@link ByteBuf}.
 ",,,,,,,1,61
FileSegmentManagedBuffer.java,21,116,0.1810344827586207,"
 * A {@link ManagedBuffer} backed by a segment in a file.
 ",,,,,,,1,57
ManagedBuffer.java,21,116,0.1810344827586207,"
 * This interface provides an immutable view for data in the form of bytes. The implementation
 * should specify how the data is provided:
 *
 * - {@link FileSegmentManagedBuffer}: data backed by part of a file
 * - {@link NioManagedBuffer}: data backed by a NIO ByteBuffer
 * - {@link NettyManagedBuffer}: data backed by a Netty ByteBuf
 *
 * The concrete buffer implementation might be managed outside the JVM garbage collector.
 * For example, in the case of {@link NettyManagedBuffer}, the buffers are reference counted.
 * In that case, if the buffer is going to be passed around to a different thread, retain/release
 * should be called.
 ","/**
 * Number of bytes of the data. If this buffer will decrypt for all of the views into the data,
 * this is the size of the decrypted data.
 */
 
/**
 * Exposes this buffer's data as an InputStream. The underlying implementation does not
 * necessarily check for the length of bytes read, so the caller is responsible for making sure
 * it does not go over the limit.
 */
 
/**
 * Increment the reference count by one if applicable.
 */
 
/**
 * If applicable, decrement the reference count by one and deallocates the buffer if the
 * reference count reaches zero.
 */
 
/**
 * Convert the buffer into an Netty object, used to write the data out. The return value is either
 * a {@link io.netty.buffer.ByteBuf} or a {@link io.netty.channel.FileRegion}.
 *
 * If this method returns a ByteBuf, then that buffer's reference count will be incremented and
 * the caller will be responsible for releasing this new reference.
 */
 
","size 
createInputStream 
retain 
release 
convertToNetty 
",,,,,11,623
TestShuffleDataContext.java,26,95,0.2736842105263158,"
 * Manages some sort-shuffle data, including the creation
 * and cleanup of directories that can be read by the {@link ExternalShuffleBlockResolver}.
 ","/**
 * Creates reducer blocks in a sort-based data format within our local dirs.
 */
 
/**
 * Creates spill file(s) within the local dirs.
 */
 
/**
 * Creates an ExecutorShuffleInfo object based on the given shuffle manager which targets this
 * context's directories.
 */
 
","{
    String blockId = ""shuffle_"" + shuffleId + ""_"" + mapId + ""_0"";
    OutputStream dataStream = null;
    DataOutputStream indexStream = null;
    boolean suppressExceptionsDuringClose = true;
    try {
        dataStream = new FileOutputStream(ExecutorDiskUtils.getFile(localDirs, subDirsPerLocalDir, blockId + "".data""));
        indexStream = new DataOutputStream(new FileOutputStream(ExecutorDiskUtils.getFile(localDirs, subDirsPerLocalDir, blockId + "".index"")));
        long offset = 0;
        indexStream.writeLong(offset);
        for (byte[] block : blocks) {
            offset += block.length;
            dataStream.write(block);
            indexStream.writeLong(offset);
        }
        suppressExceptionsDuringClose = false;
    } finally {
        Closeables.close(dataStream, suppressExceptionsDuringClose);
        Closeables.close(indexStream, suppressExceptionsDuringClose);
    }
} 
{
    String filename = ""temp_local_uuid"";
    insertFile(filename);
} 
{
    return new ExecutorShuffleInfo(localDirs, subDirsPerLocalDir, shuffleManager);
} 
",,,,,2,147
CleanupNonShuffleServiceServedFilesSuite.java,19,189,0.10052910052910052,,,,,,,,1,0
BlockTransferMessagesSuite.java,17,27,0.6296296296296297," Verifies that all BlockTransferMessages can be serialized correctly. ",,,,,,,1,70
ExternalShuffleIntegrationSuite.java,20,296,0.06756756756756757,,,,,,,,1,0
OneForOneBlockFetcherSuite.java,28,197,0.14213197969543148,,"/**
 * Begins a fetch on the given set of blocks by mocking out the server side of the RPC which
 * simply returns the given (BlockId, Block) pairs.
 * As ""blocks"" is a LinkedHashMap, the blocks are guaranteed to be returned in the same order
 * that they were inserted in.
 *
 * If a block's buffer is ""null"", an exception will be thrown instead.
 */
 
","{
    TransportClient client = mock(TransportClient.class);
    BlockFetchingListener listener = mock(BlockFetchingListener.class);
    OneForOneBlockFetcher fetcher = new OneForOneBlockFetcher(client, ""app-id"", ""exec-id"", blockIds, listener, transportConf);
    // Respond to the ""OpenBlocks"" message with an appropriate ShuffleStreamHandle with streamId 123
    doAnswer(invocationOnMock -> {
        BlockTransferMessage message = BlockTransferMessage.Decoder.fromByteBuffer((ByteBuffer) invocationOnMock.getArguments()[0]);
        RpcResponseCallback callback = (RpcResponseCallback) invocationOnMock.getArguments()[1];
        callback.onSuccess(new StreamHandle(123, blocks.size()).toByteBuffer());
        assertEquals(expectMessage, message);
        return null;
    }).when(client).sendRpc(any(ByteBuffer.class), any(RpcResponseCallback.class));
    // Respond to each chunk request with a single buffer from our blocks array.
    AtomicInteger expectedChunkIndex = new AtomicInteger(0);
    Iterator<ManagedBuffer> blockIterator = blocks.values().iterator();
    doAnswer(invocation -> {
        try {
            long streamId = (Long) invocation.getArguments()[0];
            int myChunkIndex = (Integer) invocation.getArguments()[1];
            assertEquals(123, streamId);
            assertEquals(expectedChunkIndex.getAndIncrement(), myChunkIndex);
            ChunkReceivedCallback callback = (ChunkReceivedCallback) invocation.getArguments()[2];
            ManagedBuffer result = blockIterator.next();
            if (result != null) {
                callback.onSuccess(myChunkIndex, result);
            } else {
                callback.onFailure(myChunkIndex, new RuntimeException(""Failed "" + myChunkIndex));
            }
        } catch (Exception e) {
            e.printStackTrace();
            fail(""Unexpected failure"");
        }
        return null;
    }).when(client).fetchChunk(anyLong(), anyInt(), any());
    fetcher.start();
    return listener;
} 
",,,,,1,0
RetryingBlockFetcherSuite.java,48,204,0.23529411764705882,"
 * Tests retry logic by throwing IOExceptions and ensuring that subsequent attempts are made to
 * fetch the lost blocks.
 ","/**
 * Performs a set of interactions in response to block requests from a RetryingBlockFetcher.
 * Each interaction is a Map from BlockId to either ManagedBuffer or Exception. This interaction
 * means ""respond to the next block fetch request with these Successful buffers and these Failure
 * exceptions"". We verify that the expected block ids are exactly the ones requested.
 *
 * If multiple interactions are supplied, they will be used in order. This is useful for encoding
 * retries -- the first interaction may include an IOException, which causes a retry of some
 * subset of the original blocks in a second interaction.
 */
 
","{
    MapConfigProvider provider = new MapConfigProvider(ImmutableMap.of(""spark.shuffle.io.maxRetries"", ""2"", ""spark.shuffle.io.retryWait"", ""0""));
    TransportConf conf = new TransportConf(""shuffle"", provider);
    BlockFetchStarter fetchStarter = mock(BlockFetchStarter.class);
    Stubber stub = null;
    // Contains all blockIds that are referenced across all interactions.
    LinkedHashSet<String> blockIds = Sets.newLinkedHashSet();
    for (Map<String, Object> interaction : interactions) {
        blockIds.addAll(interaction.keySet());
        Answer<Void> answer = invocationOnMock -> {
            try {
                // Verify that the RetryingBlockFetcher requested the expected blocks.
                String[] requestedBlockIds = (String[]) invocationOnMock.getArguments()[0];
                String[] desiredBlockIds = interaction.keySet().toArray(new String[interaction.size()]);
                assertArrayEquals(desiredBlockIds, requestedBlockIds);
                // Now actually invoke the success/failure callbacks on each block.
                BlockFetchingListener retryListener = (BlockFetchingListener) invocationOnMock.getArguments()[1];
                for (Map.Entry<String, Object> block : interaction.entrySet()) {
                    String blockId = block.getKey();
                    Object blockValue = block.getValue();
                    if (blockValue instanceof ManagedBuffer) {
                        retryListener.onBlockFetchSuccess(blockId, (ManagedBuffer) blockValue);
                    } else if (blockValue instanceof Exception) {
                        retryListener.onBlockFetchFailure(blockId, (Exception) blockValue);
                    } else {
                        fail(""Can only handle ManagedBuffers and Exceptions, got "" + blockValue);
                    }
                }
                return null;
            } catch (Throwable e) {
                e.printStackTrace();
                throw e;
            }
        };
        // This is either the first stub, or should be chained behind the prior ones.
        if (stub == null) {
            stub = doAnswer(answer);
        } else {
            stub.doAnswer(answer);
        }
    }
    assertNotNull(stub);
    stub.when(fetchStarter).createAndStart(any(), any());
    String[] blockIdArray = blockIds.toArray(new String[blockIds.size()]);
    new RetryingBlockFetcher(conf, fetchStarter, blockIdArray, listener).start();
} 
",,,,,2,119
ExternalShuffleCleanupSuite.java,19,96,0.19791666666666666,,,,,,,,1,0
ExternalBlockHandlerSuite.java,20,169,0.11834319526627218,,,,,,,,1,0
ExternalShuffleBlockResolverSuite.java,24,119,0.20168067226890757,,,,,,,,1,0
ExternalShuffleSecuritySuite.java,19,100,0.19," Provides a secret key holder which always returns the given secret key, for a single appId. ","/**
 * Creates an ExternalBlockStoreClient and attempts to register with the server.
 */
 
","{
    TransportConf testConf = conf;
    if (encrypt) {
        testConf = new TransportConf(""shuffle"", new MapConfigProvider(ImmutableMap.of(""spark.authenticate.enableSaslEncryption"", ""true"")));
    }
    try (ExternalBlockStoreClient client = new ExternalBlockStoreClient(testConf, new TestSecretKeyHolder(appId, secretKey), true, 5000)) {
        client.init(appId);
        // Registration either succeeds or throws an exception.
        client.registerWithShuffleServer(TestUtils.getLocalHost(), server.getPort(), ""exec0"", new ExecutorShuffleInfo(new String[0], 0, ""org.apache.spark.shuffle.sort.SortShuffleManager""));
    }
} 
",,,,,1,93
ShuffleSecretManagerSuite.java,17,30,0.5666666666666667,,,,,,,,1,0
SaslIntegrationSuite.java,32,205,0.15609756097560976," Use a long timeout to account for slow / overloaded build machines. In the normal case,| RPC handler which simply responds with the message it received. ","/**
 * This test is not actually testing SASL behavior, but testing that the shuffle service
 * performs correct authorization checks based on the SASL authentication data.
 */
 
","{
    // Start a new server with the correct RPC handler to serve block data.
    ExternalShuffleBlockResolver blockResolver = mock(ExternalShuffleBlockResolver.class);
    ExternalBlockHandler blockHandler = new ExternalBlockHandler(new OneForOneStreamManager(), blockResolver);
    TransportServerBootstrap bootstrap = new SaslServerBootstrap(conf, secretKeyHolder);
    try (TransportContext blockServerContext = new TransportContext(conf, blockHandler);
        TransportServer blockServer = blockServerContext.createServer(Arrays.asList(bootstrap));
        // Create a client, and make a request to fetch blocks from a different app.
        TransportClientFactory clientFactory1 = blockServerContext.createClientFactory(Arrays.asList(new SaslClientBootstrap(conf, ""app-1"", secretKeyHolder)));
        TransportClient client1 = clientFactory1.createClient(TestUtils.getLocalHost(), blockServer.getPort())) {
        AtomicReference<Throwable> exception = new AtomicReference<>();
        CountDownLatch blockFetchLatch = new CountDownLatch(1);
        BlockFetchingListener listener = new BlockFetchingListener() {

            @Override
            public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
                blockFetchLatch.countDown();
            }

            @Override
            public void onBlockFetchFailure(String blockId, Throwable t) {
                exception.set(t);
                blockFetchLatch.countDown();
            }
        };
        String[] blockIds = { ""shuffle_0_1_2"", ""shuffle_0_3_4"" };
        OneForOneBlockFetcher fetcher = new OneForOneBlockFetcher(client1, ""app-2"", ""0"", blockIds, listener, conf);
        fetcher.start();
        blockFetchLatch.await();
        checkSecurityException(exception.get());
        // Register an executor so that the next steps work.
        ExecutorShuffleInfo executorInfo = new ExecutorShuffleInfo(new String[] { System.getProperty(""java.io.tmpdir"") }, 1, ""org.apache.spark.shuffle.sort.SortShuffleManager"");
        RegisterExecutor regmsg = new RegisterExecutor(""app-1"", ""0"", executorInfo);
        client1.sendRpcSync(regmsg.toByteBuffer(), TIMEOUT_MS);
        // Make a successful request to fetch blocks, which creates a new stream. But do not actually
        // fetch any blocks, to keep the stream open.
        OpenBlocks openMessage = new OpenBlocks(""app-1"", ""0"", blockIds);
        ByteBuffer response = client1.sendRpcSync(openMessage.toByteBuffer(), TIMEOUT_MS);
        StreamHandle stream = (StreamHandle) BlockTransferMessage.Decoder.fromByteBuffer(response);
        long streamId = stream.streamId;
        try (// Create a second client, authenticated with a different app ID, and try to read from
        // the stream created for the previous app.
        TransportClientFactory clientFactory2 = blockServerContext.createClientFactory(Arrays.asList(new SaslClientBootstrap(conf, ""app-2"", secretKeyHolder)));
            TransportClient client2 = clientFactory2.createClient(TestUtils.getLocalHost(), blockServer.getPort())) {
            CountDownLatch chunkReceivedLatch = new CountDownLatch(1);
            ChunkReceivedCallback callback = new ChunkReceivedCallback() {

                @Override
                public void onSuccess(int chunkIndex, ManagedBuffer buffer) {
                    chunkReceivedLatch.countDown();
                }

                @Override
                public void onFailure(int chunkIndex, Throwable t) {
                    exception.set(t);
                    chunkReceivedLatch.countDown();
                }
            };
            exception.set(null);
            client2.fetchChunk(streamId, 0, callback);
            chunkReceivedLatch.await();
            checkSecurityException(exception.get());
        }
    }
} 
",,,,,1,154
SimpleDownloadFile.java,23,53,0.4339622641509434,"
 * A DownloadFile that does not take any encryption settings into account for reading and
 * writing data.
 *
 * This does *not* mean the data in the file is un-encrypted -- it could be that the data is
 * already encrypted when its written, and subsequent layer is responsible for decrypting.
 ",,,,,,,5,283
OneForOneBlockFetcher.java,46,187,0.24598930481283424,"
 * Simple wrapper on top of a TransportClient which interprets each chunk as a whole block, and
 * invokes the BlockFetchingListener appropriately. This class is agnostic to the actual RPC
 * handler, as long as there is a single ""open blocks"" message which returns a ShuffleStreamHandle,
 * and Java serialization is used.
 *
 * Note that this typically corresponds to a
 * {@link org.apache.spark.network.server.OneForOneStreamManager} on the server side.
 | Callback invoked on receipt of each chunk. We equate a single chunk to a single block. ","/**
 * Analyze the pass in blockIds and create FetchShuffleBlocks message.
 * The blockIds has been sorted by mapId and reduceId. It's produced in
 * org.apache.spark.MapOutputTracker.convertMapStatuses.
 */
 
/**
 * Split the shuffleBlockId and return shuffleId, mapId and reduceIds.
 */
 
/**
 * Begins the fetching process, calling the listener with every block fetched.
 * The given message will be serialized with the Java serializer, and the RPC must return a
 * {@link StreamHandle}. We will send all fetch requests immediately, without throttling.
 */
 
/**
 * Invokes the ""onBlockFetchFailure"" callback for every listed block id.
 */
 
","{
    String[] firstBlock = splitBlockId(blockIds[0]);
    int shuffleId = Integer.parseInt(firstBlock[1]);
    boolean batchFetchEnabled = firstBlock.length == 5;
    HashMap<Long, ArrayList<Integer>> mapIdToReduceIds = new HashMap<>();
    for (String blockId : blockIds) {
        String[] blockIdParts = splitBlockId(blockId);
        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {
            throw new IllegalArgumentException(""Expected shuffleId="" + shuffleId + "", got:"" + blockId);
        }
        long mapId = Long.parseLong(blockIdParts[2]);
        if (!mapIdToReduceIds.containsKey(mapId)) {
            mapIdToReduceIds.put(mapId, new ArrayList<>());
        }
        mapIdToReduceIds.get(mapId).add(Integer.parseInt(blockIdParts[3]));
        if (batchFetchEnabled) {
            // When we read continuous shuffle blocks in batch, we will reuse reduceIds in
            // FetchShuffleBlocks to store the start and end reduce id for range
            // [startReduceId, endReduceId).
            assert (blockIdParts.length == 5);
            mapIdToReduceIds.get(mapId).add(Integer.parseInt(blockIdParts[4]));
        }
    }
    long[] mapIds = Longs.toArray(mapIdToReduceIds.keySet());
    int[][] reduceIdArr = new int[mapIds.length][];
    for (int i = 0; i < mapIds.length; i++) {
        reduceIdArr[i] = Ints.toArray(mapIdToReduceIds.get(mapIds[i]));
    }
    return new FetchShuffleBlocks(appId, execId, shuffleId, mapIds, reduceIdArr, batchFetchEnabled);
} 
{
    String[] blockIdParts = blockId.split(""_"");
    if (blockIdParts.length < 4 || blockIdParts.length > 5 || !blockIdParts[0].equals(""shuffle"")) {
        throw new IllegalArgumentException(""Unexpected shuffle block id format: "" + blockId);
    }
    return blockIdParts;
} 
{
    client.sendRpc(message.toByteBuffer(), new RpcResponseCallback() {

        @Override
        public void onSuccess(ByteBuffer response) {
            try {
                streamHandle = (StreamHandle) BlockTransferMessage.Decoder.fromByteBuffer(response);
                logger.trace(""Successfully opened blocks {}, preparing to fetch chunks."", streamHandle);
                // Immediately request all chunks -- we expect that the total size of the request is
                // reasonable due to higher level chunking in [[ShuffleBlockFetcherIterator]].
                for (int i = 0; i < streamHandle.numChunks; i++) {
                    if (downloadFileManager != null) {
                        client.stream(OneForOneStreamManager.genStreamChunkId(streamHandle.streamId, i), new DownloadCallback(i));
                    } else {
                        client.fetchChunk(streamHandle.streamId, i, chunkCallback);
                    }
                }
            } catch (Exception e) {
                logger.error(""Failed while starting block fetches after success"", e);
                failRemainingBlocks(blockIds, e);
            }
        }

        @Override
        public void onFailure(Throwable e) {
            logger.error(""Failed while starting block fetches"", e);
            failRemainingBlocks(blockIds, e);
        }
    });
} 
{
    for (String blockId : failedBlockIds) {
        try {
            listener.onBlockFetchFailure(blockId, e);
        } catch (Exception e2) {
            logger.error(""Error in block fetch failure callback"", e2);
        }
    }
} 
",,,,,8,534
BlockStoreClient.java,41,143,0.2867132867132867,"
 * Provides an interface for reading both shuffle files and RDD blocks, either from an Executor
 * or external service.
 ","/**
 * Fetch a sequence of blocks from a remote node asynchronously,
 *
 * Note that this API takes a sequence so the implementation can batch requests, and does not
 * return a future so the underlying implementation can invoke onBlockFetchSuccess as soon as
 * the data of a block is fetched, rather than waiting for all blocks to be fetched.
 *
 * @param host the host of the remote node.
 * @param port the port of the remote node.
 * @param execId the executor id.
 * @param blockIds block ids to fetch.
 * @param listener the listener to receive block fetching status.
 * @param downloadFileManager DownloadFileManager to create and clean temp files.
 *                        If it's not <code>null</code>, the remote blocks will be streamed
 *                        into temp shuffle files to reduce the memory usage, otherwise,
 *                        they will be kept in memory.
 */
 
/**
 * Get the shuffle MetricsSet from BlockStoreClient, this will be used in MetricsSystem to
 * get the Shuffle related metrics.
 */
 
","fetchBlocks 
{
    // Return an empty MetricSet by default.
    return () -> Collections.emptyMap();
} 
",,,,,2,117
BlockFetchingListener.java,24,7,3.4285714285714284,,"/**
 * Called once per successfully fetched block. After this call returns, data will be released
 * automatically. If the data will be passed to another thread, the receiver should retain()
 * and release() the buffer on their own, or copy the data to a new buffer.
 */
 
/**
 * Called at least once per block upon failures.
 */
 
","onBlockFetchSuccess 
onBlockFetchFailure 
",,,,,1,0
ExecutorDiskUtils.java,31,26,1.1923076923076923,,"/**
 * Hashes a filename into the corresponding local directory, in a manner consistent with
 * Spark's DiskBlockManager.getFile().
 */
 
/**
 * This method is needed to avoid the situation when multiple File instances for the
 * same pathname ""foo/bar"" are created, each with a separate copy of the ""foo/bar"" String.
 * According to measurements, in some scenarios such duplicate strings may waste a lot
 * of memory (~ 10% of the heap). To avoid that, we intern the pathname, and before that
 * we make sure that it's in a normalized form (contains no ""//"", ""///"" etc.) Otherwise,
 * the internal code in java.io.File would normalize it later, creating a new ""foo/bar""
 * String copy. Unfortunately, we cannot just reuse the normalization code that java.io.File
 * uses, since it is in the package-private class java.io.FileSystem.
 */
 
","{
    int hash = JavaUtils.nonNegativeHash(filename);
    String localDir = localDirs[hash % localDirs.length];
    int subDirId = (hash / localDirs.length) % subDirsPerLocalDir;
    return new File(createNormalizedInternedPathname(localDir, String.format(""%02x"", subDirId), filename));
} 
{
    String pathname = dir1 + File.separator + dir2 + File.separator + fname;
    Matcher m = MULTIPLE_SEPARATORS.matcher(pathname);
    pathname = m.replaceAll(""/"");
    // A single trailing slash needs to be taken care of separately
    if (pathname.length() > 1 && pathname.endsWith(""/"")) {
        pathname = pathname.substring(0, pathname.length() - 1);
    }
    return pathname.intern();
} 
",,,,,1,0
ShuffleIndexRecord.java,19,15,1.2666666666666666,"
 * Contains offset and length of the shuffle block data.
 ",,,,,,,1,56
RetryingBlockFetcher.java,85,119,0.7142857142857143,"
 * Wraps another BlockFetcher with the ability to automatically retry fetches which fail due to
 * IOExceptions, which we hope are due to transient network conditions.
 *
 * This fetcher provides stronger guarantees regarding the parent BlockFetchingListener. In
 * particular, the listener will be invoked exactly once per blockId, with a success or failure.
 | NOTE:| All of our non-final fields are synchronized under 'this' and should only be accessed/mutated| while inside a synchronized block.|
   * Used to initiate the first fetch for all blocks, and subsequently for retrying the fetch on any
   * remaining blocks.
   | NOTE:| All of our non-final fields are synchronized under 'this' and should only be accessed/mutated| while inside a synchronized block.|
   * Our RetryListener intercepts block fetch responses and forwards them to our parent listener.
   * Note that in the event of a retry, we will immediately replace the 'currentListener' field,
   * indicating that any responses from non-current Listeners should be ignored.
   ","/**
 * Creates a new BlockFetcher to fetch the given block ids which may do some synchronous
 * bootstrapping followed by fully asynchronous block fetching.
 * The BlockFetcher must eventually invoke the Listener on every input blockId, or else this
 * method must throw an exception.
 *
 * This method should always attempt to get a new TransportClient from the
 * {@link org.apache.spark.network.client.TransportClientFactory} in order to fix connection
 * issues.
 */
 
/**
 * Initiates the fetch of all blocks provided in the constructor, with possible retries in the
 * event of transient IOExceptions.
 */
 
/**
 * Fires off a request to fetch all blocks that have not been fetched successfully or permanently
 * failed (i.e., by a non-IOException).
 */
 
/**
 * Lightweight method which initiates a retry in a different thread. The retry will involve
 * calling fetchAllOutstanding() after a configured wait time.
 */
 
/**
 * Returns true if we should retry due a block fetch failure. We will retry if and only if
 * the exception was an IOException and we haven't retried 'maxRetries' times already.
 */
 
","createAndStart 
{
    fetchAllOutstanding();
} 
{
    // Start by retrieving our shared state within a synchronized block.
    String[] blockIdsToFetch;
    int numRetries;
    RetryingBlockFetchListener myListener;
    synchronized (this) {
        blockIdsToFetch = outstandingBlocksIds.toArray(new String[outstandingBlocksIds.size()]);
        numRetries = retryCount;
        myListener = currentListener;
    }
    // Now initiate the fetch on all outstanding blocks, possibly initiating a retry if that fails.
    try {
        fetchStarter.createAndStart(blockIdsToFetch, myListener);
    } catch (Exception e) {
        logger.error(String.format(""Exception while beginning fetch of %s outstanding blocks %s"", blockIdsToFetch.length, numRetries > 0 ? ""(after "" + numRetries + "" retries)"" : """"), e);
        if (shouldRetry(e)) {
            initiateRetry();
        } else {
            for (String bid : blockIdsToFetch) {
                listener.onBlockFetchFailure(bid, e);
            }
        }
    }
} 
{
    retryCount += 1;
    currentListener = new RetryingBlockFetchListener();
    logger.info(""Retrying fetch ({}/{}) for {} outstanding blocks after {} ms"", retryCount, maxRetries, outstandingBlocksIds.size(), retryWaitTime);
    executorService.submit(() -> {
        Uninterruptibles.sleepUninterruptibly(retryWaitTime, TimeUnit.MILLISECONDS);
        fetchAllOutstanding();
    });
} 
{
    boolean isIOException = e instanceof IOException || (e.getCause() != null && e.getCause() instanceof IOException);
    boolean hasRemainingRetries = retryCount < maxRetries;
    return isIOException && hasRemainingRetries;
} 
",,,"/**
 * Shared executor service used for waiting and retrying.
 */
 
/**
 * Used to initiate new Block Fetches on our remaining blocks.
 */
 
/**
 * Parent listener which we delegate all successful or permanently failed block fetches to.
 */
 
/**
 * Max number of times we are allowed to retry.
 */
 
/**
 * Milliseconds to wait before each retry.
 */
 
/**
 * Number of times we've attempted to retry so far.
 */
 
/**
 * Set of all block ids which have not been fetched successfully or with a non-IO Exception.
 * A retry involves requesting every outstanding block. Note that since this is a LinkedHashSet,
 * input ordering is preserved, so we always request blocks in the same order the user provided.
 */
 
/**
 * The BlockFetchingListener that is active with our current BlockFetcher.
 * When we start a retry, we immediately replace this with a new Listener, which causes all any
 * old Listeners to ignore all further responses.
 */
 
","Field executorService
Field fetchStarter
Field listener
Field maxRetries
Field retryWaitTime
Field retryCount
Field outstandingBlocksIds
Field currentListener
",12,1025
BlocksRemoved.java,18,42,0.42857142857142855," The reply to remove blocks giving back the number of removed blocks. ",,,,,,,1,70
OpenBlocks.java,18,58,0.3103448275862069," Request to read a set of blocks. Returns {@link StreamHandle}. ",,,,,,,1,64
StreamHandle.java,21,48,0.4375,"
 * Identifier for a fixed number of chunks to read from a stream created by an ""open blocks""
 * message. This is used by {@link org.apache.spark.network.shuffle.OneForOneBlockFetcher}.
 ",,,,,,,2,182
RegisterExecutor.java,21,60,0.35,"
 * Initial registration message between an executor and its local shuffle server.
 * Returns nothing (empty byte array).
 ",,,,,,,2,118
UploadBlock.java,24,78,0.3076923076923077," Request to upload a block with a certain StorageLevel. Returns nothing (empty byte array). ",,,"/**
 * @param metadata Meta-information about block, typically StorageLevel.
 * @param blockData The actual block's bytes.
 */
 
","{
    this.appId = appId;
    this.execId = execId;
    this.blockId = blockId;
    this.metadata = metadata;
    this.blockData = blockData;
} 
",,,1,92
RemoveBlocks.java,18,58,0.3103448275862069," Request to remove a set of blocks. ",,,,,,,1,36
ExecutorShuffleInfo.java,20,62,0.3225806451612903," Contains all configuration necessary for locating the shuffle files of an executor. ",,,,,"/**
 * The base set of local directories that the executor stores its shuffle files in.
 */
 
/**
 * Number of subdirectories created within each localDir.
 */
 
/**
 * Shuffle manager (SortShuffleManager) that the executor is using.
 */
 
","Field localDirs
Field subDirsPerLocalDir
Field shuffleManager
",1,85
ShuffleServiceHeartbeat.java,20,23,0.8695652173913043,"
 * A heartbeat sent from the driver to the MesosExternalShuffleService.
 ",,,,,,,1,71
RegisterDriver.java,20,43,0.46511627906976744,"
 * A message sent from the driver to register with the MesosExternalShuffleService.
 ",,,,,,,1,83
FetchShuffleBlocks.java,24,108,0.2222222222222222," Request to read a set of blocks. Returns {@link StreamHandle}. ",,,,,,,1,64
UploadBlockStream.java,23,52,0.4423076923076923,"
 * A request to Upload a block, which the destination should receive as a stream.
 *
 * The actual block data is not contained here.  It will be passed to the StreamCallbackWithID
 * that is returned from RpcHandler.receiveStream()
 ",,,,,,,4,225
BlockTransferMessage.java,34,48,0.7083333333333334,"
 * Messages handled by the {@link ExternalBlockHandler}, or
 * by Spark's NettyBlockTransferService.
 *
 * At a high level:
 *   - OpenBlock is logically only handled by the NettyBlockTransferService, but for the capability
 *     for old version Spark, we still keep it in external shuffle service.
 *     It returns a StreamHandle.
 *   - UploadBlock is only handled by the NettyBlockTransferService.
 *   - RegisterExecutor is only handled by the external shuffle service.
 *   - RemoveBlocks is only handled by the external shuffle service.
 *   - FetchShuffleBlocks is handled by both services for shuffle files. It returns a StreamHandle.
 | Preceding every serialized message is its type, which allows us to deserialize it. | NB: Java does not support static methods in interfaces, so we must put this in a static class.","/**
 * Deserializes the 'type' byte followed by the message itself.
 */
 
/**
 * Serializes the 'type' byte followed by the message itself.
 */
 
","{
    ByteBuf buf = Unpooled.wrappedBuffer(msg);
    byte type = buf.readByte();
    switch(type) {
        case 0:
            return OpenBlocks.decode(buf);
        case 1:
            return UploadBlock.decode(buf);
        case 2:
            return RegisterExecutor.decode(buf);
        case 3:
            return StreamHandle.decode(buf);
        case 4:
            return RegisterDriver.decode(buf);
        case 5:
            return ShuffleServiceHeartbeat.decode(buf);
        case 6:
            return UploadBlockStream.decode(buf);
        case 7:
            return RemoveBlocks.decode(buf);
        case 8:
            return BlocksRemoved.decode(buf);
        case 9:
            return FetchShuffleBlocks.decode(buf);
        default:
            throw new IllegalArgumentException(""Unknown message type: "" + type);
    }
} 
{
    // Allow room for encoded message, plus the type byte
    ByteBuf buf = Unpooled.buffer(encodedLength() + 1);
    buf.writeByte(type().id);
    encode(buf);
    assert buf.writableBytes() == 0 : ""Writable bytes remain: "" + buf.writableBytes();
    return buf.nioBuffer();
} 
",,,,,12,805
DownloadFileManager.java,25,6,4.166666666666667,"
 * A manager to create temp block files used when fetching remote data to reduce the memory usage.
 * It will clean files when they won't be used any more.
 ","/**
 * Create a temp block file.
 */
 
/**
 * Register a temp file to clean up when it won't be used any more. Return whether the
 * file is registered successfully. If `false`, the caller should clean up the file by itself.
 */
 
","createTempFile 
registerTempFileToClean 
",,,,,2,153
ExternalShuffleBlockResolver.java,74,325,0.2276923076923077,"
 * Manages converting shuffle BlockIds into physical segments of local files, from a process outside
 * of Executors. Each Executor must register its own configuration about where it stores its files
 * (local dirs) and how (shuffle manager). The logic for retrieval of individual files is replicated
 * from Spark's IndexShuffleBlockResolver.
 | Simply encodes an executor's full ID, which is appId + execId. ","/**
 * Registers a new Executor with all the configuration we need to find its shuffle files.
 */
 
/**
 * Obtains a FileSegmentManagedBuffer from a single block (shuffleId, mapId, reduceId).
 */
 
/**
 * Obtains a FileSegmentManagedBuffer from (shuffleId, mapId, [startReduceId, endReduceId)).
 * We make assumptions about how the hash and sort based shuffles store their data.
 */
 
/**
 * Removes our metadata of all executors registered for the given application, and optionally
 * also deletes the local directories associated with the executors of that application in a
 * separate thread.
 *
 * It is not valid to call registerExecutor() for an executor with this appId after invoking
 * this method.
 */
 
/**
 * Removes all the files which cannot be served by the external shuffle service (non-shuffle and
 * non-RDD files) in any local directories associated with the finished executor.
 */
 
/**
 * Synchronously deletes each directory one at a time.
 * Should be executed in its own thread, as this may take a long time.
 */
 
/**
 * Synchronously deletes files not served by shuffle service in each directory recursively.
 * Should be executed in its own thread, as this may take a long time.
 */
 
/**
 * Sort-based shuffle data uses an index called ""shuffle_ShuffleId_MapId_0.index"" into a data file
 * called ""shuffle_ShuffleId_MapId_0.data"". This logic is from IndexShuffleBlockResolver,
 * and the block id format is from ShuffleDataBlockId and ShuffleIndexBlockId.
 */
 
","{
    AppExecId fullId = new AppExecId(appId, execId);
    logger.info(""Registered executor {} with {}"", fullId, executorInfo);
    if (!knownManagers.contains(executorInfo.shuffleManager)) {
        throw new UnsupportedOperationException(""Unsupported shuffle manager of executor: "" + executorInfo);
    }
    try {
        if (db != null) {
            byte[] key = dbAppExecKey(fullId);
            byte[] value = mapper.writeValueAsString(executorInfo).getBytes(StandardCharsets.UTF_8);
            db.put(key, value);
        }
    } catch (Exception e) {
        logger.error(""Error saving registered executors"", e);
    }
    executors.put(fullId, executorInfo);
} 
{
    return getContinuousBlocksData(appId, execId, shuffleId, mapId, reduceId, reduceId + 1);
} 
{
    ExecutorShuffleInfo executor = executors.get(new AppExecId(appId, execId));
    if (executor == null) {
        throw new RuntimeException(String.format(""Executor is not registered (appId=%s, execId=%s)"", appId, execId));
    }
    return getSortBasedShuffleBlockData(executor, shuffleId, mapId, startReduceId, endReduceId);
} 
{
    logger.info(""Application {} removed, cleanupLocalDirs = {}"", appId, cleanupLocalDirs);
    Iterator<Map.Entry<AppExecId, ExecutorShuffleInfo>> it = executors.entrySet().iterator();
    while (it.hasNext()) {
        Map.Entry<AppExecId, ExecutorShuffleInfo> entry = it.next();
        AppExecId fullId = entry.getKey();
        final ExecutorShuffleInfo executor = entry.getValue();
        // Only touch executors associated with the appId that was removed.
        if (appId.equals(fullId.appId)) {
            it.remove();
            if (db != null) {
                try {
                    db.delete(dbAppExecKey(fullId));
                } catch (IOException e) {
                    logger.error(""Error deleting {} from executor state db"", appId, e);
                }
            }
            if (cleanupLocalDirs) {
                logger.info(""Cleaning up executor {}'s {} local dirs"", fullId, executor.localDirs.length);
                // Execute the actual deletion in a different thread, as it may take some time.
                directoryCleaner.execute(() -> deleteExecutorDirs(executor.localDirs));
            }
        }
    }
} 
{
    logger.info(""Clean up non-shuffle and non-RDD files associated with the finished executor {}"", executorId);
    AppExecId fullId = new AppExecId(appId, executorId);
    final ExecutorShuffleInfo executor = executors.get(fullId);
    if (executor == null) {
        // Executor not registered, skip clean up of the local directories.
        logger.info(""Executor is not registered (appId={}, execId={})"", appId, executorId);
    } else {
        logger.info(""Cleaning up non-shuffle and non-RDD files in executor {}'s {} local dirs"", fullId, executor.localDirs.length);
        // Execute the actual deletion in a different thread, as it may take some time.
        directoryCleaner.execute(() -> deleteNonShuffleServiceServedFiles(executor.localDirs));
    }
} 
{
    for (String localDir : dirs) {
        try {
            JavaUtils.deleteRecursively(new File(localDir));
            logger.debug(""Successfully cleaned up directory: {}"", localDir);
        } catch (Exception e) {
            logger.error(""Failed to delete directory: "" + localDir, e);
        }
    }
} 
{
    FilenameFilter filter = (dir, name) -> {
        // Don't delete shuffle data, shuffle index files or cached RDD files.
        return !name.endsWith("".index"") && !name.endsWith("".data"") && (!rddFetchEnabled || !name.startsWith(""rdd_""));
    };
    for (String localDir : dirs) {
        try {
            JavaUtils.deleteRecursively(new File(localDir), filter);
            logger.debug(""Successfully cleaned up files not served by shuffle service in directory: {}"", localDir);
        } catch (Exception e) {
            logger.error(""Failed to delete files not served by shuffle service in directory: "" + localDir, e);
        }
    }
} 
{
    File indexFile = ExecutorDiskUtils.getFile(executor.localDirs, executor.subDirsPerLocalDir, ""shuffle_"" + shuffleId + ""_"" + mapId + ""_0.index"");
    try {
        ShuffleIndexInformation shuffleIndexInformation = shuffleIndexCache.get(indexFile);
        ShuffleIndexRecord shuffleIndexRecord = shuffleIndexInformation.getIndex(startReduceId, endReduceId);
        return new FileSegmentManagedBuffer(conf, ExecutorDiskUtils.getFile(executor.localDirs, executor.subDirsPerLocalDir, ""shuffle_"" + shuffleId + ""_"" + mapId + ""_0.data""), shuffleIndexRecord.getOffset(), shuffleIndexRecord.getLength());
    } catch (ExecutionException e) {
        throw new RuntimeException(""Failed to open file: "" + indexFile, e);
    }
} 
",,,"/**
 * This a common prefix to the key for each app registration we stick in leveldb, so they
 * are easy to find, since leveldb lets you search based on prefix.
 */
 
/**
 *  Caches index file information so that we can avoid open/close the index files
 *  for each block fetch.
 */
 
","Field APP_KEY_PREFIX
Field shuffleIndexCache
",5,402
DownloadFile.java,23,53,0.4339622641509434,"
 * A handle on the file used when fetching remote data to disk.  Used to ensure the lifecycle of
 * writing the data, reading it back, and then cleaning it up is followed.  Specific implementations
 * may also handle encryption.  The data can be read only via DownloadFileWritableChannel,
 * which ensures data is not read until after the writer is closed.
 ","/**
 * Delete the file.
 *
 * @return  <code>true</code> if and only if the file or directory is
 *          successfully deleted; <code>false</code> otherwise
 */
 
/**
 * A channel for writing data to the file.  This special channel allows access to the data for
 * reading, after the channel is closed, via {@link DownloadFileWritableChannel#closeAndRead()}.
 */
 
/**
 * The path of the file, intended only for debug purposes.
 */
 
","delete 
openForWriting 
path 
",,,,,4,350
ExternalBlockStoreClient.java,41,143,0.2867132867132867,"
 * Client for reading both RDD blocks and shuffle blocks which points to an external
 * (outside of executor) server. This is instead of reading blocks directly from other executors
 * (via BlockTransferService), which has the downside of losing the data if we lose the executors.
 ","/**
 * Initializes the BlockStoreClient, specifying this Executor's appId.
 * Must be called before any other method on the BlockStoreClient.
 */
 
/**
 * Registers this executor with an external shuffle server. This registration is required to
 * inform the shuffle server about where and how we store our shuffle files.
 *
 * @param host Host of shuffle server.
 * @param port Port of shuffle server.
 * @param execId This Executor's id.
 * @param executorInfo Contains all info necessary for the service to find our shuffle files.
 */
 
","{
    this.appId = appId;
    TransportContext context = new TransportContext(conf, new NoOpRpcHandler(), true, true);
    List<TransportClientBootstrap> bootstraps = Lists.newArrayList();
    if (authEnabled) {
        bootstraps.add(new AuthClientBootstrap(conf, appId, secretKeyHolder));
    }
    clientFactory = context.createClientFactory(bootstraps);
} 
{
    checkInit();
    try (TransportClient client = clientFactory.createClient(host, port)) {
        ByteBuffer registerMessage = new RegisterExecutor(appId, execId, executorInfo).toByteBuffer();
        client.sendRpcSync(registerMessage, registrationTimeoutMs);
    }
} 
","/**
 * Creates an external shuffle client, with SASL optionally enabled. If SASL is not enabled,
 * then secretKeyHolder may be null.
 */
 
","{
    this.conf = conf;
    this.secretKeyHolder = secretKeyHolder;
    this.authEnabled = authEnabled;
    this.registrationTimeoutMs = registrationTimeoutMs;
} 
",,,3,276
ExternalBlockHandler.java,55,289,0.1903114186851211,"
 * RPC Handler for a server which can serve both RDD blocks and shuffle blocks from outside
 * of an Executor process.
 *
 * Handles registering executors and opening shuffle or disk persisted RDD blocks from them.
 * Blocks are registered with the ""one-for-one"" strategy, meaning each Transport-layer Chunk
 * is equivalent to one block.
 |
   * A simple class to wrap all shuffle service wrapper metrics
   ","/**
 * Removes an application (once it has been terminated), and optionally will clean up any
 * local directories associated with the executors of that application in a separate thread.
 */
 
/**
 * Clean up any non-shuffle files in any local directories associated with an finished executor.
 */
 
/**
 * Register an (application, executor) with the given shuffle info.
 *
 * The ""re-"" is meant to highlight the intended use of this method -- when this service is
 * restarted, this is used to restore the state of executors from before the restart.  Normal
 * registration will happen via a message handled in receive()
 *
 * @param appExecId
 * @param executorInfo
 */
 
","{
    blockManager.applicationRemoved(appId, cleanupLocalDirs);
} 
{
    blockManager.executorRemoved(executorId, appId);
} 
{
    blockManager.registerExecutor(appExecId.appId, appExecId.execId, executorInfo);
} 
","/**
 * Enables mocking out the StreamManager and BlockManager.
 */
 
","{
    this.metrics = new ShuffleMetrics();
    this.streamManager = streamManager;
    this.blockManager = blockManager;
} 
",,,8,394
ShuffleIndexInformation.java,31,30,1.0333333333333334,"
 * Keeps the index information for a particular map output
 * as an in-memory LongBuffer.
 ","/**
 * Size of the index file
 * @return size
 */
 
/**
 * Get index offset for a particular reducer.
 */
 
/**
 * Get index offset for the reducer range of [startReduceId, endReduceId).
 */
 
","{
    return size;
} 
{
    return getIndex(reduceId, reduceId + 1);
} 
{
    long offset = offsets.get(startReduceId);
    long nextOffset = offsets.get(endReduceId);
    return new ShuffleIndexRecord(offset, nextOffset - offset);
} 
",,,"/**
 * offsets as long buffer
 */
 
","Field offsets
",2,87
DownloadFileWritableChannel.java,20,6,3.3333333333333335,"
 * A channel for writing data which is fetched to disk, which allows access to the written data only
 * after the writer has been closed.  Used with DownloadFile and DownloadFileManager.
 ",,,,,,,2,184
Constants.java,6,89,0.06741573033707865,,,,,,,,1,0
ShuffleSecretManager.java,45,33,1.3636363636363635,"
 * A class that manages shuffle secret used by the external shuffle service.
 ","/**
 * Register an application with its secret.
 * Executors need to first authenticate themselves with the same secret before
 * fetching shuffle files written by other executors in this application.
 */
 
/**
 * Register an application with its secret specified as a byte buffer.
 */
 
/**
 * Unregister an application along with its secret.
 * This is called when the application terminates.
 */
 
/**
 * Return the Spark user for authenticating SASL connections.
 */
 
/**
 * Return the secret key registered with the given application.
 * This key is used to authenticate the executors before they can fetch shuffle files
 * written by this application from the external shuffle service. If the specified
 * application is not registered, return null.
 */
 
","{
    // Always put the new secret information to make sure it's the most up to date.
    // Otherwise we have to specifically look at the application attempt in addition
    // to the applicationId since the secrets change between application attempts on yarn.
    shuffleSecretMap.put(appId, shuffleSecret);
    logger.info(""Registered shuffle secret for application {}"", appId);
} 
{
    registerApp(appId, JavaUtils.bytesToString(shuffleSecret));
} 
{
    shuffleSecretMap.remove(appId);
    logger.info(""Unregistered shuffle secret for application {}"", appId);
} 
{
    return SPARK_SASL_USER;
} 
{
    return shuffleSecretMap.get(appId);
} 
",,,,,1,76
ExtendedSQLTest.java,16,10,1.6,,,,,,,,1,0
DockerTest.java,16,7,2.2857142857142856,,,,,,,,1,0
ExtendedHiveTest.java,16,7,2.2857142857142856,,,,,,,,1,0
ExtendedYarnTest.java,16,7,2.2857142857142856,,,,,,,,1,0
AlphaComponent.java,24,6,4.0,"
 * A new component of Spark which may have unstable API's.
 *
 * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first
 * line of the comment must be "":: AlphaComponent ::"" with no trailing blank line. This is because
 * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
 * comes first.
 ",,,,,,,6,361
Unstable.java,20,7,2.857142857142857,"
 * Unstable APIs, with no guarantee on stability.
 * Classes that are unannotated are considered Unstable.
 ",,,,,,,2,104
Evolving.java,20,7,2.857142857142857,"
 * APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.
 * Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).
 ",,,,,,,2,186
DeveloperApi.java,26,7,3.7142857142857144,"
 * A lower-level, unstable API intended for developers.
 *
 * Developer API's might change or be removed in minor versions of Spark.
 *
 * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first
 * line of the comment must be "":: DeveloperApi ::"" with no trailing blank line. This is because
 * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
 * comes first.
 ",,,,,,,8,429
Experimental.java,27,7,3.857142857142857,"
 * An experimental user-facing API.
 *
 * Experimental API's might change or be removed in minor versions of Spark, or be adopted as
 * first-class Spark API's.
 *
 * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first
 * line of the comment must be "":: Experimental ::"" with no trailing blank line. This is because
 * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
 * comes first.
 ",,,,,,,9,455
Private.java,29,7,4.142857142857143,"
 * A class that is considered private to the internals of Spark -- there is a high-likelihood
 * they will be changed in future versions of Spark.
 *
 * This should be used only when the standard Scala / Java means of protecting classes are
 * insufficient.  In particular, Java has no equivalent of private[spark], so we use this annotation
 * in its place.
 *
 * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first
 * line of the comment must be "":: Private ::"" with no trailing blank line. This is because
 * of the known issue that Scaladoc displays only either the annotation or the comment, whichever
 * comes first.
 ",,,,,,,11,644
Stable.java,21,7,3.0,"
 * Stable APIs that retain source and binary compatibility within a major release.
 * These interfaces can change from one major release to another major release
 * (e.g. from 1.0 to 2.0).
 ",,,,,,,3,184
CalendarIntervalSuite.java,17,229,0.07423580786026202,,,,,,,,1,0
UTF8StringSuite.java,35,720,0.04861111111111111,,,,,,,,1,0
Murmur3_x86_32Suite.java,26,91,0.2857142857142857,"
 * Test file based on Guava's Murmur3Hash32Test.
 ",,,,,,,1,48
LongArraySuite.java,16,20,0.8,,,,,,,,1,0
PlatformUtilSuite.java,20,122,0.16393442622950818,,,,,,,,1,0
BitSetMethods.java,56,61,0.9180327868852459,"
 * Methods for working with fixed-size uncompressed bitsets.
 *
 * We assume that the bitset data is word-aligned (that is, a multiple of 8 bytes in length).
 *
 * Each bit occupies exactly one bit of storage.
 ","/**
 * Sets the bit at the specified index to {@code true}.
 */
 
/**
 * Sets the bit at the specified index to {@code false}.
 */
 
/**
 * Returns {@code true} if the bit is set at the specified index.
 */
 
/**
 * Returns {@code true} if any bit is set.
 */
 
/**
 * Returns the index of the first bit that is set to true that occurs on or after the
 * specified starting index. If no such bit exists then {@code -1} is returned.
 * <p>
 * To iterate over the true bits in a BitSet, use the following loop:
 * <pre>
 * <code>
 *  for (long i = bs.nextSetBit(0, sizeInWords); i &gt;= 0;
 *    i = bs.nextSetBit(i + 1, sizeInWords)) {
 *    // operate on index i here
 *  }
 * </code>
 * </pre>
 *
 * @param fromIndex the index to start checking from (inclusive)
 * @param bitsetSizeInWords the size of the bitset, measured in 8-byte words
 * @return the index of the next set bit, or -1 if there is no such bit
 */
 
","{
    assert index >= 0 : ""index ("" + index + "") should >= 0"";
    // mod 64 and shift
    final long mask = 1L << (index & 0x3f);
    final long wordOffset = baseOffset + (index >> 6) * WORD_SIZE;
    final long word = Platform.getLong(baseObject, wordOffset);
    Platform.putLong(baseObject, wordOffset, word | mask);
} 
{
    assert index >= 0 : ""index ("" + index + "") should >= 0"";
    // mod 64 and shift
    final long mask = 1L << (index & 0x3f);
    final long wordOffset = baseOffset + (index >> 6) * WORD_SIZE;
    final long word = Platform.getLong(baseObject, wordOffset);
    Platform.putLong(baseObject, wordOffset, word & ~mask);
} 
{
    assert index >= 0 : ""index ("" + index + "") should >= 0"";
    // mod 64 and shift
    final long mask = 1L << (index & 0x3f);
    final long wordOffset = baseOffset + (index >> 6) * WORD_SIZE;
    final long word = Platform.getLong(baseObject, wordOffset);
    return (word & mask) != 0;
} 
{
    long addr = baseOffset;
    for (int i = 0; i < bitSetWidthInWords; i++, addr += WORD_SIZE) {
        if (Platform.getLong(baseObject, addr) != 0) {
            return true;
        }
    }
    return false;
} 
{
    int wi = fromIndex >> 6;
    if (wi >= bitsetSizeInWords) {
        return -1;
    }
    // Try to find the next set bit in the current word
    final int subIndex = fromIndex & 0x3f;
    long word = Platform.getLong(baseObject, baseOffset + wi * WORD_SIZE) >> subIndex;
    if (word != 0) {
        return (wi << 6) + subIndex + java.lang.Long.numberOfTrailingZeros(word);
    }
    // Find the next set bit in the rest of the words
    wi += 1;
    while (wi < bitsetSizeInWords) {
        word = Platform.getLong(baseObject, baseOffset + wi * WORD_SIZE);
        if (word != 0) {
            return (wi << 6) + java.lang.Long.numberOfTrailingZeros(word);
        }
        wi += 1;
    }
    return -1;
} 
",,,,,5,201
CalendarInterval.java,67,309,0.2168284789644013,"
 * The internal representation of interval type.
 ","/**
 * A function to generate regex which matches interval string's unit part like ""3 years"".
 *
 * First, we can leave out some units in interval string, and we only care about the value of
 * unit, so here we use non-capturing group to wrap the actual regex.
 * At the beginning of the actual regex, we should match spaces before the unit part.
 * Next is the number part, starts with an optional ""-"" to represent negative value. We use
 * capturing group to wrap this part as we need the value later.
 * Finally is the unit name, ends with an optional ""s"".
 */
 
/**
 * Convert a string to CalendarInterval. Return null if the input string is not a valid interval.
 * This method is case-insensitive.
 */
 
/**
 * Convert a string to CalendarInterval. This method can handle
 * strings without the `interval` prefix and throws IllegalArgumentException
 * when the input string is not a valid interval.
 *
 * @throws IllegalArgumentException if the string is not a valid internal.
 */
 
/**
 * Parse YearMonth string in form: [-]YYYY-MM
 *
 * adapted from HiveIntervalYearMonth.valueOf
 */
 
/**
 * Parse dayTime string in form: [-]d HH:mm:ss.nnnnnnnnn and [-]HH:mm:ss.nnnnnnnnn
 *
 * adapted from HiveIntervalDayTime.valueOf
 */
 
/**
 * Parse dayTime string in form: [-]d HH:mm:ss.nnnnnnnnn and [-]HH:mm:ss.nnnnnnnnn
 *
 * adapted from HiveIntervalDayTime.valueOf.
 * Below interval conversion patterns are supported:
 * - DAY TO (HOUR|MINUTE|SECOND)
 * - HOUR TO (MINUTE|SECOND)
 * - MINUTE TO SECOND
 */
 
/**
 * Parse second_nano string in ss.nnnnnnnnn format to microseconds
 */
 
","{
    return ""(?:\\s+(-?\\d+)\\s+"" + unit + ""s?)?"";
} 
{
    try {
        return fromCaseInsensitiveString(s);
    } catch (IllegalArgumentException e) {
        return null;
    }
} 
{
    if (s == null) {
        throw new IllegalArgumentException(""Interval cannot be null"");
    }
    String trimmed = s.trim();
    if (trimmed.isEmpty()) {
        throw new IllegalArgumentException(""Interval cannot be blank"");
    }
    String prefix = ""interval"";
    String intervalStr = trimmed;
    // Checks the given interval string does not start with the `interval` prefix
    if (!intervalStr.regionMatches(true, 0, prefix, 0, prefix.length())) {
        // Prepend `interval` if it does not present because
        // the regular expression strictly require it.
        intervalStr = prefix + "" "" + trimmed;
    } else if (intervalStr.length() == prefix.length()) {
        throw new IllegalArgumentException(""Interval string must have time units"");
    }
    Matcher m = p.matcher(intervalStr);
    if (!m.matches()) {
        throw new IllegalArgumentException(""Invalid interval: "" + s);
    }
    long months = toLong(m.group(1)) * 12 + toLong(m.group(2));
    long microseconds = toLong(m.group(3)) * MICROS_PER_WEEK;
    microseconds += toLong(m.group(4)) * MICROS_PER_DAY;
    microseconds += toLong(m.group(5)) * MICROS_PER_HOUR;
    microseconds += toLong(m.group(6)) * MICROS_PER_MINUTE;
    microseconds += toLong(m.group(7)) * MICROS_PER_SECOND;
    microseconds += toLong(m.group(8)) * MICROS_PER_MILLI;
    microseconds += toLong(m.group(9));
    return new CalendarInterval((int) months, microseconds);
} 
{
    CalendarInterval result = null;
    if (s == null) {
        throw new IllegalArgumentException(""Interval year-month string was null"");
    }
    s = s.trim();
    Matcher m = yearMonthPattern.matcher(s);
    if (!m.matches()) {
        throw new IllegalArgumentException(""Interval string does not match year-month format of 'y-m': "" + s);
    } else {
        try {
            int sign = m.group(1) != null && m.group(1).equals(""-"") ? -1 : 1;
            int years = (int) toLongWithRange(""year"", m.group(2), 0, Integer.MAX_VALUE);
            int months = (int) toLongWithRange(""month"", m.group(3), 0, 11);
            result = new CalendarInterval(sign * (years * 12 + months), 0);
        } catch (Exception e) {
            throw new IllegalArgumentException(""Error parsing interval year-month string: "" + e.getMessage(), e);
        }
    }
    return result;
} 
{
    return fromDayTimeString(s, ""day"", ""second"");
} 
{
    CalendarInterval result = null;
    if (s == null) {
        throw new IllegalArgumentException(""Interval day-time string was null"");
    }
    s = s.trim();
    Matcher m = dayTimePattern.matcher(s);
    if (!m.matches()) {
        throw new IllegalArgumentException(""Interval string does not match day-time format of 'd h:m:s.n': "" + s);
    } else {
        try {
            int sign = m.group(1) != null && m.group(1).equals(""-"") ? -1 : 1;
            long days = m.group(2) == null ? 0 : toLongWithRange(""day"", m.group(3), 0, Integer.MAX_VALUE);
            long hours = 0;
            long minutes;
            long seconds = 0;
            if (m.group(5) != null || from.equals(""minute"")) {
                // 'HH:mm:ss' or 'mm:ss minute'
                hours = toLongWithRange(""hour"", m.group(5), 0, 23);
                minutes = toLongWithRange(""minute"", m.group(6), 0, 59);
                seconds = toLongWithRange(""second"", m.group(7), 0, 59);
            } else if (m.group(8) != null) {
                // 'mm:ss.nn'
                minutes = toLongWithRange(""minute"", m.group(6), 0, 59);
                seconds = toLongWithRange(""second"", m.group(7), 0, 59);
            } else {
                // 'HH:mm'
                hours = toLongWithRange(""hour"", m.group(6), 0, 23);
                minutes = toLongWithRange(""second"", m.group(7), 0, 59);
            }
            // Hive allow nanosecond precision interval
            String nanoStr = m.group(9) == null ? null : (m.group(9) + ""000000000"").substring(0, 9);
            long nanos = toLongWithRange(""nanosecond"", nanoStr, 0L, 999999999L);
            switch(to) {
                case ""hour"":
                    minutes = 0;
                    seconds = 0;
                    nanos = 0;
                    break;
                case ""minute"":
                    seconds = 0;
                    nanos = 0;
                    break;
                case ""second"":
                    // No-op
                    break;
                default:
                    throw new IllegalArgumentException(String.format(""Cannot support (interval '%s' %s to %s) expression"", s, from, to));
            }
            result = new CalendarInterval(0, sign * (days * MICROS_PER_DAY + hours * MICROS_PER_HOUR + minutes * MICROS_PER_MINUTE + seconds * MICROS_PER_SECOND + nanos / 1000L));
        } catch (Exception e) {
            throw new IllegalArgumentException(""Error parsing interval day-time string: "" + e.getMessage(), e);
        }
    }
    return result;
} 
{
    String[] parts = secondNano.split(""\\."");
    if (parts.length == 1) {
        return toLongWithRange(""second"", parts[0], Long.MIN_VALUE / MICROS_PER_SECOND, Long.MAX_VALUE / MICROS_PER_SECOND) * MICROS_PER_SECOND;
    } else if (parts.length == 2) {
        long seconds = parts[0].equals("""") ? 0L : toLongWithRange(""second"", parts[0], Long.MIN_VALUE / MICROS_PER_SECOND, Long.MAX_VALUE / MICROS_PER_SECOND);
        long nanos = toLongWithRange(""nanosecond"", parts[1], 0L, 999999999L);
        return seconds * MICROS_PER_SECOND + nanos / 1000L;
    } else {
        throw new IllegalArgumentException(""Interval string does not match second-nano format of ss.nnnnnnnnn"");
    }
} 
",,,,,1,48
ByteArray.java,27,66,0.4090909090909091,,"/**
 * Writes the content of a byte array into a memory address, identified by an object and an
 * offset. The target memory address must already been allocated, and have enough space to
 * hold all the bytes in this string.
 */
 
/**
 * Returns a 64-bit integer that can be used as the prefix used in sorting.
 */
 
","{
    Platform.copyMemory(src, Platform.BYTE_ARRAY_OFFSET, target, targetOffset, src.length);
} 
{
    if (bytes == null) {
        return 0L;
    } else {
        final int minLen = Math.min(bytes.length, 8);
        long p = 0;
        for (int i = 0; i < minLen; ++i) {
            p |= ((long) Platform.getByte(bytes, Platform.BYTE_ARRAY_OFFSET + i) & 0xff) << (56 - 8 * i);
        }
        return p;
    }
} 
",,,,,1,0
UTF8String.java,347,978,0.35480572597137017,"
 * A UTF-8 String for internal Spark use.
 * <p>
 * A String encoded in UTF-8 as an Array[Byte], which can be used for comparison,
 * search, see http://en.wikipedia.org/wiki/UTF-8 for details.
 * <p>
 * Note: This is not designed for general use cases, should not be used outside SQL.
 |
   * Wrapper over `long` to allow result of parsing long from string to be accessed via reference.
   * This is done solely for better performance and is not expected to be used by end users.
   |
   * Wrapper over `int` to allow result of parsing integer from string to be accessed via reference.
   * This is done solely for better performance and is not expected to be used by end users.
   *
   * {@link LongWrapper} could have been used here but using `int` directly save the extra cost of
   * conversion from `long` to `int`
   ","/**
 * Creates an UTF8String from byte array, which should be encoded in UTF-8.
 *
 * Note: `bytes` will be hold by returned UTF8String.
 */
 
/**
 * Creates an UTF8String from byte array, which should be encoded in UTF-8.
 *
 * Note: `bytes` will be hold by returned UTF8String.
 */
 
/**
 * Creates an UTF8String from given address (base and offset) and length.
 */
 
/**
 * Creates an UTF8String from String.
 */
 
/**
 * Creates an UTF8String that contains `length` spaces.
 */
 
/**
 * Writes the content of this string into a memory address, identified by an object and an offset.
 * The target memory address must already been allocated, and have enough space to hold all the
 * bytes in this string.
 */
 
/**
 * Returns a {@link ByteBuffer} wrapping the base object if it is a byte array
 * or a copy of the data if the base object is not a byte array.
 *
 * Unlike getBytes this will not create a copy the array if this is a slice.
 */
 
/**
 * Returns the number of bytes for a code point with the first byte as `b`
 * @param b The first byte of a code point
 */
 
/**
 * Returns the number of bytes
 */
 
/**
 * Returns the number of code points in it.
 */
 
/**
 * Returns a 64-bit integer that can be used as the prefix used in sorting.
 */
 
/**
 * Returns the underline bytes, will be a copy of it if it's part of another array.
 */
 
/**
 * Returns a substring of this.
 * @param start the position of first code point
 * @param until the position after last code point, exclusive.
 */
 
/**
 * Returns whether this contains `substring` or not.
 */
 
/**
 * Returns the byte at position `i`.
 */
 
/**
 * Returns the upper case of this string
 */
 
/**
 * Returns the lower case of this string
 */
 
/**
 * Returns the title case of this string, that could be used as title.
 */
 
/**
 * Copy the bytes from the current UTF8String, and make a new UTF8String.
 * @param start the start position of the current UTF8String in bytes.
 * @param end the end position of the current UTF8String in bytes.
 * @return a new UTF8String in the position of [start, end] of current UTF8String bytes.
 */
 
/**
 * Trims space characters (ASCII 32) from both ends of this string.
 *
 * @return this string with no spaces at the start or end
 */
 
/**
 * Trims instances of the given trim string from both ends of this string.
 *
 * @param trimString the trim character string
 * @return this string with no occurrences of the trim string at the start or end, or `null`
 *  if `trimString` is `null`
 */
 
/**
 * Trims space characters (ASCII 32) from the start of this string.
 *
 * @return this string with no spaces at the start
 */
 
/**
 * Trims instances of the given trim string from the start of this string.
 *
 * @param trimString the trim character string
 * @return this string with no occurrences of the trim string at the start, or `null`
 *  if `trimString` is `null`
 */
 
/**
 * Trims space characters (ASCII 32) from the end of this string.
 *
 * @return this string with no spaces at the end
 */
 
/**
 * Trims instances of the given trim string from the end of this string.
 *
 * @param trimString the trim character string
 * @return this string with no occurrences of the trim string at the end, or `null`
 *  if `trimString` is `null`
 */
 
/**
 * Returns the position of the first occurrence of substr in
 * current string from the specified position (0-based index).
 *
 * @param v the string to be searched
 * @param start the start position of the current string for searching
 * @return the position of the first occurrence of substr, if not found, -1 returned.
 */
 
/**
 * Find the `str` from left to right.
 */
 
/**
 * Find the `str` from right to left.
 */
 
/**
 * Returns the substring from string str before count occurrences of the delimiter delim.
 * If count is positive, everything the left of the final delimiter (counting from left) is
 * returned. If count is negative, every to the right of the final delimiter (counting from the
 * right) is returned. subStringIndex performs a case-sensitive match when searching for delim.
 */
 
/**
 * Returns str, right-padded with pad to a length of len
 * For example:
 *   ('hi', 5, '??') =&gt; 'hi???'
 *   ('hi', 1, '??') =&gt; 'h'
 */
 
/**
 * Returns str, left-padded with pad to a length of len.
 * For example:
 *   ('hi', 5, '??') =&gt; '???hi'
 *   ('hi', 1, '??') =&gt; 'h'
 */
 
/**
 * Concatenates input strings together into a single string. Returns null if any input is null.
 */
 
/**
 * Concatenates input strings together into a single string using the separator.
 * A null input is skipped. For example, concat("","", ""a"", null, ""c"") would yield ""a,c"".
 */
 
/**
 * Parses this UTF8String to long.
 *
 * Note that, in this method we accumulate the result in negative format, and convert it to
 * positive format at the end, if this string is not started with '-'. This is because min value
 * is bigger than max value in digits, e.g. Long.MAX_VALUE is '9223372036854775807' and
 * Long.MIN_VALUE is '-9223372036854775808'.
 *
 * This code is mostly copied from LazyLong.parseLong in Hive.
 *
 * @param toLongResult If a valid `long` was parsed from this UTF8String, then its value would
 *                     be set in `toLongResult`
 * @return true if the parsing was successful else false
 */
 
/**
 * Parses this UTF8String to int.
 *
 * Note that, in this method we accumulate the result in negative format, and convert it to
 * positive format at the end, if this string is not started with '-'. This is because min value
 * is bigger than max value in digits, e.g. Integer.MAX_VALUE is '2147483647' and
 * Integer.MIN_VALUE is '-2147483648'.
 *
 * This code is mostly copied from LazyInt.parseInt in Hive.
 *
 * Note that, this method is almost same as `toLong`, but we leave it duplicated for performance
 * reasons, like Hive does.
 *
 * @param intWrapper If a valid `int` was parsed from this UTF8String, then its value would
 *                    be set in `intWrapper`
 * @return true if the parsing was successful else false
 */
 
/**
 * Levenshtein distance is a metric for measuring the distance of two strings. The distance is
 * defined by the minimum number of single-character edits (i.e. insertions, deletions or
 * substitutions) that are required to change one of the strings into the other.
 */
 
/**
 * Encodes a string into a Soundex value. Soundex is an encoding used to relate similar names,
 * but can also be used as a general purpose scheme to find word with similar phonemes.
 * https://en.wikipedia.org/wiki/Soundex
 */
 
","{
    if (bytes != null) {
        return new UTF8String(bytes, BYTE_ARRAY_OFFSET, bytes.length);
    } else {
        return null;
    }
} 
{
    if (bytes != null) {
        return new UTF8String(bytes, BYTE_ARRAY_OFFSET + offset, numBytes);
    } else {
        return null;
    }
} 
{
    return new UTF8String(base, offset, numBytes);
} 
{
    return str == null ? null : fromBytes(str.getBytes(StandardCharsets.UTF_8));
} 
{
    byte[] spaces = new byte[length];
    Arrays.fill(spaces, (byte) ' ');
    return fromBytes(spaces);
} 
{
    Platform.copyMemory(base, offset, target, targetOffset, numBytes);
} 
{
    if (base instanceof byte[] && offset >= BYTE_ARRAY_OFFSET) {
        final byte[] bytes = (byte[]) base;
        // the offset includes an object header... this is only needed for unsafe copies
        final long arrayOffset = offset - BYTE_ARRAY_OFFSET;
        // verify that the offset and length points somewhere inside the byte array
        // and that the offset can safely be truncated to a 32-bit integer
        if ((long) bytes.length < arrayOffset + numBytes) {
            throw new ArrayIndexOutOfBoundsException();
        }
        return ByteBuffer.wrap(bytes, (int) arrayOffset, numBytes);
    } else {
        return ByteBuffer.wrap(getBytes());
    }
} 
{
    final int offset = b & 0xFF;
    byte numBytes = bytesOfCodePointInUTF8[offset];
    // Skip the first byte disallowed in UTF-8
    return (numBytes == 0) ? 1 : numBytes;
} 
{
    return numBytes;
} 
{
    int len = 0;
    for (int i = 0; i < numBytes; i += numBytesForFirstByte(getByte(i))) {
        len += 1;
    }
    return len;
} 
{
    // Since JVMs are either 4-byte aligned or 8-byte aligned, we check the size of the string.
    // If size is 0, just return 0.
    // If size is between 0 and 4 (inclusive), assume data is 4-byte aligned under the hood and
    // use a getInt to fetch the prefix.
    // If size is greater than 4, assume we have at least 8 bytes of data to fetch.
    // After getting the data, we use a mask to mask out data that is not part of the string.
    long p;
    long mask = 0;
    if (IS_LITTLE_ENDIAN) {
        if (numBytes >= 8) {
            p = Platform.getLong(base, offset);
        } else if (numBytes > 4) {
            p = Platform.getLong(base, offset);
            mask = (1L << (8 - numBytes) * 8) - 1;
        } else if (numBytes > 0) {
            p = (long) Platform.getInt(base, offset);
            mask = (1L << (8 - numBytes) * 8) - 1;
        } else {
            p = 0;
        }
        p = java.lang.Long.reverseBytes(p);
    } else {
        // byteOrder == ByteOrder.BIG_ENDIAN
        if (numBytes >= 8) {
            p = Platform.getLong(base, offset);
        } else if (numBytes > 4) {
            p = Platform.getLong(base, offset);
            mask = (1L << (8 - numBytes) * 8) - 1;
        } else if (numBytes > 0) {
            p = ((long) Platform.getInt(base, offset)) << 32;
            mask = (1L << (8 - numBytes) * 8) - 1;
        } else {
            p = 0;
        }
    }
    p &= ~mask;
    return p;
} 
{
    // avoid copy if `base` is `byte[]`
    if (offset == BYTE_ARRAY_OFFSET && base instanceof byte[] && ((byte[]) base).length == numBytes) {
        return (byte[]) base;
    } else {
        byte[] bytes = new byte[numBytes];
        copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, numBytes);
        return bytes;
    }
} 
{
    if (until <= start || start >= numBytes) {
        return EMPTY_UTF8;
    }
    int i = 0;
    int c = 0;
    while (i < numBytes && c < start) {
        i += numBytesForFirstByte(getByte(i));
        c += 1;
    }
    int j = i;
    while (i < numBytes && c < until) {
        i += numBytesForFirstByte(getByte(i));
        c += 1;
    }
    if (i > j) {
        byte[] bytes = new byte[i - j];
        copyMemory(base, offset + j, bytes, BYTE_ARRAY_OFFSET, i - j);
        return fromBytes(bytes);
    } else {
        return EMPTY_UTF8;
    }
} 
{
    if (substring.numBytes == 0) {
        return true;
    }
    byte first = substring.getByte(0);
    for (int i = 0; i <= numBytes - substring.numBytes; i++) {
        if (getByte(i) == first && matchAt(substring, i)) {
            return true;
        }
    }
    return false;
} 
{
    return Platform.getByte(base, offset + i);
} 
{
    if (numBytes == 0) {
        return EMPTY_UTF8;
    }
    byte[] bytes = new byte[numBytes];
    bytes[0] = (byte) Character.toTitleCase(getByte(0));
    for (int i = 0; i < numBytes; i++) {
        byte b = getByte(i);
        if (numBytesForFirstByte(b) != 1) {
            // fallback
            return toUpperCaseSlow();
        }
        int upper = Character.toUpperCase((int) b);
        if (upper > 127) {
            // fallback
            return toUpperCaseSlow();
        }
        bytes[i] = (byte) upper;
    }
    return fromBytes(bytes);
} 
{
    if (numBytes == 0) {
        return EMPTY_UTF8;
    }
    byte[] bytes = new byte[numBytes];
    bytes[0] = (byte) Character.toTitleCase(getByte(0));
    for (int i = 0; i < numBytes; i++) {
        byte b = getByte(i);
        if (numBytesForFirstByte(b) != 1) {
            // fallback
            return toLowerCaseSlow();
        }
        int lower = Character.toLowerCase((int) b);
        if (lower > 127) {
            // fallback
            return toLowerCaseSlow();
        }
        bytes[i] = (byte) lower;
    }
    return fromBytes(bytes);
} 
{
    if (numBytes == 0) {
        return EMPTY_UTF8;
    }
    byte[] bytes = new byte[numBytes];
    for (int i = 0; i < numBytes; i++) {
        byte b = getByte(i);
        if (i == 0 || getByte(i - 1) == ' ') {
            if (numBytesForFirstByte(b) != 1) {
                // fallback
                return toTitleCaseSlow();
            }
            int upper = Character.toTitleCase(b);
            if (upper > 127) {
                // fallback
                return toTitleCaseSlow();
            }
            bytes[i] = (byte) upper;
        } else {
            bytes[i] = b;
        }
    }
    return fromBytes(bytes);
} 
{
    int len = end - start + 1;
    byte[] newBytes = new byte[len];
    copyMemory(base, offset + start, newBytes, BYTE_ARRAY_OFFSET, len);
    return UTF8String.fromBytes(newBytes);
} 
{
    int s = 0;
    // skip all of the space (0x20) in the left side
    while (s < this.numBytes && getByte(s) == 0x20) s++;
    if (s == this.numBytes) {
        // Everything trimmed
        return EMPTY_UTF8;
    }
    // skip all of the space (0x20) in the right side
    int e = this.numBytes - 1;
    while (e > s && getByte(e) == 0x20) e--;
    if (s == 0 && e == numBytes - 1) {
        // Nothing trimmed
        return this;
    }
    return copyUTF8String(s, e);
} 
{
    if (trimString != null) {
        return trimLeft(trimString).trimRight(trimString);
    } else {
        return null;
    }
} 
{
    int s = 0;
    // skip all of the space (0x20) in the left side
    while (s < this.numBytes && getByte(s) == 0x20) s++;
    if (s == 0) {
        // Nothing trimmed
        return this;
    }
    if (s == this.numBytes) {
        // Everything trimmed
        return EMPTY_UTF8;
    }
    return copyUTF8String(s, this.numBytes - 1);
} 
{
    if (trimString == null)
        return null;
    // the searching byte position in the source string
    int srchIdx = 0;
    // the first beginning byte position of a non-matching character
    int trimIdx = 0;
    while (srchIdx < numBytes) {
        UTF8String searchChar = copyUTF8String(srchIdx, srchIdx + numBytesForFirstByte(this.getByte(srchIdx)) - 1);
        int searchCharBytes = searchChar.numBytes;
        // try to find the matching for the searchChar in the trimString set
        if (trimString.find(searchChar, 0) >= 0) {
            trimIdx += searchCharBytes;
        } else {
            // no matching, exit the search
            break;
        }
        srchIdx += searchCharBytes;
    }
    if (srchIdx == 0) {
        // Nothing trimmed
        return this;
    }
    if (trimIdx >= numBytes) {
        // Everything trimmed
        return EMPTY_UTF8;
    }
    return copyUTF8String(trimIdx, numBytes - 1);
} 
{
    int e = numBytes - 1;
    // skip all of the space (0x20) in the right side
    while (e >= 0 && getByte(e) == 0x20) e--;
    if (e == numBytes - 1) {
        // Nothing trimmed
        return this;
    }
    if (e < 0) {
        // Everything trimmed
        return EMPTY_UTF8;
    }
    return copyUTF8String(0, e);
} 
{
    if (trimString == null)
        return null;
    int charIdx = 0;
    // number of characters from the source string
    int numChars = 0;
    // array of character length for the source string
    int[] stringCharLen = new int[numBytes];
    // array of the first byte position for each character in the source string
    int[] stringCharPos = new int[numBytes];
    // build the position and length array
    while (charIdx < numBytes) {
        stringCharPos[numChars] = charIdx;
        stringCharLen[numChars] = numBytesForFirstByte(getByte(charIdx));
        charIdx += stringCharLen[numChars];
        numChars++;
    }
    // index trimEnd points to the first no matching byte position from the right side of
    // the source string.
    int trimEnd = numBytes - 1;
    while (numChars > 0) {
        UTF8String searchChar = copyUTF8String(stringCharPos[numChars - 1], stringCharPos[numChars - 1] + stringCharLen[numChars - 1] - 1);
        if (trimString.find(searchChar, 0) >= 0) {
            trimEnd -= stringCharLen[numChars - 1];
        } else {
            break;
        }
        numChars--;
    }
    if (trimEnd == numBytes - 1) {
        // Nothing trimmed
        return this;
    }
    if (trimEnd < 0) {
        // Everything trimmed
        return EMPTY_UTF8;
    }
    return copyUTF8String(0, trimEnd);
} 
{
    if (v.numBytes() == 0) {
        return 0;
    }
    // locate to the start position.
    // position in byte
    int i = 0;
    // position in character
    int c = 0;
    while (i < numBytes && c < start) {
        i += numBytesForFirstByte(getByte(i));
        c += 1;
    }
    do {
        if (i + v.numBytes > numBytes) {
            return -1;
        }
        if (ByteArrayMethods.arrayEquals(base, offset + i, v.base, v.offset, v.numBytes)) {
            return c;
        }
        i += numBytesForFirstByte(getByte(i));
        c += 1;
    } while (i < numBytes);
    return -1;
} 
{
    assert (str.numBytes > 0);
    while (start <= numBytes - str.numBytes) {
        if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {
            return start;
        }
        start += 1;
    }
    return -1;
} 
{
    assert (str.numBytes > 0);
    while (start >= 0) {
        if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {
            return start;
        }
        start -= 1;
    }
    return -1;
} 
{
    if (delim.numBytes == 0 || count == 0) {
        return EMPTY_UTF8;
    }
    if (count > 0) {
        int idx = -1;
        while (count > 0) {
            idx = find(delim, idx + 1);
            if (idx >= 0) {
                count--;
            } else {
                // can not find enough delim
                return this;
            }
        }
        if (idx == 0) {
            return EMPTY_UTF8;
        }
        byte[] bytes = new byte[idx];
        copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, idx);
        return fromBytes(bytes);
    } else {
        int idx = numBytes - delim.numBytes + 1;
        count = -count;
        while (count > 0) {
            idx = rfind(delim, idx - 1);
            if (idx >= 0) {
                count--;
            } else {
                // can not find enough delim
                return this;
            }
        }
        if (idx + delim.numBytes == numBytes) {
            return EMPTY_UTF8;
        }
        int size = numBytes - delim.numBytes - idx;
        byte[] bytes = new byte[size];
        copyMemory(base, offset + idx + delim.numBytes, bytes, BYTE_ARRAY_OFFSET, size);
        return fromBytes(bytes);
    }
} 
{
    // number of char need to pad
    int spaces = len - this.numChars();
    if (spaces <= 0 || pad.numBytes() == 0) {
        // no padding at all, return the substring of the current string
        return substring(0, len);
    } else {
        int padChars = pad.numChars();
        // how many padding string needed
        int count = spaces / padChars;
        // the partial string of the padding
        UTF8String remain = pad.substring(0, spaces - padChars * count);
        byte[] data = new byte[this.numBytes + pad.numBytes * count + remain.numBytes];
        copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET, this.numBytes);
        int offset = this.numBytes;
        int idx = 0;
        while (idx < count) {
            copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
            ++idx;
            offset += pad.numBytes;
        }
        copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
        return UTF8String.fromBytes(data);
    }
} 
{
    // number of char need to pad
    int spaces = len - this.numChars();
    if (spaces <= 0 || pad.numBytes() == 0) {
        // no padding at all, return the substring of the current string
        return substring(0, len);
    } else {
        int padChars = pad.numChars();
        // how many padding string needed
        int count = spaces / padChars;
        // the partial string of the padding
        UTF8String remain = pad.substring(0, spaces - padChars * count);
        byte[] data = new byte[this.numBytes + pad.numBytes * count + remain.numBytes];
        int offset = 0;
        int idx = 0;
        while (idx < count) {
            copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
            ++idx;
            offset += pad.numBytes;
        }
        copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);
        offset += remain.numBytes;
        copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET + offset, numBytes());
        return UTF8String.fromBytes(data);
    }
} 
{
    // Compute the total length of the result.
    long totalLength = 0;
    for (int i = 0; i < inputs.length; i++) {
        if (inputs[i] != null) {
            totalLength += (long) inputs[i].numBytes;
        } else {
            return null;
        }
    }
    // Allocate a new byte array, and copy the inputs one by one into it.
    final byte[] result = new byte[Ints.checkedCast(totalLength)];
    int offset = 0;
    for (int i = 0; i < inputs.length; i++) {
        int len = inputs[i].numBytes;
        copyMemory(inputs[i].base, inputs[i].offset, result, BYTE_ARRAY_OFFSET + offset, len);
        offset += len;
    }
    return fromBytes(result);
} 
{
    if (separator == null) {
        return null;
    }
    // total number of bytes from the inputs
    int numInputBytes = 0;
    // number of non-null inputs
    int numInputs = 0;
    for (int i = 0; i < inputs.length; i++) {
        if (inputs[i] != null) {
            numInputBytes += inputs[i].numBytes;
            numInputs++;
        }
    }
    if (numInputs == 0) {
        // Return an empty string if there is no input, or all the inputs are null.
        return EMPTY_UTF8;
    }
    // Allocate a new byte array, and copy the inputs one by one into it.
    // The size of the new array is the size of all inputs, plus the separators.
    final byte[] result = new byte[numInputBytes + (numInputs - 1) * separator.numBytes];
    int offset = 0;
    for (int i = 0, j = 0; i < inputs.length; i++) {
        if (inputs[i] != null) {
            int len = inputs[i].numBytes;
            copyMemory(inputs[i].base, inputs[i].offset, result, BYTE_ARRAY_OFFSET + offset, len);
            offset += len;
            j++;
            // Add separator if this is not the last input.
            if (j < numInputs) {
                copyMemory(separator.base, separator.offset, result, BYTE_ARRAY_OFFSET + offset, separator.numBytes);
                offset += separator.numBytes;
            }
        }
    }
    return fromBytes(result);
} 
{
    if (numBytes == 0) {
        return false;
    }
    byte b = getByte(0);
    final boolean negative = b == '-';
    int offset = 0;
    if (negative || b == '+') {
        offset++;
        if (numBytes == 1) {
            return false;
        }
    }
    final byte separator = '.';
    final int radix = 10;
    final long stopValue = Long.MIN_VALUE / radix;
    long result = 0;
    while (offset < numBytes) {
        b = getByte(offset);
        offset++;
        if (b == separator) {
            // We allow decimals and will return a truncated integral in that case.
            // Therefore we won't throw an exception here (checking the fractional
            // part happens below.)
            break;
        }
        int digit;
        if (b >= '0' && b <= '9') {
            digit = b - '0';
        } else {
            return false;
        }
        // We are going to process the new digit and accumulate the result. However, before doing
        // this, if the result is already smaller than the stopValue(Long.MIN_VALUE / radix), then
        // result * 10 will definitely be smaller than minValue, and we can stop.
        if (result < stopValue) {
            return false;
        }
        result = result * radix - digit;
        // Since the previous result is less than or equal to stopValue(Long.MIN_VALUE / radix), we
        // can just use `result > 0` to check overflow. If result overflows, we should stop.
        if (result > 0) {
            return false;
        }
    }
    // This is the case when we've encountered a decimal separator. The fractional
    // part will not change the number, but we will verify that the fractional part
    // is well formed.
    while (offset < numBytes) {
        byte currentByte = getByte(offset);
        if (currentByte < '0' || currentByte > '9') {
            return false;
        }
        offset++;
    }
    if (!negative) {
        result = -result;
        if (result < 0) {
            return false;
        }
    }
    toLongResult.value = result;
    return true;
} 
{
    if (numBytes == 0) {
        return false;
    }
    byte b = getByte(0);
    final boolean negative = b == '-';
    int offset = 0;
    if (negative || b == '+') {
        offset++;
        if (numBytes == 1) {
            return false;
        }
    }
    final byte separator = '.';
    final int radix = 10;
    final int stopValue = Integer.MIN_VALUE / radix;
    int result = 0;
    while (offset < numBytes) {
        b = getByte(offset);
        offset++;
        if (b == separator) {
            // We allow decimals and will return a truncated integral in that case.
            // Therefore we won't throw an exception here (checking the fractional
            // part happens below.)
            break;
        }
        int digit;
        if (b >= '0' && b <= '9') {
            digit = b - '0';
        } else {
            return false;
        }
        // We are going to process the new digit and accumulate the result. However, before doing
        // this, if the result is already smaller than the stopValue(Integer.MIN_VALUE / radix), then
        // result * 10 will definitely be smaller than minValue, and we can stop
        if (result < stopValue) {
            return false;
        }
        result = result * radix - digit;
        // Since the previous result is less than or equal to stopValue(Integer.MIN_VALUE / radix),
        // we can just use `result > 0` to check overflow. If result overflows, we should stop
        if (result > 0) {
            return false;
        }
    }
    // This is the case when we've encountered a decimal separator. The fractional
    // part will not change the number, but we will verify that the fractional part
    // is well formed.
    while (offset < numBytes) {
        byte currentByte = getByte(offset);
        if (currentByte < '0' || currentByte > '9') {
            return false;
        }
        offset++;
    }
    if (!negative) {
        result = -result;
        if (result < 0) {
            return false;
        }
    }
    intWrapper.value = result;
    return true;
} 
{
    // Implementation adopted from org.apache.common.lang3.StringUtils.getLevenshteinDistance
    int n = numChars();
    int m = other.numChars();
    if (n == 0) {
        return m;
    } else if (m == 0) {
        return n;
    }
    UTF8String s, t;
    if (n <= m) {
        s = this;
        t = other;
    } else {
        s = other;
        t = this;
        int swap;
        swap = n;
        n = m;
        m = swap;
    }
    int[] p = new int[n + 1];
    int[] d = new int[n + 1];
    int[] swap;
    int i, i_bytes, j, j_bytes, num_bytes_j, cost;
    for (i = 0; i <= n; i++) {
        p[i] = i;
    }
    for (j = 0, j_bytes = 0; j < m; j_bytes += num_bytes_j, j++) {
        num_bytes_j = numBytesForFirstByte(t.getByte(j_bytes));
        d[0] = j + 1;
        for (i = 0, i_bytes = 0; i < n; i_bytes += numBytesForFirstByte(s.getByte(i_bytes)), i++) {
            if (s.getByte(i_bytes) != t.getByte(j_bytes) || num_bytes_j != numBytesForFirstByte(s.getByte(i_bytes))) {
                cost = 1;
            } else {
                cost = (ByteArrayMethods.arrayEquals(t.base, t.offset + j_bytes, s.base, s.offset + i_bytes, num_bytes_j)) ? 0 : 1;
            }
            d[i + 1] = Math.min(Math.min(d[i] + 1, p[i + 1] + 1), p[i] + cost);
        }
        swap = p;
        p = d;
        d = swap;
    }
    return p[n];
} 
{
    if (numBytes == 0) {
        return EMPTY_UTF8;
    }
    byte b = getByte(0);
    if ('a' <= b && b <= 'z') {
        b -= 32;
    } else if (b < 'A' || 'Z' < b) {
        // first character must be a letter
        return this;
    }
    byte[] sx = { '0', '0', '0', '0' };
    sx[0] = b;
    int sxi = 1;
    int idx = b - 'A';
    byte lastCode = US_ENGLISH_MAPPING[idx];
    for (int i = 1; i < numBytes; i++) {
        b = getByte(i);
        if ('a' <= b && b <= 'z') {
            b -= 32;
        } else if (b < 'A' || 'Z' < b) {
            // not a letter, skip it
            lastCode = '0';
            continue;
        }
        idx = b - 'A';
        byte code = US_ENGLISH_MAPPING[idx];
        if (code == '7') {
        // ignore it
        } else {
            if (code != '0' && code != lastCode) {
                sx[sxi++] = code;
                if (sxi > 3)
                    break;
            }
            lastCode = code;
        }
    }
    return UTF8String.fromBytes(sx);
} 
",,,"/**
 * A char in UTF-8 encoding can take 1-4 bytes depending on the first byte which
 * indicates the size of the char. See Unicode standard in page 126, Table 3-6:
 * http://www.unicode.org/versions/Unicode10.0.0/UnicodeStandard-10.0.pdf
 *
 * Binary    Hex          Comments
 * 0xxxxxxx  0x00..0x7F   Only byte of a 1-byte character encoding
 * 10xxxxxx  0x80..0xBF   Continuation bytes (1-3 continuation bytes)
 * 110xxxxx  0xC0..0xDF   First byte of a 2-byte character encoding
 * 1110xxxx  0xE0..0xEF   First byte of a 3-byte character encoding
 * 11110xxx  0xF0..0xF7   First byte of a 4-byte character encoding
 *
 * As a consequence of the well-formedness conditions specified in
 * Table 3-7 (page 126), the following byte values are disallowed in UTF-8:
 *   C0–C1, F5–FF.
 */
 
/**
 * Soundex mapping table
 */
 
","Field bytesOfCodePointInUTF8
Field US_ENGLISH_MAPPING
",15,796
MemoryBlock.java,46,23,2.0,"
 * A consecutive block of memory, starting at a {@link MemoryLocation} with a fixed size.
 ","/**
 * Returns the size of the memory block.
 */
 
/**
 * Creates a memory block pointing to the memory used by the long array.
 */
 
/**
 * Fills the memory block with the specified byte value.
 */
 
","{
    return length;
} 
{
    return new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8L);
} 
{
    Platform.setMemory(obj, offset, length, value);
} 
",,,"/**
 * Special `pageNumber` value for pages which were not allocated by TaskMemoryManagers
 */
 
/**
 * Special `pageNumber` value for marking pages that have been freed in the TaskMemoryManager.
 * We set `pageNumber` to this value in TaskMemoryManager.freePage() so that MemoryAllocator
 * can detect if pages which were allocated by TaskMemoryManager have been freed in the TMM
 * before being passed to MemoryAllocator.free() (it is an error to allocate a page in
 * TaskMemoryManager and then directly free it in a MemoryAllocator without going through
 * the TMM freePage() call).
 */
 
/**
 * Special `pageNumber` value for pages that have been freed by the MemoryAllocator. This allows
 * us to detect double-frees.
 */
 
/**
 * Optional page number; used when this MemoryBlock represents a page allocated by a
 * TaskMemoryManager. This field is public so that it can be modified by the TaskMemoryManager,
 * which lives in a different package.
 */
 
","Field NO_PAGE_NUMBER
Field FREED_IN_TMM_PAGE_NUMBER
Field FREED_IN_ALLOCATOR_PAGE_NUMBER
Field pageNumber
",1,89
HeapMemoryAllocator.java,28,77,0.36363636363636365,"
 * A simple {@link MemoryAllocator} that can allocate up to 16GB using a JVM long primitive array.
 ","/**
 * Returns true if allocations of the given size should go through the pooling mechanism and
 * false otherwise.
 */
 
","{
    // Very small allocations are less likely to benefit from pooling.
    return size >= POOLING_THRESHOLD_BYTES;
} 
",,,,,1,98
MemoryLocation.java,20,24,0.8333333333333334,"
 * A memory location. Tracked either by a memory address (with off-heap allocation),
 * or by an offset from a JVM object (on-heap allocation).
 ",,,,,,,2,141
UnsafeMemoryAllocator.java,22,29,0.7586206896551724,"
 * A simple {@link MemoryAllocator} that uses {@code Unsafe} to allocate off-heap memory.
 ",,,,,,,1,89
MemoryAllocator.java,28,77,0.36363636363636365,,"/**
 * Allocates a contiguous block of memory. Note that the allocated memory is not guaranteed
 * to be zeroed out (call `fill(0)` on the result if this is necessary).
 */
 
","allocate 
",,,"/**
 * Whether to fill newly allocated and deallocated memory with 0xa5 and 0x5a bytes respectively.
 * This helps catch misuse of uninitialized or freed memory, but imposes some overhead.
 */
 
","Field MEMORY_DEBUG_FILL_ENABLED
",1,0
Murmur3_x86_32.java,25,95,0.2631578947368421,"
 * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.
 ",,,,,,,1,75
LongArray.java,36,44,0.8181818181818182,"
 * An array of long values. Compared with native JVM arrays, this:
 * <ul>
 *   <li>supports using both on-heap and off-heap memory</li>
 *   <li>has no bound checking, and thus can crash the JVM process when assert is turned off</li>
 * </ul>
 ","/**
 * Returns the number of elements this array can hold.
 */
 
/**
 * Fill this all with 0L.
 */
 
/**
 * Sets the value at position {@code index}.
 */
 
/**
 * Returns the value at position {@code index}.
 */
 
","{
    return length;
} 
{
    for (long off = baseOffset; off < baseOffset + length * WIDTH; off += WIDTH) {
        Platform.putLong(baseObj, off, 0);
    }
} 
{
    assert index >= 0 : ""index ("" + index + "") should >= 0"";
    assert index < length : ""index ("" + index + "") should < length ("" + length + "")"";
    Platform.putLong(baseObj, baseOffset + index * WIDTH, value);
} 
{
    assert index >= 0 : ""index ("" + index + "") should >= 0"";
    assert index < length : ""index ("" + index + "") should < length ("" + length + "")"";
    return Platform.getLong(baseObj, baseOffset + index * WIDTH);
} 
",,,,,5,235
ByteArrayMethods.java,31,53,0.5849056603773585,,"/**
 * Returns the next number greater or equal num that is power of 2.
 */
 
/**
 * Optimized byte array equality check for byte arrays.
 * @return true if the arrays are equal, false otherwise
 */
 
","{
    final long highBit = Long.highestOneBit(num);
    return (highBit == num) ? num : highBit << 1;
} 
{
    int i = 0;
    // check if stars align and we can get both offsets to be aligned
    if ((leftOffset % 8) == (rightOffset % 8)) {
        while ((leftOffset + i) % 8 != 0 && i < length) {
            if (Platform.getByte(leftBase, leftOffset + i) != Platform.getByte(rightBase, rightOffset + i)) {
                return false;
            }
            i += 1;
        }
    }
    // for architectures that support unaligned accesses, chew it up 8 bytes at a time
    if (unaligned || (((leftOffset + i) % 8 == 0) && ((rightOffset + i) % 8 == 0))) {
        while (i <= length - 8) {
            if (Platform.getLong(leftBase, leftOffset + i) != Platform.getLong(rightBase, rightOffset + i)) {
                return false;
            }
            i += 8;
        }
    }
    // this will finish off the unaligned comparisons, or do the entire aligned
    // comparison whichever is needed.
    while (i < length) {
        if (Platform.getByte(leftBase, leftOffset + i) != Platform.getByte(rightBase, rightOffset + i)) {
            return false;
        }
        i += 1;
    }
    return true;
} 
",,,,,1,0
UnsafeAlignedOffset.java,27,29,0.9310344827586207,"
 * Class to make changes to record length offsets uniform through out
 * various areas of Apache Spark core and unsafe.  The SPARC platform
 * requires this because using a 4 byte Int for record lengths causes
 * the entire record of 8 byte Items to become misaligned by 4 bytes.
 * Using a 8 byte long for record length keeps things 8 byte aligned.
 ",,,,,,,5,341
Platform.java,55,231,0.23809523809523808,,"/**
 * @return true when running JVM is having sun's Unsafe package available in it and underlying
 *         system having unaligned-access capability.
 */
 
/**
 * Allocate a DirectByteBuffer, potentially bypassing the JVM's MaxDirectMemorySize limit.
 */
 
/**
 * Raises an exception bypassing compiler checks for checked exceptions.
 */
 
","{
    return unaligned;
} 
{
    try {
        if (CLEANER_CREATE_METHOD == null) {
            // Can't set a Cleaner (see comments on field), so need to allocate via normal Java APIs
            try {
                return ByteBuffer.allocateDirect(size);
            } catch (OutOfMemoryError oome) {
                // checkstyle.off: RegexpSinglelineJava
                throw new OutOfMemoryError(""Failed to allocate direct buffer ("" + oome.getMessage() + ""); try increasing -XX:MaxDirectMemorySize=... to, for example, your heap size"");
            // checkstyle.on: RegexpSinglelineJava
            }
        }
        // Otherwise, use internal JDK APIs to allocate a DirectByteBuffer while ignoring the JVM's
        // MaxDirectMemorySize limit (the default limit is too low and we do not want to
        // require users to increase it).
        long memory = allocateMemory(size);
        ByteBuffer buffer = (ByteBuffer) DBB_CONSTRUCTOR.newInstance(memory, size);
        try {
            DBB_CLEANER_FIELD.set(buffer, CLEANER_CREATE_METHOD.invoke(null, buffer, (Runnable) () -> freeMemory(memory)));
        } catch (IllegalAccessException | InvocationTargetException e) {
            freeMemory(memory);
            throw new IllegalStateException(e);
        }
        return buffer;
    } catch (Exception e) {
        throwException(e);
    }
    throw new IllegalStateException(""unreachable"");
} 
{
    _UNSAFE.throwException(t);
} 
",,,"/**
 * Limits the number of bytes to copy per {@link Unsafe#copyMemory(long, long, long)} to
 * allow safepoint polling during a large copy.
 */
 
","Field UNSAFE_COPY_THRESHOLD
",1,0
UTF8StringBuilder.java,22,64,0.34375,"
 * A helper class to write {@link UTF8String}s to an internal buffer and build the concatenated
 * {@link UTF8String} at the end.
 ",,,,,,,2,127
KVIterator.java,16,8,2.0,,,,,,,,1,0
HiveHasher.java,20,22,0.9090909090909091,"
 * Simulates Hive's hashing function from Hive v1.2.1
 * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode()
 ",,,,,,,2,132
InMemoryIteratorSuite.java,16,7,2.2857142857142856,,,,,,,,1,0
InMemoryStoreSuite.java,19,147,0.1292517006802721,,,,,,,,1,0
DBIteratorSuite.java,43,381,0.11286089238845144,,"/**
 * Returns a comparator that falls back to natural order if this comparator's ordering
 * returns equality for two elements. Used to mimic how the index sorts things internally.
 */
 
/**
 * Reverses the order of this comparator.
 */
 
/**
 * Implementations should override this method; it is called only once, before all tests are
 * run. Any state can be safely stored in static variables and cleaned up in a @AfterClass
 * handler.
 */
 
/**
 * Compares the two values and falls back to comparing the natural key of CustomType1
 * if they're the same, to mimic the behavior of the indexing code.
 */
 
/**
 * Could use assertEquals(), but that creates hard to read errors for large lists.
 */
 
","{
    return (t1, t2) -> {
        int result = BaseComparator.this.compare(t1, t2);
        if (result != 0) {
            return result;
        }
        return t1.key.compareTo(t2.key);
    };
} 
{
    return (t1, t2) -> -BaseComparator.this.compare(t1, t2);
} 
createStore 
{
    int result = v1.compareTo(v2);
    if (result != 0) {
        return result;
    }
    return ct1.key.compareTo(ct2.key);
} 
{
    Iterator<?> expectedIt = expected.iterator();
    Iterator<?> actualIt = actual.iterator();
    int count = 0;
    while (expectedIt.hasNext()) {
        if (!actualIt.hasNext()) {
            break;
        }
        count++;
        assertEquals(expectedIt.next(), actualIt.next());
    }
    String message;
    Object[] remaining;
    int expectedCount = count;
    int actualCount = count;
    if (expectedIt.hasNext()) {
        remaining = Iterators.toArray(expectedIt, Object.class);
        expectedCount += remaining.length;
        message = ""missing"";
    } else {
        remaining = Iterators.toArray(actualIt, Object.class);
        actualCount += remaining.length;
        message = ""stray"";
    }
    assertEquals(String.format(""Found %s elements: %s"", message, Arrays.asList(remaining)), expectedCount, actualCount);
} 
",,,,,1,0
LevelDBSuite.java,29,237,0.12236286919831224,,,,,,,,1,0
LevelDBIteratorSuite.java,16,24,0.6666666666666666,,,,,,,,1,0
CustomType1.java,16,35,0.45714285714285713,,,,,,,,1,0
ArrayKeyIndexType.java,16,20,0.8,,,,,,,,1,0
LevelDBBenchmark.java,30,206,0.14563106796116504,"
 * A set of small benchmarks for the LevelDB implementation.
 *
 * The benchmarks are run over two different types (one with just a natural index, and one
 * with a ref index), over a set of 2^20 elements, and the following tests are performed:
 *
 * - write (then update) elements in sequential natural key order
 * - write (then update) elements in random natural key order
 * - iterate over natural index, ascending and descending
 * - iterate over ref index, ascending and descending
 ",,,,,,,9,471
ArrayWrappersSuite.java,16,32,0.5,,,,,,,,1,0
LevelDBTypeInfoSuite.java,17,137,0.12408759124087591,,,,,,,,1,0
LevelDBTypeInfo.java,162,287,0.5644599303135889,"
 * Holds metadata about app-specific types stored in LevelDB. Serves as a cache for data collected
 * via reflection, to make it cheaper to access it multiple times.
 *
 * <p>
 * The hierarchy of keys stored in LevelDB looks roughly like the following. This hierarchy ensures
 * that iteration over indices is easy, and that updating values in the store is not overly
 * expensive. Of note, indices choose using more disk space (one value per key) instead of keeping
 * lists of pointers, which would be more expensive to update at runtime.
 * </p>
 *
 * <p>
 * Indentation defines when a sub-key lives under a parent key. In LevelDB, this means the full
 * key would be the concatenation of everything up to that point in the hierarchy, with each
 * component separated by a NULL byte.
 * </p>
 *
 * <pre>
 * +TYPE_NAME
 *   NATURAL_INDEX
 *     +NATURAL_KEY
 *     -
 *   -NATURAL_INDEX
 *   INDEX_NAME
 *     +INDEX_VALUE
 *       +NATURAL_KEY
 *     -INDEX_VALUE
 *     .INDEX_VALUE
 *       CHILD_INDEX_NAME
 *         +CHILD_INDEX_VALUE
 *           NATURAL_KEY_OR_DATA
 *         -
 *   -INDEX_NAME
 * </pre>
 *
 * <p>
 * Entity data (either the entity's natural key or a copy of the data) is stored in all keys
 * that end with ""+<something>"". A count of all objects that match a particular top-level index
 * value is kept at the end marker (""-<something>""). A count is also kept at the natural index's end
 * marker, to make it easy to retrieve the number of all elements of a particular type.
 * </p>
 *
 * <p>
 * To illustrate, given a type ""Foo"", with a natural index and a second index called ""bar"", you'd
 * have these keys and values in the store for two instances, one with natural key ""key1"" and the
 * other ""key2"", both with value ""yes"" for ""bar"":
 * </p>
 *
 * <pre>
 * Foo __main__ +key1   [data for instance 1]
 * Foo __main__ +key2   [data for instance 2]
 * Foo __main__ -       [count of all Foo]
 * Foo bar +yes +key1   [instance 1 key or data, depending on index type]
 * Foo bar +yes +key2   [instance 2 key or data, depending on index type]
 * Foo bar +yes -       [count of all Foo with ""bar=yes"" ]
 * </pre>
 *
 * <p>
 * Note that all indexed values are prepended with ""+"", even if the index itself does not have an
 * explicit end marker. This allows for easily skipping to the end of an index by telling LevelDB
 * to seek to the ""phantom"" end marker of the index. Throughout the code and comments, this part
 * of the full LevelDB key is generally referred to as the ""index value"" of the entity.
 * </p>
 *
 * <p>
 * Child indices are stored after their parent index. In the example above, let's assume there is
 * a child index ""child"", whose parent is ""bar"". If both instances have value ""no"" for this field,
 * the data in the store would look something like the following:
 * </p>
 *
 * <pre>
 * ...
 * Foo bar +yes -
 * Foo bar .yes .child +no +key1   [instance 1 key or data, depending on index type]
 * Foo bar .yes .child +no +key2   [instance 2 key or data, depending on index type]
 * ...
 * </pre>
 |
   * Models a single index in LevelDB. See top-level class's javadoc for a description of how the
   * keys are generated.
   ","/**
 * Creates a key prefix for child indices of this index. This allows the prefix to be
 * calculated only once, avoiding redundant work when multiple child indices of the
 * same parent index exist.
 */
 
/**
 * Gets the index value for a particular entity (which is the value of the field or method
 * tagged with the index annotation). This is used as part of the LevelDB key where the
 * entity (or its id) is stored.
 */
 
/**
 * The prefix for all keys that belong to this index.
 */
 
/**
 * The key where to start ascending iteration for entities whose value for the indexed field
 * match the given value.
 */
 
/**
 * The key for the index's end marker.
 */
 
/**
 * The key for the end marker for entries with the given value.
 */
 
/**
 * The full key in the index that identifies the given entity.
 */
 
/**
 * Add an entry to the index.
 *
 * @param batch Write batch with other related changes.
 * @param entity The entity being added to the index.
 * @param existing The entity being replaced in the index, or null.
 * @param data Serialized entity to store (when storing the entity, not a reference).
 * @param naturalKey The value's natural key (to avoid re-computing it for every index).
 * @param prefix The parent index prefix, if this is a child index.
 */
 
/**
 * Remove a value from the index.
 *
 * @param batch Write batch with other related changes.
 * @param entity The entity being removed, to identify the index entry to modify.
 * @param naturalKey The value's natural key (to avoid re-computing it for every index).
 * @param prefix The parent index prefix, if this is a child index.
 */
 
/**
 * Translates a value to be used as part of the store key.
 *
 * Integral numbers are encoded as a string in a way that preserves lexicographical
 * ordering. The string is prepended with a marker telling whether the number is negative
 * or positive (""*"" for negative and ""="" for positive are used since ""-"" and ""+"" have the
 * opposite of the desired order), and then the number is encoded into a hex string (so
 * it occupies twice the number of bytes as the original type).
 *
 * Arrays are encoded by encoding each element separately, separated by KEY_SEPARATOR.
 */
 
","{
    Preconditions.checkState(parent == null, ""Not a parent index."");
    return buildKey(name, toParentKey(value));
} 
{
    return accessor.get(entity);
} 
{
    checkParent(prefix);
    return (parent != null) ? buildKey(false, prefix, name) : buildKey(name);
} 
{
    checkParent(prefix);
    return (parent != null) ? buildKey(false, prefix, name, toKey(value)) : buildKey(name, toKey(value));
} 
{
    checkParent(prefix);
    return (parent != null) ? buildKey(false, prefix, name, END_MARKER) : buildKey(name, END_MARKER);
} 
{
    checkParent(prefix);
    return (parent != null) ? buildKey(false, prefix, name, toKey(value), END_MARKER) : buildKey(name, toKey(value), END_MARKER);
} 
{
    Object indexValue = getValue(entity);
    Preconditions.checkNotNull(indexValue, ""Null index value for %s in type %s."", name, type.getName());
    byte[] entityKey = start(prefix, indexValue);
    if (!isNatural) {
        entityKey = buildKey(false, entityKey, toKey(naturalIndex().getValue(entity)));
    }
    return entityKey;
} 
{
    addOrRemove(batch, entity, existing, data, naturalKey, prefix);
} 
{
    addOrRemove(batch, entity, null, null, naturalKey, prefix);
} 
{
    final byte[] result;
    if (value instanceof String) {
        byte[] str = ((String) value).getBytes(UTF_8);
        result = new byte[str.length + 1];
        result[0] = prefix;
        System.arraycopy(str, 0, result, 1, str.length);
    } else if (value instanceof Boolean) {
        result = new byte[] { prefix, (Boolean) value ? TRUE : FALSE };
    } else if (value.getClass().isArray()) {
        int length = Array.getLength(value);
        byte[][] components = new byte[length][];
        for (int i = 0; i < length; i++) {
            components[i] = toKey(Array.get(value, i));
        }
        result = buildKey(false, components);
    } else {
        int bytes;
        if (value instanceof Integer) {
            bytes = Integer.SIZE;
        } else if (value instanceof Long) {
            bytes = Long.SIZE;
        } else if (value instanceof Short) {
            bytes = Short.SIZE;
        } else if (value instanceof Byte) {
            bytes = Byte.SIZE;
        } else {
            throw new IllegalArgumentException(String.format(""Type %s not allowed as key."", value.getClass().getName()));
        }
        bytes = bytes / Byte.SIZE;
        byte[] key = new byte[bytes * 2 + 2];
        long longValue = ((Number) value).longValue();
        key[0] = prefix;
        key[1] = longValue >= 0 ? POSITIVE_MARKER : NEGATIVE_MARKER;
        for (int i = 0; i < key.length - 2; i++) {
            int masked = (int) ((longValue >>> (4 * i)) & 0xF);
            key[key.length - i - 1] = HEX_BYTES[masked];
        }
        result = key;
    }
    return result;
} 
",,,,,79,3016
ArrayWrappers.java,29,150,0.19333333333333333,"
 * A factory for array wrappers so that arrays can be used as keys in a map, sorted or not.
 *
 * The comparator implementation makes two assumptions:
 * - All elements are instances of Comparable
 * - When comparing two arrays, they both contain elements of the same type in corresponding
 *   indices.
 *
 * Otherwise, ClassCastExceptions may occur. The equality method can compare any two arrays.
 *
 * This class is not efficient and is mostly meant to compare really small arrays, like those
 * generally used as indices and keys in a KVStore.
 ",,,,,,,11,528
KVIndex.java,61,15,4.066666666666666,"
 * Tags a field to be indexed when storing an object.
 *
 * <p>
 * Types are required to have a natural index that uniquely identifies instances in the store.
 * The default value of the annotation identifies the natural index for the type.
 * </p>
 *
 * <p>
 * Indexes allow for more efficient sorting of data read from the store. By annotating a field or
 * ""getter"" method with this annotation, an index will be created that will provide sorting based on
 * the string value of that field.
 * </p>
 *
 * <p>
 * Note that creating indices means more space will be needed, and maintenance operations like
 * updating or deleting a value will become more expensive.
 * </p>
 *
 * <p>
 * Indices are restricted to String, integral types (byte, short, int, long, boolean), and arrays
 * of those values.
 * </p>
 ",,,,,,,22,767
InMemoryStore.java,41,302,0.1357615894039735,"
 * Implementation of KVStore that keeps data deserialized in memory. This store does not index
 * data; instead, whenever iterating over an indexed field, the stored data is copied and sorted
 * according to the index. This saves memory but makes iteration more expensive.
 |
   * Encapsulates ConcurrentHashMap so that the typing in and out of the map strictly maps a
   * class of type T to an InstanceList of type T.
   |
     * A BiConsumer to control multi-entity removal.  We use this in a forEach rather than an
     * iterator because there is a bug in jdk8 which affects remove() on all concurrent map
     * iterators.  https://bugs.openjdk.java.net/browse/JDK-8078645
     ","/**
 * Create a copy of the input elements, filtering the values for child indices if needed.
 */
 
","{
    if (parent != null) {
        KVTypeInfo.Accessor parentGetter = ti.getParentAccessor(index);
        Preconditions.checkArgument(parentGetter != null, ""Parent filter for non-child index."");
        Comparable<?> parentKey = asKey(parent);
        return elements.stream().filter(e -> compare(e, parentGetter, parentKey) == 0).collect(Collectors.toList());
    } else {
        return new ArrayList<>(elements);
    }
} 
",,,"/**
 * Keeps a count of the number of elements removed.  This count is not currently surfaced
 * to clients of KVStore as Java's generic removeAll() construct returns only a boolean,
 * but I found it handy to have the count of elements removed while debugging; a count being
 * no more complicated than a boolean, I've retained that behavior here, even though there
 * is no current requirement.
 */
 
","Field count
",10,666
LevelDBIterator.java,32,214,0.14953271028037382,,"/**
 * Because it's tricky to expose closeable iterators through many internal APIs, especially
 * when Scala wrappers are used, this makes sure that, hopefully, the JNI resources held by
 * the iterator will eventually be released.
 */
 
","{
    db.closeIterator(this);
} 
",,,,,1,0
UnsupportedStoreVersionException.java,19,6,3.1666666666666665,"
 * Exception thrown when the store implementation is not compatible with the underlying data.
 ",,,,,,,1,93
LevelDB.java,38,257,0.14785992217898833,"
 * Implementation of KVStore that uses LevelDB as the underlying data store.
 | Needs to be public for Jackson. ","/**
 * Closes the given iterator if the DB is still open. Trying to close a JNI LevelDB handle
 * with a closed DB can cause JVM crashes, so this ensures that situation does not happen.
 */
 
/**
 * Returns metadata about indices for the given type.
 */
 
/**
 * Try to avoid use-after close since that has the tendency of crashing the JVM. This doesn't
 * prevent methods that retrieved the instance from using it after close, but hopefully will
 * catch most cases; otherwise, we'll need some kind of locking.
 */
 
","{
    synchronized (this._db) {
        DB _db = this._db.get();
        if (_db != null) {
            it.close();
        }
    }
} 
{
    LevelDBTypeInfo ti = types.get(type);
    if (ti == null) {
        LevelDBTypeInfo tmp = new LevelDBTypeInfo(this, type, getTypeAlias(type));
        ti = types.putIfAbsent(type, tmp);
        if (ti == null) {
            ti = tmp;
        }
    }
    return ti;
} 
{
    DB _db = this._db.get();
    if (_db == null) {
        throw new IllegalStateException(""DB is closed."");
    }
    return _db;
} 
",,,"/**
 * DB key where app metadata is stored.
 */
 
/**
 * DB key where type aliases are stored.
 */
 
/**
 * Keep a mapping of class names to a shorter, unique ID managed by the store. This serves two
 * purposes: make the keys stored on disk shorter, and spread out the keys, since class names
 * will often have a long, redundant prefix (think ""org.apache.spark."").
 */
 
","Field METADATA_KEY
Field TYPE_ALIASES_KEY
Field typeAliases
",2,110
KVStoreIterator.java,34,10,3.4,"
 * An iterator for KVStore.
 *
 * <p>
 * Iterators may keep references to resources that need to be closed. It's recommended that users
 * explicitly close iterators after they're used.
 * </p>
 ","/**
 * Retrieve multiple elements from the store.
 *
 * @param max Maximum number of elements to retrieve.
 */
 
/**
 * Skip in the iterator.
 *
 * @return Whether there are items left after skipping.
 */
 
","next 
skip 
",,,,,6,183
KVStore.java,105,17,6.176470588235294,"
 * Abstraction for a local key/value store for storing app data.
 *
 * <p>
 * There are two main features provided by the implementations of this interface:
 * </p>
 *
 * <h3>Serialization</h3>
 *
 * <p>
 * If the underlying data store requires serialization, data will be serialized to and deserialized
 * using a {@link KVStoreSerializer}, which can be customized by the application. The serializer is
 * based on Jackson, so it supports all the Jackson annotations for controlling the serialization of
 * app-defined types.
 * </p>
 *
 * <p>
 * Data is also automatically compressed to save disk space.
 * </p>
 *
 * <h3>Automatic Key Management</h3>
 *
 * <p>
 * When using the built-in key management, the implementation will automatically create unique
 * keys for each type written to the store. Keys are based on the type name, and always start
 * with the ""+"" prefix character (so that it's easy to use both manual and automatic key
 * management APIs without conflicts).
 * </p>
 *
 * <p>
 * Another feature of automatic key management is indexing; by annotating fields or methods of
 * objects written to the store with {@link KVIndex}, indices are created to sort the data
 * by the values of those properties. This makes it possible to provide sorting without having
 * to load all instances of those types from the store.
 * </p>
 *
 * <p>
 * KVStore instances are thread-safe for both reads and writes.
 * </p>
 ","/**
 * Returns app-specific metadata from the store, or null if it's not currently set.
 *
 * <p>
 * The metadata type is application-specific. This is a convenience method so that applications
 * don't need to define their own keys for this information.
 * </p>
 */
 
/**
 * Writes the given value in the store metadata key.
 */
 
/**
 * Read a specific instance of an object.
 *
 * @param naturalKey The object's ""natural key"", which uniquely identifies it. Null keys
 *                   are not allowed.
 * @throws java.util.NoSuchElementException If an element with the given key does not exist.
 */
 
/**
 * Writes the given object to the store, including indexed fields. Indices are updated based
 * on the annotated fields of the object's class.
 *
 * <p>
 * Writes may be slower when the object already exists in the store, since it will involve
 * updating existing indices.
 * </p>
 *
 * @param value The object to write.
 */
 
/**
 * Removes an object and all data related to it, like index entries, from the store.
 *
 * @param type The object's type.
 * @param naturalKey The object's ""natural key"", which uniquely identifies it. Null keys
 *                   are not allowed.
 * @throws java.util.NoSuchElementException If an element with the given key does not exist.
 */
 
/**
 * Returns a configurable view for iterating over entities of the given type.
 */
 
/**
 * Returns the number of items of the given type currently in the store.
 */
 
/**
 * Returns the number of items of the given type which match the given indexed value.
 */
 
/**
 * A cheaper way to remove multiple items from the KVStore
 */
 
","getMetadata 
setMetadata 
read 
write 
delete 
view 
count 
count 
removeAllByIndexValues 
",,,,,38,1351
KVStoreView.java,61,45,1.3555555555555556,"
 * A configurable view that allows iterating over values in a {@link KVStore}.
 *
 * <p>
 * The different methods can be used to configure the behavior of the iterator. Calling the same
 * method multiple times is allowed; the most recent value will be used.
 * </p>
 *
 * <p>
 * The iterators returned by this view are of type {@link KVStoreIterator}; they auto-close
 * when used in a for loop that exhausts their contents, but when used manually, they need
 * to be closed explicitly unless all elements are read.
 * </p>
 ","/**
 * Reverses the order of iteration. By default, iterates in ascending order.
 */
 
/**
 * Iterates according to the given index.
 */
 
/**
 * Defines the value of the parent index when iterating over a child index. Only elements that
 * match the parent index's value will be included in the iteration.
 *
 * <p>
 * Required for iterating over child indices, will generate an error if iterating over a
 * parent-less index.
 * </p>
 */
 
/**
 * Iterates starting at the given value of the chosen index (inclusive).
 */
 
/**
 * Stops iteration at the given value of the chosen index (inclusive).
 */
 
/**
 * Stops iteration after a number of elements has been retrieved.
 */
 
/**
 * Skips a number of elements at the start of iteration. Skipped elements are not accounted
 * when using {@link #max(long)}.
 */
 
/**
 * Returns an iterator for the current configuration.
 */
 
","{
    ascending = !ascending;
    return this;
} 
{
    this.index = Preconditions.checkNotNull(name);
    return this;
} 
{
    this.parent = value;
    return this;
} 
{
    this.first = value;
    return this;
} 
{
    this.last = value;
    return this;
} 
{
    Preconditions.checkArgument(max > 0L, ""max must be positive."");
    this.max = max;
    return this;
} 
{
    this.skip = n;
    return this;
} 
{
    return (KVStoreIterator<T>) iterator();
} 
",,,,,12,502
KVTypeInfo.java,22,115,0.19130434782608696,"
 * Wrapper around types managed in a KVStore, providing easy access to their indexed fields.
 |
   * Abstracts the difference between invoking a Field and a Method.
   ",,,,,,,3,163
KVStoreSerializer.java,29,42,0.6904761904761905,"
 * Serializer used to translate between app-defined types and the LevelDB store.
 *
 * <p>
 * The serializer is based on Jackson, so values are written as JSON. It also allows ""naked strings""
 * and integers to be written as values directly, which will be written as UTF-8 strings.
 * </p>
 ",,,,,"/**
 * Object mapper used to process app-specific types. If an application requires a specific
 * configuration of the mapper, it can subclass this serializer and add custom configuration
 * to this object.
 */
 
","Field mapper
",6,279
BitArray.java,20,81,0.24691358024691357,,"/**
 * Returns true if the bit changed value.
 */
 
/**
 * Number of bits
 */
 
/**
 * Number of set bits (1s)
 */
 
/**
 * Combines the two BitArrays using bitwise OR.
 */
 
","{
    if (!get(index)) {
        data[(int) (index >>> 6)] |= (1L << index);
        bitCount++;
        return true;
    }
    return false;
} 
{
    return (long) data.length * Long.SIZE;
} 
{
    return bitCount;
} 
{
    assert data.length == array.data.length : ""BitArrays must be of equal length when merging"";
    long bitCount = 0;
    for (int i = 0; i < data.length; i++) {
        data[i] |= array.data[i];
        bitCount += Long.bitCount(data[i]);
    }
    this.bitCount = bitCount;
} 
",,,,,1,0
Murmur3_x86_32.java,25,95,0.2631578947368421,"
 * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.
 | This class is duplicated from `org.apache.spark.unsafe.hash.Murmur3_x86_32` to make sure| spark-sketch has no external dependencies.",,,,,,,2,209
IncompatibleMergeException.java,16,6,2.6666666666666665,,,,,,,,1,0
Utils.java,72,232,0.3103448275862069,,,,,,,,1,0
Platform.java,55,231,0.23809523809523808," This class is duplicated from `org.apache.spark.unsafe.Platform` to make sure spark-sketch has no| external dependencies.","/**
 * Raises an exception bypassing compiler checks for checked exceptions.
 */
 
","{
    _UNSAFE.throwException(t);
} 
",,,"/**
 * Limits the number of bytes to copy per {@link Unsafe#copyMemory(long, long, long)} to
 * allow safepoint polling during a large copy.
 */
 
","Field UNSAFE_COPY_THRESHOLD
",1,122
BloomFilterImpl.java,26,187,0.13903743315508021,,,,,,,,1,0
BloomFilter.java,147,59,2.4915254237288136,"
 * A Bloom filter is a space-efficient probabilistic data structure that offers an approximate
 * containment test with one-sided error: if it claims that an item is contained in it, this
 * might be in error, but if it claims that an item is <i>not</i> contained in it, then this is
 * definitely true. Currently supported data types include:
 * <ul>
 *   <li>{@link Byte}</li>
 *   <li>{@link Short}</li>
 *   <li>{@link Integer}</li>
 *   <li>{@link Long}</li>
 *   <li>{@link String}</li>
 * </ul>
 * The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability that
 * {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that has
 * not actually been put in the {@code BloomFilter}.
 *
 * The implementation is largely based on the {@code BloomFilter} class from Guava.
 ","/**
 * Returns the probability that {@linkplain #mightContain(Object)} erroneously return {@code true}
 * for an object that has not actually been put in the {@code BloomFilter}.
 *
 * Ideally, this number should be close to the {@code fpp} parameter passed in
 * {@linkplain #create(long, double)}, or smaller. If it is significantly higher, it is usually
 * the case that too many items (more than expected) have been put in the {@code BloomFilter},
 * degenerating it.
 */
 
/**
 * Returns the number of bits in the underlying bit array.
 */
 
/**
 * Puts an item into this {@code BloomFilter}. Ensures that subsequent invocations of
 * {@linkplain #mightContain(Object)} with the same item will always return {@code true}.
 *
 * @return true if the bloom filter's bits changed as a result of this operation. If the bits
 *     changed, this is <i>definitely</i> the first time {@code object} has been added to the
 *     filter. If the bits haven't changed, this <i>might</i> be the first time {@code object}
 *     has been added to the filter. Note that {@code put(t)} always returns the
 *     <i>opposite</i> result to what {@code mightContain(t)} would have returned at the time
 *     it is called.
 */
 
/**
 * A specialized variant of {@link #put(Object)} that only supports {@code String} items.
 */
 
/**
 * A specialized variant of {@link #put(Object)} that only supports {@code long} items.
 */
 
/**
 * A specialized variant of {@link #put(Object)} that only supports byte array items.
 */
 
/**
 * Determines whether a given bloom filter is compatible with this bloom filter. For two
 * bloom filters to be compatible, they must have the same bit size.
 *
 * @param other The bloom filter to check for compatibility.
 */
 
/**
 * Combines this bloom filter with another bloom filter by performing a bitwise OR of the
 * underlying data. The mutations happen to <b>this</b> instance. Callers must ensure the
 * bloom filters are appropriately sized to avoid saturating them.
 *
 * @param other The bloom filter to combine this bloom filter with. It is not mutated.
 * @throws IncompatibleMergeException if {@code isCompatible(other) == false}
 */
 
/**
 * Returns {@code true} if the element <i>might</i> have been put in this Bloom filter,
 * {@code false} if this is <i>definitely</i> not the case.
 */
 
/**
 * A specialized variant of {@link #mightContain(Object)} that only tests {@code String} items.
 */
 
/**
 * A specialized variant of {@link #mightContain(Object)} that only tests {@code long} items.
 */
 
/**
 * A specialized variant of {@link #mightContain(Object)} that only tests byte array items.
 */
 
/**
 * Writes out this {@link BloomFilter} to an output stream in binary format. It is the caller's
 * responsibility to close the stream.
 */
 
/**
 * Reads in a {@link BloomFilter} from an input stream. It is the caller's responsibility to close
 * the stream.
 */
 
/**
 * Computes the optimal k (number of hashes per item inserted in Bloom filter), given the
 * expected insertions and total number of bits in the Bloom filter.
 *
 * See http://en.wikipedia.org/wiki/File:Bloom_filter_fp_probability.svg for the formula.
 *
 * @param n expected insertions (must be positive)
 * @param m total number of bits in Bloom filter (must be positive)
 */
 
/**
 * Computes m (total bits of Bloom filter) which is expected to achieve, for the specified
 * expected insertions, the required false positive probability.
 *
 * See http://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives for the formula.
 *
 * @param n expected insertions (must be positive)
 * @param p false positive rate (must be 0 < p < 1)
 */
 
/**
 * Creates a {@link BloomFilter} with the expected number of insertions and a default expected
 * false positive probability of 3%.
 *
 * Note that overflowing a {@code BloomFilter} with significantly more elements than specified,
 * will result in its saturation, and a sharp deterioration of its false positive probability.
 */
 
/**
 * Creates a {@link BloomFilter} with the expected number of insertions and expected false
 * positive probability.
 *
 * Note that overflowing a {@code BloomFilter} with significantly more elements than specified,
 * will result in its saturation, and a sharp deterioration of its false positive probability.
 */
 
/**
 * Creates a {@link BloomFilter} with given {@code expectedNumItems} and {@code numBits}, it will
 * pick an optimal {@code numHashFunctions} which can minimize {@code fpp} for the bloom filter.
 */
 
","expectedFpp 
bitSize 
put 
putString 
putLong 
putBinary 
isCompatible 
mergeInPlace 
mightContain 
mightContainString 
mightContainLong 
mightContainBinary 
writeTo 
{
    return BloomFilterImpl.readFrom(in);
} 
{
    // (m / n) * log(2), but avoid truncation due to division!
    return Math.max(1, (int) Math.round((double) m / n * Math.log(2)));
} 
{
    return (long) (-n * Math.log(p) / (Math.log(2) * Math.log(2)));
} 
{
    return create(expectedNumItems, DEFAULT_FPP);
} 
{
    if (fpp <= 0D || fpp >= 1D) {
        throw new IllegalArgumentException(""False positive probability must be within range (0.0, 1.0)"");
    }
    return create(expectedNumItems, optimalNumOfBits(expectedNumItems, fpp));
} 
{
    if (expectedNumItems <= 0) {
        throw new IllegalArgumentException(""Expected insertions must be positive"");
    }
    if (numBits <= 0) {
        throw new IllegalArgumentException(""Number of bits must be positive"");
    }
    return new BloomFilterImpl(optimalNumOfHashFunctions(expectedNumItems, numBits), numBits);
} 
",,,,,16,810
CountMinSketch.java,145,49,2.9591836734693877,"
 * A Count-min sketch is a probabilistic data structure used for cardinality estimation using
 * sub-linear space.  Currently, supported data types include:
 * <ul>
 *   <li>{@link Byte}</li>
 *   <li>{@link Short}</li>
 *   <li>{@link Integer}</li>
 *   <li>{@link Long}</li>
 *   <li>{@link String}</li>
 * </ul>
 * A {@link CountMinSketch} is initialized with a random seed, and a pair of parameters:
 * <ol>
 *   <li>relative error (or {@code eps}), and
 *   <li>confidence (or {@code delta})
 * </ol>
 * Suppose you want to estimate the number of times an element {@code x} has appeared in a data
 * stream so far.  With probability {@code delta}, the estimate of this frequency is within the
 * range {@code true frequency <= estimate <= true frequency + eps * N}, where {@code N} is the
 * total count of items have appeared the data stream so far.
 *
 * Under the cover, a {@link CountMinSketch} is essentially a two-dimensional {@code long} array
 * with depth {@code d} and width {@code w}, where
 * <ul>
 *   <li>{@code d = ceil(2 / eps)}</li>
 *   <li>{@code w = ceil(-log(1 - confidence) / log(2))}</li>
 * </ul>
 *
 * This implementation is largely based on the {@code CountMinSketch} class from stream-lib.
 ","/**
 * Returns the relative error (or {@code eps}) of this {@link CountMinSketch}.
 */
 
/**
 * Returns the confidence (or {@code delta}) of this {@link CountMinSketch}.
 */
 
/**
 * Depth of this {@link CountMinSketch}.
 */
 
/**
 * Width of this {@link CountMinSketch}.
 */
 
/**
 * Total count of items added to this {@link CountMinSketch} so far.
 */
 
/**
 * Increments {@code item}'s count by one.
 */
 
/**
 * Increments {@code item}'s count by {@code count}.
 */
 
/**
 * Increments {@code item}'s count by one.
 */
 
/**
 * Increments {@code item}'s count by {@code count}.
 */
 
/**
 * Increments {@code item}'s count by one.
 */
 
/**
 * Increments {@code item}'s count by {@code count}.
 */
 
/**
 * Increments {@code item}'s count by one.
 */
 
/**
 * Increments {@code item}'s count by {@code count}.
 */
 
/**
 * Returns the estimated frequency of {@code item}.
 */
 
/**
 * Merges another {@link CountMinSketch} with this one in place.
 *
 * Note that only Count-Min sketches with the same {@code depth}, {@code width}, and random seed
 * can be merged.
 *
 * @exception IncompatibleMergeException if the {@code other} {@link CountMinSketch} has
 *            incompatible depth, width, relative-error, confidence, or random seed.
 */
 
/**
 * Writes out this {@link CountMinSketch} to an output stream in binary format. It is the caller's
 * responsibility to close the stream.
 */
 
/**
 * Serializes this {@link CountMinSketch} and returns the serialized form.
 */
 
/**
 * Reads in a {@link CountMinSketch} from an input stream. It is the caller's responsibility to
 * close the stream.
 */
 
/**
 * Reads in a {@link CountMinSketch} from a byte array.
 */
 
/**
 * Creates a {@link CountMinSketch} with given {@code depth}, {@code width}, and random
 * {@code seed}.
 *
 * @param depth depth of the Count-min Sketch, must be positive
 * @param width width of the Count-min Sketch, must be positive
 * @param seed random seed
 */
 
/**
 * Creates a {@link CountMinSketch} with given relative error ({@code eps}), {@code confidence},
 * and random {@code seed}.
 *
 * @param eps relative error, must be positive
 * @param confidence confidence, must be positive and less than 1.0
 * @param seed random seed
 */
 
","relativeError 
confidence 
depth 
width 
totalCount 
add 
add 
addLong 
addLong 
addString 
addString 
addBinary 
addBinary 
estimateCount 
mergeInPlace 
writeTo 
toByteArray 
{
    return CountMinSketchImpl.readFrom(in);
} 
{
    try (InputStream in = new ByteArrayInputStream(bytes)) {
        return readFrom(in);
    }
} 
{
    return new CountMinSketchImpl(depth, width, seed);
} 
{
    return new CountMinSketchImpl(eps, confidence, seed);
} 
",,,,,27,1168
CountMinSketchImpl.java,28,277,0.10108303249097472,,,,,,,,1,0
JavaNaiveBayesExample.java,29,31,0.9354838709677419,,,,,,,,1,0
JavaPrefixSpanExample.java,28,31,0.9032258064516129,,,,,,,,1,0
JavaHypothesisTestingKolmogorovSmirnovTestExample.java,22,19,1.1578947368421053,,,,,,,,1,0
JavaSVMWithSGDExample.java,29,33,0.8787878787878788," $example off$|
 * Example for SVMWithSGD.
 ",,,,,,,2,41
JavaKMeansExample.java,24,40,0.6,,,,,,,,1,0
JavaChiSqSelectorExample.java,20,43,0.46511627906976744,,,,,,,,1,0
JavaGradientBoostingRegressionExample.java,27,43,0.627906976744186,,,,,,,,1,0
JavaLogisticRegressionWithLBFGSExample.java,28,34,0.8235294117647058," $example off$|
 * Example for LogisticRegressionWithLBFGS.
 ",,,,,,,2,58
JavaDecisionTreeRegressionExample.java,31,47,0.6595744680851063,,,,,,,,1,0
JavaBisectingKMeansExample.java,25,33,0.7575757575757576," $example off$|
 * Java example for bisecting k-means clustering.
 ",,,,,,,2,64
JavaRandomForestRegressionExample.java,27,44,0.6136363636363636,,,,,,,,1,0
JavaLBFGSExample.java,26,58,0.4482758620689655,,,,,,,,1,0
JavaHypothesisTestingExample.java,32,39,0.8205128205128205,,,,,,,,1,0
JavaMultiLabelClassificationMetricsExample.java,26,42,0.6190476190476191,,,,,,,,1,0
JavaMulticlassClassificationMetricsExample.java,29,47,0.6170212765957447,,,,,,,,1,0
JavaDecisionTreeClassificationExample.java,33,57,0.5789473684210527,,,,,,,,1,0
JavaGaussianMixtureExample.java,24,34,0.7058823529411765,,,,,,,,1,0
JavaSVDExample.java,27,38,0.7105263157894737," $example off$| $example off$|
 * Example for SingularValueDecomposition.
 ",,,,,,,2,72
JavaRankingMetricsExample.java,34,87,0.39080459770114945,,,,,,,,1,0
JavaKernelDensityEstimationExample.java,24,18,1.3333333333333333,,,,,,,,1,0
JavaPowerIterationClusteringExample.java,21,41,0.5121951219512195," $example off$|
 * Java example for graph clustering using power iteration clustering (PIC).
 ",,,,,,,2,91
JavaElementwiseProductExample.java,22,39,0.5641025641025641,,,,,,,,1,0
JavaBinaryClassificationMetricsExample.java,34,48,0.7083333333333334,,,,,,,,1,0
JavaSummaryStatisticsExample.java,21,27,0.7777777777777778,,,,,,,,1,0
JavaCorrelationsExample.java,27,32,0.84375,,,,,,,,1,0
JavaAssociationRulesExample.java,20,27,0.7407407407407407,,,,,,,,1,0
JavaSimpleFPGrowth.java,20,31,0.6451612903225806,,,,,,,,1,0
JavaStratifiedSamplingExample.java,23,35,0.6571428571428571,,,,,,,,1,0
JavaPCAExample.java,20,39,0.5128205128205128," $example off$| $example off$|
 * Example for compute principal components on a 'RowMatrix'.
 ",,,,,,,2,91
JavaALS.java,19,55,0.34545454545454546,"
 * Example using MLlib ALS from Java.
 ",,,,,,,1,37
JavaLatentDirichletAllocationExample.java,24,47,0.5106382978723404,,,,,,,,1,0
JavaIsotonicRegressionExample.java,27,37,0.7297297297297297,,,,,,,,1,0
JavaRandomForestClassificationExample.java,26,44,0.5909090909090909,,,,,,,,1,0
JavaStreamingTestExample.java,41,48,0.8541666666666666,"
 * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data
 * stream arrives as text files in a directory. Stops when the two groups are statistically
 * significant (p-value < 0.05) or after a user-specified timeout in number of batches is exceeded.
 *
 * The rows of the text files must be in the form `Boolean, Double`. For example:
 *   false, -3.92
 *   true, 99.32
 *
 * Usage:
 *   JavaStreamingTestExample <dataDir> <batchDuration> <numBatchesTimeout>
 *
 * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and
 * a timeout after 100 insignificant batches, call:
 *    $ bin/run-example mllib.JavaStreamingTestExample dataDir 5 100
 *
 * As you add text files to `dataDir` the significance test wil continually update every
 * `batchDuration` seconds until the test becomes significant (p-value < 0.05) or the number of
 * batches processed exceeds `numBatchesTimeout`.
 ",,,,,,,18,923
JavaRecommendationExample.java,24,42,0.5714285714285714,,,,,,,,1,0
JavaGradientBoostingClassificationExample.java,27,42,0.6428571428571429,,,,,,,,1,0
JavaStatusTrackerDemo.java,20,44,0.45454545454545453,"
 * Example of using Spark's status APIs from Java.
 ",,,,,,,1,50
JavaTC.java,27,55,0.4909090909090909,"
 * Transitive closure on a graph, implemented in Java.
 * Usage: JavaTC [partitions]
 ",,,,,,,2,82
JavaLogQuery.java,22,80,0.275,"
 * Executes a roll up-style query against Apache logs.
 *
 * Usage: JavaLogQuery [logFile]
 | Tracks the total query count and number of aggregate bytes for a particular group. ",,,,,,,4,171
JavaStatefulNetworkWordCount.java,33,47,0.7021276595744681,"
 * Counts words cumulatively in UTF8 encoded, '\n' delimited text received from the network every
 * second starting with initial value of word count.
 * Usage: JavaStatefulNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 * data.
 * <p>
 * To run this on your local machine, you need to first run a Netcat server
 * `$ nc -lk 9999`
 * and then run the example
 * `$ bin/run-example
 * org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount localhost 9999`
 ",,,,,,,11,527
JavaQueueStream.java,21,37,0.5675675675675675,,,,,,,,1,0
JavaNetworkWordCount.java,32,31,1.032258064516129,"
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: JavaNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`
 ",,,,,,,9,456
JavaSqlNetworkWordCount.java,38,56,0.6785714285714286,"
 * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 * network every second.
 *
 * Usage: JavaSqlNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.JavaSqlNetworkWordCount localhost 9999`
 | Lazily instantiated singleton instance of SparkSession ",,,,,,,11,545
JavaRecord.java,17,10,1.7," Java Bean class to be used with the example JavaSqlNetworkWordCount. ",,,,,,,1,70
JavaRecoverableNetworkWordCount.java,60,110,0.5454545454545454,"
 * Use this singleton to get or register a Broadcast variable.
 |
 * Use this singleton to get or register an Accumulator.
 |
 * Counts words in text encoded with UTF8 received from the network every second. This example also
 * shows how to use lazily instantiated singleton instances for Accumulator and Broadcast so that
 * they can be registered on driver failures.
 *
 * Usage: JavaRecoverableNetworkWordCount <hostname> <port> <checkpoint-directory> <output-file>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data. <checkpoint-directory> directory to HDFS-compatible file system which checkpoint data
 *   <output-file> file to which the word counts will be appended
 *
 * <checkpoint-directory> and <output-file> must be absolute paths
 *
 * To run this on your local machine, you need to first run a Netcat server
 *
 *      `$ nc -lk 9999`
 *
 * and run the example as
 *
 *      `$ ./bin/run-example org.apache.spark.examples.streaming.JavaRecoverableNetworkWordCount \
 *              localhost 9999 ~/checkpoint/ ~/out`
 *
 * If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create
 * a new StreamingContext (will print ""Creating new context"" to the console). Otherwise, if
 * checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from
 * the checkpoint data.
 *
 * Refer to the online documentation for more details.
 ",,,,,,,30,1394
JavaDirectKerberizedKafkaWordCount.java,68,57,1.1929824561403508,,,,,,,,1,0
JavaDirectKafkaWordCount.java,31,52,0.5961538461538461,,,,,,,,1,0
JavaCustomReceiver.java,41,76,0.5394736842105263,,"/**
 * Create a socket connection and receive data until receiver is stopped
 */
 
","{
    try {
        Socket socket = null;
        BufferedReader reader = null;
        try {
            // connect to the server
            socket = new Socket(host, port);
            reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8));
            // Until stopped or connection broken continue reading
            String userInput;
            while (!isStopped() && (userInput = reader.readLine()) != null) {
                System.out.println(""Received data '"" + userInput + ""'"");
                store(userInput);
            }
        } finally {
            Closeables.close(reader, /* swallowIOException = */
            true);
            Closeables.close(socket, /* swallowIOException = */
            true);
        }
        // Restart in an attempt to connect again when server is active again
        restart(""Trying to connect again"");
    } catch (ConnectException ce) {
        // restart if could not connect to server
        restart(""Could not connect"", ce);
    } catch (Throwable t) {
        restart(""Error receiving data"", t);
    }
} 
",,,,,1,0
JavaEstimatorTransformerParamExample.java,41,62,0.6612903225806451,"
 * Java example for Estimator, Transformer, and Param.
 ",,,,,,,1,54
JavaNaiveBayesExample.java,29,31,0.9354838709677419," $example off$|
 * An example for Naive Bayes Classification.
 ",,,,,,,2,60
JavaPrefixSpanExample.java,28,31,0.9032258064516129," $example off$|
 * An example demonstrating PrefixSpan.
 * Run with
 * <pre>
 * bin/run-example ml.JavaPrefixSpanExample
 * </pre>
 ",,,,,,,6,121
JavaBucketizerExample.java,32,67,0.47761194029850745," $example off$|
 * An example for Bucketizer.
 * Run with
 * <pre>
 * bin/run-example ml.JavaBucketizerExample
 * </pre>
 ",,,,,,,6,111
JavaChiSquareTestExample.java,27,37,0.7297297297297297," $example off$|
 * An example for Chi-square hypothesis testing.
 * Run with
 * <pre>
 * bin/run-example ml.JavaChiSquareTestExample
 * </pre>
 ",,,,,,,6,133
JavaSummarizerExample.java,20,41,0.4878048780487805,,,,,,,,1,0
JavaVectorAssemblerExample.java,20,36,0.5555555555555556,,,,,,,,1,0
JavaImputerExample.java,25,37,0.6756756756756757," $example off$|
 * An example demonstrating Imputer.
 * Run with:
 *   bin/run-example ml.JavaImputerExample
 ",,,,,,,4,103
JavaIndexToStringExample.java,20,55,0.36363636363636365,,,,,,,,1,0
JavaKMeansExample.java,24,40,0.6,"
 * An example demonstrating k-means clustering.
 * Run with
 * <pre>
 * bin/run-example ml.JavaKMeansExample
 * </pre>
 ",,,,,,,5,110
JavaRobustScalerExample.java,22,27,0.8148148148148148,,,,,,,,1,0
JavaRandomForestClassifierExample.java,33,53,0.6226415094339622,,,,,,,,1,0
JavaLabeledDocument.java,19,13,1.4615384615384615,"
 * Labeled instance type, Spark SQL can infer schema from Java Beans.
 ",,,,,,,1,69
JavaGradientBoostedTreeRegressorExample.java,30,46,0.6521739130434783,,,,,,,,1,0
JavaWord2VecExample.java,22,41,0.5365853658536586,,,,,,,,1,0
JavaChiSqSelectorExample.java,20,43,0.46511627906976744,,,,,,,,1,0
JavaDecisionTreeRegressionExample.java,31,47,0.6595744680851063,,,,,,,,1,0
JavaRFormulaExample.java,20,38,0.5263157894736842,,,,,,,,1,0
JavaPolynomialExpansionExample.java,20,37,0.5405405405405406,,,,,,,,1,0
JavaBisectingKMeansExample.java,25,33,0.7575757575757576,"
 * An example demonstrating bisecting k-means clustering.
 * Run with
 * <pre>
 * bin/run-example ml.JavaBisectingKMeansExample
 * </pre>
 ",,,,,,,5,129
JavaNormalizerExample.java,22,42,0.5238095238095238,,,,,,,,1,0
JavaOneVsRestExample.java,36,34,1.0588235294117647,"
 * An example of Multiclass to Binary Reduction with One Vs Rest,
 * using Logistic Regression as the base classifier.
 * Run with
 * <pre>
 * bin/run-example ml.JavaOneVsRestExample
 * </pre>
 ",,,,,,,6,182
JavaQuantileDiscretizerExample.java,25,40,0.625,,,,,,,,1,0
JavaGeneralizedLinearRegressionExample.java,31,40,0.775,,,,,,,,1,0
JavaInteractionExample.java,20,50,0.4,,,,,,,,1,0
JavaStandardScalerExample.java,22,25,0.88,,,,,,,,1,0
JavaDecisionTreeClassificationExample.java,33,57,0.5789473684210527,,,,,,,,1,0
JavaDCTExample.java,20,37,0.5405405405405406,,,,,,,,1,0
JavaBucketedRandomProjectionLSHExample.java,34,60,0.5666666666666667," $example off$|
 * An example demonstrating BucketedRandomProjectionLSH.
 * Run with:
 *   bin/run-example ml.JavaBucketedRandomProjectionLSHExample
 ",,,,,,,4,143
JavaCountVectorizerExample.java,23,37,0.6216216216216216,,,,,,,,1,0
JavaStringIndexerExample.java,20,38,0.5263157894736842,,,,,,,,1,0
JavaLogisticRegressionWithElasticNetExample.java,26,32,0.8125,,,,,,,,1,0
JavaGradientBoostedTreeClassifierExample.java,33,57,0.5789473684210527,,,,,,,,1,0
JavaGaussianMixtureExample.java,24,34,0.7058823529411765,"
 * An example demonstrating Gaussian Mixture Model.
 * Run with
 * <pre>
 * bin/run-example ml.JavaGaussianMixtureExample
 * </pre>
 ",,,,,,,5,123
JavaMinHashLSHExample.java,36,59,0.6101694915254238," $example off$|
 * An example demonstrating MinHashLSH.
 * Run with:
 *   bin/run-example ml.JavaMinHashLSHExample
 ",,,,,,,4,109
JavaPowerIterationClusteringExample.java,21,41,0.5121951219512195,,,,,,,,1,0
JavaVectorSizeHintExample.java,21,46,0.45652173913043476,,,,,,,,1,0
JavaNGramExample.java,20,35,0.5714285714285714,,,,,,,,1,0
JavaMinMaxScalerExample.java,22,42,0.5238095238095238,,,,,,,,1,0
JavaElementwiseProductExample.java,22,39,0.5641025641025641,,,,,,,,1,0
JavaStopWordsRemoverExample.java,20,34,0.5882352941176471,,,,,,,,1,0
JavaLDAExample.java,32,28,1.1428571428571428," $example off$|
 * An example demonstrating LDA.
 * Run with
 * <pre>
 * bin/run-example ml.JavaLDAExample
 * </pre>
 ",,,,,,,6,107
JavaSQLTransformerExample.java,20,31,0.6451612903225806,,,,,,,,1,0
JavaOneHotEncoderExample.java,21,41,0.5121951219512195,,,,,,,,1,0
JavaCorrelationExample.java,27,34,0.7941176470588235," $example off$|
 * An example for computing correlation matrix.
 * Run with
 * <pre>
 * bin/run-example ml.JavaCorrelationExample
 * </pre>
 ",,,,,,,6,130
JavaRandomForestRegressorExample.java,30,46,0.6521739130434783,,,,,,,,1,0
JavaPipelineExample.java,28,50,0.56,"
 * Java example for simple text document 'Pipeline'.
 ",,,,,,,1,52
JavaLinearSVCExample.java,23,23,1.0,,,,,,,,1,0
JavaMulticlassLogisticRegressionWithElasticNetExample.java,25,77,0.3246753246753247,,,,,,,,1,0
JavaFPGrowthExample.java,31,36,0.8611111111111112," $example off$|
 * An example demonstrating FPGrowth.
 * Run with
 * <pre>
 * bin/run-example ml.JavaFPGrowthExample
 * </pre>
 ",,,,,,,6,117
JavaBinarizerExample.java,20,38,0.5263157894736842,,,,,,,,1,0
JavaModelSelectionViaTrainValidationSplitExample.java,37,40,0.925,"
 * Java example demonstrating model selection using TrainValidationSplit.
 *
 * Run with
 * {{{
 * bin/run-example ml.JavaModelSelectionViaTrainValidationSplitExample
 * }}}
 ",,,,,,,6,163
JavaDocument.java,19,17,1.1176470588235294,"
 * Unlabeled instance type, Spark SQL can infer schema from Java Beans.
 ",,,,,,,1,71
JavaMultilayerPerceptronClassifierExample.java,31,33,0.9393939393939394," $example off$|
 * An example for Multilayer Perceptron Classification.
 ",,,,,,,2,70
JavaPCAExample.java,20,39,0.5128205128205128,,,,,,,,1,0
JavaFeatureHasherExample.java,20,39,0.5128205128205128,,,,,,,,1,0
JavaVectorIndexerExample.java,21,30,0.7,,,,,,,,1,0
JavaTfIdfExample.java,21,46,0.45652173913043476,,,,,,,,1,0
JavaIsotonicRegressionExample.java,27,37,0.7297297297297297," $example on$|
 * An example demonstrating IsotonicRegression.
 * Run with
 * <pre>
 * bin/run-example ml.JavaIsotonicRegressionExample
 * </pre>
 ",,,,,,,6,136
JavaAFTSurvivalRegressionExample.java,28,46,0.6086956521739131," $example off$|
 * An example demonstrating AFTSurvivalRegression.
 * Run with
 * <pre>
 * bin/run-example ml.JavaAFTSurvivalRegressionExample
 * </pre>
 ",,,,,,,6,143
JavaLogisticRegressionSummaryExample.java,28,38,0.7368421052631579,,,,,,,,1,0
JavaTokenizerExample.java,21,50,0.42,,,,,,,,1,0
JavaMaxAbsScalerExample.java,22,40,0.55,,,,,,,,1,0
JavaLinearRegressionWithElasticNetExample.java,24,32,0.75,,,,,,,,1,0
JavaModelSelectionViaCrossValidationExample.java,38,72,0.5277777777777778," $example off$|
 * Java example for Model Selection via Cross Validation.
 ",,,,,,,2,72
JavaALSExample.java,29,86,0.3372093023255814," $example off$| $example on$",,,,,,,1,28
JavaVectorSlicerExample.java,21,39,0.5384615384615384,,,,,,,,1,0
JavaSparkPi.java,20,29,0.6896551724137931,"
 * Computes an approximation to pi
 * Usage: JavaSparkPi [partitions]
 ",,,,,,,2,67
JavaPageRank.java,44,60,0.7333333333333333,"
 * Computes the PageRank of URLs from an input file. Input file should
 * be in format of:
 * URL         neighbor URL
 * URL         neighbor URL
 * URL         neighbor URL
 * ...
 * where URL and their neighbors are separated by space(s).
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.graphx.lib.PageRank
 *
 * Example Usage:
 * <pre>
 * bin/run-example JavaPageRank data/mllib/pagerank_data.txt 10
 * </pre>
 ",,,,,,,15,471
JavaHdfsLR.java,23,108,0.21296296296296297,"
 * Logistic regression based classification.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.classification.LogisticRegression.
 ",,,,,,,4,210
JavaWordCount.java,16,30,0.5333333333333333,,,,,,,,1,0
JavaStructuredNetworkWordCount.java,33,34,0.9705882352941176,"
 * Counts words in UTF8 encoded, '\n' delimited text received from the network.
 *
 * Usage: JavaStructuredNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Structured Streaming
 * would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCount
 *    localhost 9999`
 ",,,,,,,11,451
JavaStructuredKerberizedKafkaWordCount.java,77,42,1.8333333333333333,"
 * Consumes messages from one or more topics in Kafka and does wordcount.
 * Usage: JavaStructuredKerberizedKafkaWordCount <bootstrap-servers> <subscribe-type> <topics>
 *   <bootstrap-servers> The Kafka ""bootstrap.servers"" configuration. A
 *   comma-separated list of host:port.
 *   <subscribe-type> There are three kinds of type, i.e. 'assign', 'subscribe',
 *   'subscribePattern'.
 *   |- <assign> Specific TopicPartitions to consume. Json string
 *   |  {""topicA"":[0,1],""topicB"":[2,4]}.
 *   |- <subscribe> The topic list to subscribe. A comma-separated list of
 *   |  topics.
 *   |- <subscribePattern> The pattern used to subscribe to topic(s).
 *   |  Java regex string.
 *   |- Only one of ""assign, ""subscribe"" or ""subscribePattern"" options can be
 *   |  specified for Kafka source.
 *   <topics> Different value format depends on the value of 'subscribe-type'.
 *
 * Example:
 *   Yarn client:
 *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \
 *      --driver-java-options ""-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf"" \
 *      --conf \
 *      ""spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf"" \
 *      --master yarn
 *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \
 *      subscribe topic1,topic2
 *   Yarn cluster:
 *    $ bin/run-example --files \
 *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \
 *      --driver-java-options \
 *      ""-Djava.security.auth.login.config=./kafka_jaas.conf \
 *      -Djava.security.krb5.conf=./krb5.conf"" \
 *      --conf \
 *      ""spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf"" \
 *      --master yarn --deploy-mode cluster \
 *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \
 *      subscribe topic1,topic2
 *
 * kafka_jaas.conf can manually create, template as:
 *   KafkaClient {
 *     com.sun.security.auth.module.Krb5LoginModule required
 *     keyTab=""./kafka.service.keytab""
 *     useKeyTab=true
 *     storeKey=true
 *     useTicketCache=false
 *     serviceName=""kafka""
 *     principal=""kafka/host@EXAMPLE.COM"";
 *   };
 * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same
 * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be
 * ""${keytab_path}/kafka.service.keytab"".
 * In addition, for IBM JVMs, please use 'com.ibm.security.auth.module.Krb5LoginModule'
 * instead of 'com.sun.security.auth.module.Krb5LoginModule'.
 *
 * Note that this example uses SASL_PLAINTEXT for simplicity; however,
 * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider
 * using SASL_SSL in production.
 ",,,,,,,56,2733
JavaStructuredNetworkWordCountWindowed.java,43,58,0.7413793103448276,"
 * Counts words in UTF8 encoded, '\n' delimited text received from the network over a
 * sliding window of configurable duration. Each line from the network is tagged
 * with a timestamp that is used to determine the windows into which it falls.
 *
 * Usage: JavaStructuredNetworkWordCountWindowed <hostname> <port> <window duration>
 *   [<slide duration>]
 * <hostname> and <port> describe the TCP server that Structured Streaming
 * would connect to receive data.
 * <window duration> gives the size of window, specified as integer number of seconds
 * <slide duration> gives the amount of time successive windows are offset from one another,
 * given in the same units as above. <slide duration> should be less than or equal to
 * <window duration>. If the two are equal, successive windows have no overlap. If
 * <slide duration> is not provided, it defaults to <window duration>.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCountWindowed
 *    localhost 9999 <window duration in seconds> [<slide duration in seconds>]`
 *
 * One recommended <window duration>, <slide duration> pair is 10, 5
 ",,,,,,,21,1205
JavaStructuredKafkaWordCount.java,41,40,1.025,"
 * Consumes messages from one or more topics in Kafka and does wordcount.
 * Usage: JavaStructuredKafkaWordCount <bootstrap-servers> <subscribe-type> <topics>
 *   <bootstrap-servers> The Kafka ""bootstrap.servers"" configuration. A
 *   comma-separated list of host:port.
 *   <subscribe-type> There are three kinds of type, i.e. 'assign', 'subscribe',
 *   'subscribePattern'.
 *   |- <assign> Specific TopicPartitions to consume. Json string
 *   |  {""topicA"":[0,1],""topicB"":[2,4]}.
 *   |- <subscribe> The topic list to subscribe. A comma-separated list of
 *   |  topics.
 *   |- <subscribePattern> The pattern used to subscribe to topic(s).
 *   |  Java regex string.
 *   |- Only one of ""assign, ""subscribe"" or ""subscribePattern"" options can be
 *   |  specified for Kafka source.
 *   <topics> Different value format depends on the value of 'subscribe-type'.
 *
 * Example:
 *    `$ bin/run-example \
 *      sql.streaming.JavaStructuredKafkaWordCount host1:port1,host2:port2 \
 *      subscribe topic1,topic2`
 ",,,,,,,20,978
JavaStructuredSessionization.java,53,163,0.32515337423312884,"
 * Counts words in UTF8 encoded, '\n' delimited text received from the network.
 * <p>
 * Usage: JavaStructuredNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Structured Streaming
 * would connect to receive data.
 * <p>
 * To run this on your local machine, you need to first run a Netcat server
 * `$ nc -lk 9999`
 * and then run the example
 * `$ bin/run-example sql.streaming.JavaStructuredSessionization
 * localhost 9999`
 |
   * User-defined data type representing the raw lines with timestamps.
   |
   * User-defined data type representing the input events
   |
   * User-defined data type for storing a session information as state in mapGroupsWithState.
   |
   * User-defined data type representing the update information returned by mapGroupsWithState.
   ",,,,,,,19,776
JavaSparkSQLExample.java,172,132,1.303030303030303," $example off:create_ds$| $example on:create_ds$",,,,,,,1,48
JavaUserDefinedTypedAggregation.java,49,91,0.5384615384615384," $example off:typed_custom_aggregation$| $example on:typed_custom_aggregation$",,,,,,,1,78
JavaSparkHiveExample.java,55,57,0.9649122807017544," $example off:spark_hive$| $example on:spark_hive$",,,,,,,1,50
JavaSQLDataSourceExample.java,105,164,0.6402439024390244," $example off:schema_merging$| $example off:schema_merging$| $example on:schema_merging$| $example off:schema_merging$| $example off:schema_merging$| $example on:schema_merging$",,,,,,,1,177
JavaUserDefinedUntypedAggregation.java,47,73,0.6438356164383562," $example off:untyped_custom_aggregation$| $example on:untyped_custom_aggregation$",,,,,,,1,82
JavaConsumerStrategySuite.java,16,71,0.22535211267605634,,,,,,,,1,0
JavaKafkaRDDSuite.java,17,91,0.18681318681318682,,,,,,,,1,0
JavaLocationStrategySuite.java,17,30,0.5666666666666667,,,,,,,,1,0
JavaDirectKafkaStreamSuite.java,19,139,0.1366906474820144,,,,,,,,1,0
JavaKinesisInputDStreamBuilderSuite.java,24,65,0.36923076923076925,,"/**
 * Basic test to ensure that the KinesisDStream.Builder interface is accessible from Java.
 */
 
/**
 * Test to ensure that the old API for InitialPositionInStream
 * is supported in KinesisDStream.Builder.
 * This test would be removed when we deprecate the KinesisUtils.
 */
 
","{
    String streamName = ""a-very-nice-stream-name"";
    String endpointUrl = ""https://kinesis.us-west-2.amazonaws.com"";
    String region = ""us-west-2"";
    KinesisInitialPosition initialPosition = new TrimHorizon();
    String appName = ""a-very-nice-kinesis-app"";
    Duration checkpointInterval = Seconds.apply(30);
    StorageLevel storageLevel = StorageLevel.MEMORY_ONLY();
    KinesisInputDStream<byte[]> kinesisDStream = KinesisInputDStream.builder().streamingContext(ssc).streamName(streamName).endpointUrl(endpointUrl).regionName(region).initialPosition(initialPosition).checkpointAppName(appName).checkpointInterval(checkpointInterval).storageLevel(storageLevel).build();
    assert (kinesisDStream.streamName() == streamName);
    assert (kinesisDStream.endpointUrl() == endpointUrl);
    assert (kinesisDStream.regionName() == region);
    assert (kinesisDStream.initialPosition().getPosition() == initialPosition.getPosition());
    assert (kinesisDStream.checkpointAppName() == appName);
    assert (kinesisDStream.checkpointInterval() == checkpointInterval);
    assert (kinesisDStream._storageLevel() == storageLevel);
    ssc.stop();
} 
{
    String streamName = ""a-very-nice-stream-name"";
    String endpointUrl = ""https://kinesis.us-west-2.amazonaws.com"";
    String region = ""us-west-2"";
    String appName = ""a-very-nice-kinesis-app"";
    Duration checkpointInterval = Seconds.apply(30);
    StorageLevel storageLevel = StorageLevel.MEMORY_ONLY();
    KinesisInputDStream<byte[]> kinesisDStream = KinesisInputDStream.builder().streamingContext(ssc).streamName(streamName).endpointUrl(endpointUrl).regionName(region).initialPositionInStream(InitialPositionInStream.LATEST).checkpointAppName(appName).checkpointInterval(checkpointInterval).storageLevel(storageLevel).build();
    assert (kinesisDStream.streamName() == streamName);
    assert (kinesisDStream.endpointUrl() == endpointUrl);
    assert (kinesisDStream.regionName() == region);
    assert (kinesisDStream.initialPosition().getPosition() == InitialPositionInStream.LATEST);
    assert (kinesisDStream.checkpointAppName() == appName);
    assert (kinesisDStream.checkpointInterval() == checkpointInterval);
    assert (kinesisDStream._storageLevel() == storageLevel);
    ssc.stop();
} 
",,,,,1,0
KinesisInitialPositions.java,30,50,0.6,"
 * A java wrapper for exposing [[InitialPositionInStream]]
 * to the corresponding Kinesis readers.
 ","/**
 * Returns instance of [[KinesisInitialPosition]] based on the passed
 * [[InitialPositionInStream]]. This method is used in KinesisUtils for translating the
 * InitialPositionInStream to InitialPosition. This function would be removed when we deprecate
 * the KinesisUtils.
 *
 * @return [[InitialPosition]]
 */
 
","{
    if (initialPositionInStream == InitialPositionInStream.LATEST) {
        return new Latest();
    } else if (initialPositionInStream == InitialPositionInStream.TRIM_HORIZON) {
        return new TrimHorizon();
    } else {
        // InitialPositionInStream.AT_TIMESTAMP is not supported.
        // Use InitialPosition.atTimestamp(timestamp) instead.
        throw new UnsupportedOperationException(""Only InitialPositionInStream.LATEST and InitialPositionInStream."" + ""TRIM_HORIZON supported in initialPositionInStream(). Please use "" + ""the initialPosition() from builder API in KinesisInputDStream for "" + ""using InitialPositionInStream.AT_TIMESTAMP"");
    }
} 
",,,,,2,97
JavaKinesisWordCountASL.java,73,102,0.7156862745098039,"
 * Consumes messages from a Amazon Kinesis streams and does wordcount.
 *
 * This example spins up 1 Kinesis Receiver per shard for the given stream.
 * It then starts pulling from the last checkpointed sequence number of the given stream.
 *
 * Usage: JavaKinesisWordCountASL [app-name] [stream-name] [endpoint-url] [region-name]
 *   [app-name] is the name of the consumer app, used to track the read data in DynamoDB
 *   [stream-name] name of the Kinesis stream (ie. mySparkStream)
 *   [endpoint-url] endpoint of the Kinesis service
 *     (e.g. https://kinesis.us-east-1.amazonaws.com)
 *
 *
 * Example:
 *      # export AWS keys if necessary
 *      $ export AWS_ACCESS_KEY_ID=[your-access-key]
 *      $ export AWS_SECRET_KEY=<your-secret-key>
 *
 *      # run the example
 *      $ SPARK_HOME/bin/run-example   streaming.JavaKinesisWordCountASL myAppName  mySparkStream \
 *             https://kinesis.us-east-1.amazonaws.com
 *
 * There is a companion helper class called KinesisWordProducerASL which puts dummy data
 * onto the Kinesis stream.
 *
 * This code uses the DefaultAWSCredentialsProviderChain to find credentials
 * in the following order:
 *    Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY
 *    Java System Properties - aws.accessKeyId and aws.secretKey
 *    Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs
 *    Instance profile credentials - delivered through the Amazon EC2 metadata service
 * For more information, see
 * http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html
 *
 * See http://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more details on
 * the Kinesis Spark Streaming integration.
 ",,,,,,,35,1665
JavaAvroFunctionsSuite.java,16,44,0.36363636363636365,,,,,,,,1,0
OrcColumnVector.java,22,142,0.15492957746478872,"
 * A column vector class wrapping Hive's ColumnVector. Because Spark ColumnarBatch only accepts
 * Spark's vectorized.ColumnVector, this column vector is used to adapt Hive ColumnVector with
 * Spark ColumnarVector.
 ",,,,,,,3,211
OrcColumnVector.java,22,142,0.15492957746478872,"
 * A column vector class wrapping Hive's ColumnVector. Because Spark ColumnarBatch only accepts
 * Spark's vectorized.ColumnVector, this column vector is used to adapt Hive ColumnVector with
 * Spark ColumnarVector.
 ",,,,,,,3,211
AvroOptionalPrimitives.java,101,311,0.3247588424437299,"
   * RecordBuilder for AvroOptionalPrimitives instances.
   ","/**
 * Gets the value of the 'maybe_bool_column' field.
 */
 
/**
 * Sets the value of the 'maybe_bool_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_int_column' field.
 */
 
/**
 * Sets the value of the 'maybe_int_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_long_column' field.
 */
 
/**
 * Sets the value of the 'maybe_long_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_float_column' field.
 */
 
/**
 * Sets the value of the 'maybe_float_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_double_column' field.
 */
 
/**
 * Sets the value of the 'maybe_double_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_binary_column' field.
 */
 
/**
 * Sets the value of the 'maybe_binary_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_string_column' field.
 */
 
/**
 * Sets the value of the 'maybe_string_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new AvroOptionalPrimitives RecordBuilder
 */
 
/**
 * Creates a new AvroOptionalPrimitives RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new AvroOptionalPrimitives RecordBuilder by copying an existing AvroOptionalPrimitives instance
 */
 
/**
 * Gets the value of the 'maybe_bool_column' field
 */
 
/**
 * Sets the value of the 'maybe_bool_column' field
 */
 
/**
 * Checks whether the 'maybe_bool_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_bool_column' field
 */
 
/**
 * Gets the value of the 'maybe_int_column' field
 */
 
/**
 * Sets the value of the 'maybe_int_column' field
 */
 
/**
 * Checks whether the 'maybe_int_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_int_column' field
 */
 
/**
 * Gets the value of the 'maybe_long_column' field
 */
 
/**
 * Sets the value of the 'maybe_long_column' field
 */
 
/**
 * Checks whether the 'maybe_long_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_long_column' field
 */
 
/**
 * Gets the value of the 'maybe_float_column' field
 */
 
/**
 * Sets the value of the 'maybe_float_column' field
 */
 
/**
 * Checks whether the 'maybe_float_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_float_column' field
 */
 
/**
 * Gets the value of the 'maybe_double_column' field
 */
 
/**
 * Sets the value of the 'maybe_double_column' field
 */
 
/**
 * Checks whether the 'maybe_double_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_double_column' field
 */
 
/**
 * Gets the value of the 'maybe_binary_column' field
 */
 
/**
 * Sets the value of the 'maybe_binary_column' field
 */
 
/**
 * Checks whether the 'maybe_binary_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_binary_column' field
 */
 
/**
 * Gets the value of the 'maybe_string_column' field
 */
 
/**
 * Sets the value of the 'maybe_string_column' field
 */
 
/**
 * Checks whether the 'maybe_string_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_string_column' field
 */
 
","{
    return maybe_bool_column;
} 
{
    this.maybe_bool_column = value;
} 
{
    return maybe_int_column;
} 
{
    this.maybe_int_column = value;
} 
{
    return maybe_long_column;
} 
{
    this.maybe_long_column = value;
} 
{
    return maybe_float_column;
} 
{
    this.maybe_float_column = value;
} 
{
    return maybe_double_column;
} 
{
    this.maybe_double_column = value;
} 
{
    return maybe_binary_column;
} 
{
    this.maybe_binary_column = value;
} 
{
    return maybe_string_column;
} 
{
    this.maybe_string_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder(other);
} 
{
    return maybe_bool_column;
} 
{
    validate(fields()[0], value);
    this.maybe_bool_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    maybe_bool_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
{
    return maybe_int_column;
} 
{
    validate(fields()[1], value);
    this.maybe_int_column = value;
    fieldSetFlags()[1] = true;
    return this;
} 
{
    return fieldSetFlags()[1];
} 
{
    maybe_int_column = null;
    fieldSetFlags()[1] = false;
    return this;
} 
{
    return maybe_long_column;
} 
{
    validate(fields()[2], value);
    this.maybe_long_column = value;
    fieldSetFlags()[2] = true;
    return this;
} 
{
    return fieldSetFlags()[2];
} 
{
    maybe_long_column = null;
    fieldSetFlags()[2] = false;
    return this;
} 
{
    return maybe_float_column;
} 
{
    validate(fields()[3], value);
    this.maybe_float_column = value;
    fieldSetFlags()[3] = true;
    return this;
} 
{
    return fieldSetFlags()[3];
} 
{
    maybe_float_column = null;
    fieldSetFlags()[3] = false;
    return this;
} 
{
    return maybe_double_column;
} 
{
    validate(fields()[4], value);
    this.maybe_double_column = value;
    fieldSetFlags()[4] = true;
    return this;
} 
{
    return fieldSetFlags()[4];
} 
{
    maybe_double_column = null;
    fieldSetFlags()[4] = false;
    return this;
} 
{
    return maybe_binary_column;
} 
{
    validate(fields()[5], value);
    this.maybe_binary_column = value;
    fieldSetFlags()[5] = true;
    return this;
} 
{
    return fieldSetFlags()[5];
} 
{
    maybe_binary_column = null;
    fieldSetFlags()[5] = false;
    return this;
} 
{
    return maybe_string_column;
} 
{
    validate(fields()[6], value);
    this.maybe_string_column = value;
    fieldSetFlags()[6] = true;
    return this;
} 
{
    return fieldSetFlags()[6];
} 
{
    maybe_string_column = null;
    fieldSetFlags()[6] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing AvroOptionalPrimitives instance
 */
 
","{
} 
{
    this.maybe_bool_column = maybe_bool_column;
    this.maybe_int_column = maybe_int_column;
    this.maybe_long_column = maybe_long_column;
    this.maybe_float_column = maybe_float_column;
    this.maybe_double_column = maybe_double_column;
    this.maybe_binary_column = maybe_binary_column;
    this.maybe_string_column = maybe_string_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.maybe_bool_column)) {
        this.maybe_bool_column = data().deepCopy(fields()[0].schema(), other.maybe_bool_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.maybe_int_column)) {
        this.maybe_int_column = data().deepCopy(fields()[1].schema(), other.maybe_int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.maybe_long_column)) {
        this.maybe_long_column = data().deepCopy(fields()[2].schema(), other.maybe_long_column);
        fieldSetFlags()[2] = true;
    }
    if (isValidValue(fields()[3], other.maybe_float_column)) {
        this.maybe_float_column = data().deepCopy(fields()[3].schema(), other.maybe_float_column);
        fieldSetFlags()[3] = true;
    }
    if (isValidValue(fields()[4], other.maybe_double_column)) {
        this.maybe_double_column = data().deepCopy(fields()[4].schema(), other.maybe_double_column);
        fieldSetFlags()[4] = true;
    }
    if (isValidValue(fields()[5], other.maybe_binary_column)) {
        this.maybe_binary_column = data().deepCopy(fields()[5].schema(), other.maybe_binary_column);
        fieldSetFlags()[5] = true;
    }
    if (isValidValue(fields()[6], other.maybe_string_column)) {
        this.maybe_string_column = data().deepCopy(fields()[6].schema(), other.maybe_string_column);
        fieldSetFlags()[6] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.SCHEMA$);
    if (isValidValue(fields()[0], other.maybe_bool_column)) {
        this.maybe_bool_column = data().deepCopy(fields()[0].schema(), other.maybe_bool_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.maybe_int_column)) {
        this.maybe_int_column = data().deepCopy(fields()[1].schema(), other.maybe_int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.maybe_long_column)) {
        this.maybe_long_column = data().deepCopy(fields()[2].schema(), other.maybe_long_column);
        fieldSetFlags()[2] = true;
    }
    if (isValidValue(fields()[3], other.maybe_float_column)) {
        this.maybe_float_column = data().deepCopy(fields()[3].schema(), other.maybe_float_column);
        fieldSetFlags()[3] = true;
    }
    if (isValidValue(fields()[4], other.maybe_double_column)) {
        this.maybe_double_column = data().deepCopy(fields()[4].schema(), other.maybe_double_column);
        fieldSetFlags()[4] = true;
    }
    if (isValidValue(fields()[5], other.maybe_binary_column)) {
        this.maybe_binary_column = data().deepCopy(fields()[5].schema(), other.maybe_binary_column);
        fieldSetFlags()[5] = true;
    }
    if (isValidValue(fields()[6], other.maybe_string_column)) {
        this.maybe_string_column = data().deepCopy(fields()[6].schema(), other.maybe_string_column);
        fieldSetFlags()[6] = true;
    }
} 
",,,1,58
CompatibilityTest.java,5,10,0.5,,,,,,,,1,0
Suit.java,5,8,0.625,,,,,,,,1,0
AvroNonNullableArrays.java,46,127,0.36220472440944884,"
   * RecordBuilder for AvroNonNullableArrays instances.
   ","/**
 * Gets the value of the 'strings_column' field.
 */
 
/**
 * Sets the value of the 'strings_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'maybe_ints_column' field.
 */
 
/**
 * Sets the value of the 'maybe_ints_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new AvroNonNullableArrays RecordBuilder
 */
 
/**
 * Creates a new AvroNonNullableArrays RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new AvroNonNullableArrays RecordBuilder by copying an existing AvroNonNullableArrays instance
 */
 
/**
 * Gets the value of the 'strings_column' field
 */
 
/**
 * Sets the value of the 'strings_column' field
 */
 
/**
 * Checks whether the 'strings_column' field has been set
 */
 
/**
 * Clears the value of the 'strings_column' field
 */
 
/**
 * Gets the value of the 'maybe_ints_column' field
 */
 
/**
 * Sets the value of the 'maybe_ints_column' field
 */
 
/**
 * Checks whether the 'maybe_ints_column' field has been set
 */
 
/**
 * Clears the value of the 'maybe_ints_column' field
 */
 
","{
    return strings_column;
} 
{
    this.strings_column = value;
} 
{
    return maybe_ints_column;
} 
{
    this.maybe_ints_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder(other);
} 
{
    return strings_column;
} 
{
    validate(fields()[0], value);
    this.strings_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    strings_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
{
    return maybe_ints_column;
} 
{
    validate(fields()[1], value);
    this.maybe_ints_column = value;
    fieldSetFlags()[1] = true;
    return this;
} 
{
    return fieldSetFlags()[1];
} 
{
    maybe_ints_column = null;
    fieldSetFlags()[1] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing AvroNonNullableArrays instance
 */
 
","{
} 
{
    this.strings_column = strings_column;
    this.maybe_ints_column = maybe_ints_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.strings_column)) {
        this.strings_column = data().deepCopy(fields()[0].schema(), other.strings_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.maybe_ints_column)) {
        this.maybe_ints_column = data().deepCopy(fields()[1].schema(), other.maybe_ints_column);
        fieldSetFlags()[1] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.SCHEMA$);
    if (isValidValue(fields()[0], other.strings_column)) {
        this.strings_column = data().deepCopy(fields()[0].schema(), other.strings_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.maybe_ints_column)) {
        this.maybe_ints_column = data().deepCopy(fields()[1].schema(), other.maybe_ints_column);
        fieldSetFlags()[1] = true;
    }
} 
",,,1,57
ParquetAvroCompat.java,57,164,0.3475609756097561,"
   * RecordBuilder for ParquetAvroCompat instances.
   ","/**
 * Gets the value of the 'strings_column' field.
 */
 
/**
 * Sets the value of the 'strings_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'string_to_int_column' field.
 */
 
/**
 * Sets the value of the 'string_to_int_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'complex_column' field.
 */
 
/**
 * Sets the value of the 'complex_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new ParquetAvroCompat RecordBuilder
 */
 
/**
 * Creates a new ParquetAvroCompat RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new ParquetAvroCompat RecordBuilder by copying an existing ParquetAvroCompat instance
 */
 
/**
 * Gets the value of the 'strings_column' field
 */
 
/**
 * Sets the value of the 'strings_column' field
 */
 
/**
 * Checks whether the 'strings_column' field has been set
 */
 
/**
 * Clears the value of the 'strings_column' field
 */
 
/**
 * Gets the value of the 'string_to_int_column' field
 */
 
/**
 * Sets the value of the 'string_to_int_column' field
 */
 
/**
 * Checks whether the 'string_to_int_column' field has been set
 */
 
/**
 * Clears the value of the 'string_to_int_column' field
 */
 
/**
 * Gets the value of the 'complex_column' field
 */
 
/**
 * Sets the value of the 'complex_column' field
 */
 
/**
 * Checks whether the 'complex_column' field has been set
 */
 
/**
 * Clears the value of the 'complex_column' field
 */
 
","{
    return strings_column;
} 
{
    this.strings_column = value;
} 
{
    return string_to_int_column;
} 
{
    this.string_to_int_column = value;
} 
{
    return complex_column;
} 
{
    this.complex_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder(other);
} 
{
    return strings_column;
} 
{
    validate(fields()[0], value);
    this.strings_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    strings_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
{
    return string_to_int_column;
} 
{
    validate(fields()[1], value);
    this.string_to_int_column = value;
    fieldSetFlags()[1] = true;
    return this;
} 
{
    return fieldSetFlags()[1];
} 
{
    string_to_int_column = null;
    fieldSetFlags()[1] = false;
    return this;
} 
{
    return complex_column;
} 
{
    validate(fields()[2], value);
    this.complex_column = value;
    fieldSetFlags()[2] = true;
    return this;
} 
{
    return fieldSetFlags()[2];
} 
{
    complex_column = null;
    fieldSetFlags()[2] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing ParquetAvroCompat instance
 */
 
","{
} 
{
    this.strings_column = strings_column;
    this.string_to_int_column = string_to_int_column;
    this.complex_column = complex_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.strings_column)) {
        this.strings_column = data().deepCopy(fields()[0].schema(), other.strings_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.string_to_int_column)) {
        this.string_to_int_column = data().deepCopy(fields()[1].schema(), other.string_to_int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.complex_column)) {
        this.complex_column = data().deepCopy(fields()[2].schema(), other.complex_column);
        fieldSetFlags()[2] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.SCHEMA$);
    if (isValidValue(fields()[0], other.strings_column)) {
        this.strings_column = data().deepCopy(fields()[0].schema(), other.strings_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.string_to_int_column)) {
        this.string_to_int_column = data().deepCopy(fields()[1].schema(), other.string_to_int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.complex_column)) {
        this.complex_column = data().deepCopy(fields()[2].schema(), other.complex_column);
        fieldSetFlags()[2] = true;
    }
} 
",,,1,53
AvroPrimitives.java,101,306,0.3300653594771242,"
   * RecordBuilder for AvroPrimitives instances.
   ","/**
 * Gets the value of the 'bool_column' field.
 */
 
/**
 * Sets the value of the 'bool_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'int_column' field.
 */
 
/**
 * Sets the value of the 'int_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'long_column' field.
 */
 
/**
 * Sets the value of the 'long_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'float_column' field.
 */
 
/**
 * Sets the value of the 'float_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'double_column' field.
 */
 
/**
 * Sets the value of the 'double_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'binary_column' field.
 */
 
/**
 * Sets the value of the 'binary_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'string_column' field.
 */
 
/**
 * Sets the value of the 'string_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new AvroPrimitives RecordBuilder
 */
 
/**
 * Creates a new AvroPrimitives RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new AvroPrimitives RecordBuilder by copying an existing AvroPrimitives instance
 */
 
/**
 * Gets the value of the 'bool_column' field
 */
 
/**
 * Sets the value of the 'bool_column' field
 */
 
/**
 * Checks whether the 'bool_column' field has been set
 */
 
/**
 * Clears the value of the 'bool_column' field
 */
 
/**
 * Gets the value of the 'int_column' field
 */
 
/**
 * Sets the value of the 'int_column' field
 */
 
/**
 * Checks whether the 'int_column' field has been set
 */
 
/**
 * Clears the value of the 'int_column' field
 */
 
/**
 * Gets the value of the 'long_column' field
 */
 
/**
 * Sets the value of the 'long_column' field
 */
 
/**
 * Checks whether the 'long_column' field has been set
 */
 
/**
 * Clears the value of the 'long_column' field
 */
 
/**
 * Gets the value of the 'float_column' field
 */
 
/**
 * Sets the value of the 'float_column' field
 */
 
/**
 * Checks whether the 'float_column' field has been set
 */
 
/**
 * Clears the value of the 'float_column' field
 */
 
/**
 * Gets the value of the 'double_column' field
 */
 
/**
 * Sets the value of the 'double_column' field
 */
 
/**
 * Checks whether the 'double_column' field has been set
 */
 
/**
 * Clears the value of the 'double_column' field
 */
 
/**
 * Gets the value of the 'binary_column' field
 */
 
/**
 * Sets the value of the 'binary_column' field
 */
 
/**
 * Checks whether the 'binary_column' field has been set
 */
 
/**
 * Clears the value of the 'binary_column' field
 */
 
/**
 * Gets the value of the 'string_column' field
 */
 
/**
 * Sets the value of the 'string_column' field
 */
 
/**
 * Checks whether the 'string_column' field has been set
 */
 
/**
 * Clears the value of the 'string_column' field
 */
 
","{
    return bool_column;
} 
{
    this.bool_column = value;
} 
{
    return int_column;
} 
{
    this.int_column = value;
} 
{
    return long_column;
} 
{
    this.long_column = value;
} 
{
    return float_column;
} 
{
    this.float_column = value;
} 
{
    return double_column;
} 
{
    this.double_column = value;
} 
{
    return binary_column;
} 
{
    this.binary_column = value;
} 
{
    return string_column;
} 
{
    this.string_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder(other);
} 
{
    return bool_column;
} 
{
    validate(fields()[0], value);
    this.bool_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    fieldSetFlags()[0] = false;
    return this;
} 
{
    return int_column;
} 
{
    validate(fields()[1], value);
    this.int_column = value;
    fieldSetFlags()[1] = true;
    return this;
} 
{
    return fieldSetFlags()[1];
} 
{
    fieldSetFlags()[1] = false;
    return this;
} 
{
    return long_column;
} 
{
    validate(fields()[2], value);
    this.long_column = value;
    fieldSetFlags()[2] = true;
    return this;
} 
{
    return fieldSetFlags()[2];
} 
{
    fieldSetFlags()[2] = false;
    return this;
} 
{
    return float_column;
} 
{
    validate(fields()[3], value);
    this.float_column = value;
    fieldSetFlags()[3] = true;
    return this;
} 
{
    return fieldSetFlags()[3];
} 
{
    fieldSetFlags()[3] = false;
    return this;
} 
{
    return double_column;
} 
{
    validate(fields()[4], value);
    this.double_column = value;
    fieldSetFlags()[4] = true;
    return this;
} 
{
    return fieldSetFlags()[4];
} 
{
    fieldSetFlags()[4] = false;
    return this;
} 
{
    return binary_column;
} 
{
    validate(fields()[5], value);
    this.binary_column = value;
    fieldSetFlags()[5] = true;
    return this;
} 
{
    return fieldSetFlags()[5];
} 
{
    binary_column = null;
    fieldSetFlags()[5] = false;
    return this;
} 
{
    return string_column;
} 
{
    validate(fields()[6], value);
    this.string_column = value;
    fieldSetFlags()[6] = true;
    return this;
} 
{
    return fieldSetFlags()[6];
} 
{
    string_column = null;
    fieldSetFlags()[6] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing AvroPrimitives instance
 */
 
","{
} 
{
    this.bool_column = bool_column;
    this.int_column = int_column;
    this.long_column = long_column;
    this.float_column = float_column;
    this.double_column = double_column;
    this.binary_column = binary_column;
    this.string_column = string_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.bool_column)) {
        this.bool_column = data().deepCopy(fields()[0].schema(), other.bool_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.int_column)) {
        this.int_column = data().deepCopy(fields()[1].schema(), other.int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.long_column)) {
        this.long_column = data().deepCopy(fields()[2].schema(), other.long_column);
        fieldSetFlags()[2] = true;
    }
    if (isValidValue(fields()[3], other.float_column)) {
        this.float_column = data().deepCopy(fields()[3].schema(), other.float_column);
        fieldSetFlags()[3] = true;
    }
    if (isValidValue(fields()[4], other.double_column)) {
        this.double_column = data().deepCopy(fields()[4].schema(), other.double_column);
        fieldSetFlags()[4] = true;
    }
    if (isValidValue(fields()[5], other.binary_column)) {
        this.binary_column = data().deepCopy(fields()[5].schema(), other.binary_column);
        fieldSetFlags()[5] = true;
    }
    if (isValidValue(fields()[6], other.string_column)) {
        this.string_column = data().deepCopy(fields()[6].schema(), other.string_column);
        fieldSetFlags()[6] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.SCHEMA$);
    if (isValidValue(fields()[0], other.bool_column)) {
        this.bool_column = data().deepCopy(fields()[0].schema(), other.bool_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.int_column)) {
        this.int_column = data().deepCopy(fields()[1].schema(), other.int_column);
        fieldSetFlags()[1] = true;
    }
    if (isValidValue(fields()[2], other.long_column)) {
        this.long_column = data().deepCopy(fields()[2].schema(), other.long_column);
        fieldSetFlags()[2] = true;
    }
    if (isValidValue(fields()[3], other.float_column)) {
        this.float_column = data().deepCopy(fields()[3].schema(), other.float_column);
        fieldSetFlags()[3] = true;
    }
    if (isValidValue(fields()[4], other.double_column)) {
        this.double_column = data().deepCopy(fields()[4].schema(), other.double_column);
        fieldSetFlags()[4] = true;
    }
    if (isValidValue(fields()[5], other.binary_column)) {
        this.binary_column = data().deepCopy(fields()[5].schema(), other.binary_column);
        fieldSetFlags()[5] = true;
    }
    if (isValidValue(fields()[6], other.string_column)) {
        this.string_column = data().deepCopy(fields()[6].schema(), other.string_column);
        fieldSetFlags()[6] = true;
    }
} 
",,,1,50
ParquetEnum.java,35,89,0.39325842696629215,"
   * RecordBuilder for ParquetEnum instances.
   ","/**
 * Gets the value of the 'suit' field.
 */
 
/**
 * Sets the value of the 'suit' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new ParquetEnum RecordBuilder
 */
 
/**
 * Creates a new ParquetEnum RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new ParquetEnum RecordBuilder by copying an existing ParquetEnum instance
 */
 
/**
 * Gets the value of the 'suit' field
 */
 
/**
 * Sets the value of the 'suit' field
 */
 
/**
 * Checks whether the 'suit' field has been set
 */
 
/**
 * Clears the value of the 'suit' field
 */
 
","{
    return suit;
} 
{
    this.suit = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder(other);
} 
{
    return suit;
} 
{
    validate(fields()[0], value);
    this.suit = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    suit = null;
    fieldSetFlags()[0] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing ParquetEnum instance
 */
 
","{
} 
{
    this.suit = suit;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.suit)) {
        this.suit = data().deepCopy(fields()[0].schema(), other.suit);
        fieldSetFlags()[0] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.SCHEMA$);
    if (isValidValue(fields()[0], other.suit)) {
        this.suit = data().deepCopy(fields()[0].schema(), other.suit);
        fieldSetFlags()[0] = true;
    }
} 
",,,1,47
AvroMapOfArray.java,35,90,0.3888888888888889,"
   * RecordBuilder for AvroMapOfArray instances.
   ","/**
 * Gets the value of the 'string_to_ints_column' field.
 */
 
/**
 * Sets the value of the 'string_to_ints_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new AvroMapOfArray RecordBuilder
 */
 
/**
 * Creates a new AvroMapOfArray RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new AvroMapOfArray RecordBuilder by copying an existing AvroMapOfArray instance
 */
 
/**
 * Gets the value of the 'string_to_ints_column' field
 */
 
/**
 * Sets the value of the 'string_to_ints_column' field
 */
 
/**
 * Checks whether the 'string_to_ints_column' field has been set
 */
 
/**
 * Clears the value of the 'string_to_ints_column' field
 */
 
","{
    return string_to_ints_column;
} 
{
    this.string_to_ints_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder(other);
} 
{
    return string_to_ints_column;
} 
{
    validate(fields()[0], value);
    this.string_to_ints_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    string_to_ints_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing AvroMapOfArray instance
 */
 
","{
} 
{
    this.string_to_ints_column = string_to_ints_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.string_to_ints_column)) {
        this.string_to_ints_column = data().deepCopy(fields()[0].schema(), other.string_to_ints_column);
        fieldSetFlags()[0] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.SCHEMA$);
    if (isValidValue(fields()[0], other.string_to_ints_column)) {
        this.string_to_ints_column = data().deepCopy(fields()[0].schema(), other.string_to_ints_column);
        fieldSetFlags()[0] = true;
    }
} 
",,,1,50
AvroArrayOfArray.java,35,90,0.3888888888888889,"
   * RecordBuilder for AvroArrayOfArray instances.
   ","/**
 * Gets the value of the 'int_arrays_column' field.
 */
 
/**
 * Sets the value of the 'int_arrays_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new AvroArrayOfArray RecordBuilder
 */
 
/**
 * Creates a new AvroArrayOfArray RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new AvroArrayOfArray RecordBuilder by copying an existing AvroArrayOfArray instance
 */
 
/**
 * Gets the value of the 'int_arrays_column' field
 */
 
/**
 * Sets the value of the 'int_arrays_column' field
 */
 
/**
 * Checks whether the 'int_arrays_column' field has been set
 */
 
/**
 * Clears the value of the 'int_arrays_column' field
 */
 
","{
    return int_arrays_column;
} 
{
    this.int_arrays_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder(other);
} 
{
    return int_arrays_column;
} 
{
    validate(fields()[0], value);
    this.int_arrays_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    int_arrays_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing AvroArrayOfArray instance
 */
 
","{
} 
{
    this.int_arrays_column = int_arrays_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.int_arrays_column)) {
        this.int_arrays_column = data().deepCopy(fields()[0].schema(), other.int_arrays_column);
        fieldSetFlags()[0] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.SCHEMA$);
    if (isValidValue(fields()[0], other.int_arrays_column)) {
        this.int_arrays_column = data().deepCopy(fields()[0].schema(), other.int_arrays_column);
        fieldSetFlags()[0] = true;
    }
} 
",,,1,52
Nested.java,46,127,0.36220472440944884,"
   * RecordBuilder for Nested instances.
   ","/**
 * Gets the value of the 'nested_ints_column' field.
 */
 
/**
 * Sets the value of the 'nested_ints_column' field.
 * @param value the value to set.
 */
 
/**
 * Gets the value of the 'nested_string_column' field.
 */
 
/**
 * Sets the value of the 'nested_string_column' field.
 * @param value the value to set.
 */
 
/**
 * Creates a new Nested RecordBuilder
 */
 
/**
 * Creates a new Nested RecordBuilder by copying an existing Builder
 */
 
/**
 * Creates a new Nested RecordBuilder by copying an existing Nested instance
 */
 
/**
 * Gets the value of the 'nested_ints_column' field
 */
 
/**
 * Sets the value of the 'nested_ints_column' field
 */
 
/**
 * Checks whether the 'nested_ints_column' field has been set
 */
 
/**
 * Clears the value of the 'nested_ints_column' field
 */
 
/**
 * Gets the value of the 'nested_string_column' field
 */
 
/**
 * Sets the value of the 'nested_string_column' field
 */
 
/**
 * Checks whether the 'nested_string_column' field has been set
 */
 
/**
 * Clears the value of the 'nested_string_column' field
 */
 
","{
    return nested_ints_column;
} 
{
    this.nested_ints_column = value;
} 
{
    return nested_string_column;
} 
{
    this.nested_string_column = value;
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder();
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder(other);
} 
{
    return new org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder(other);
} 
{
    return nested_ints_column;
} 
{
    validate(fields()[0], value);
    this.nested_ints_column = value;
    fieldSetFlags()[0] = true;
    return this;
} 
{
    return fieldSetFlags()[0];
} 
{
    nested_ints_column = null;
    fieldSetFlags()[0] = false;
    return this;
} 
{
    return nested_string_column;
} 
{
    validate(fields()[1], value);
    this.nested_string_column = value;
    fieldSetFlags()[1] = true;
    return this;
} 
{
    return fieldSetFlags()[1];
} 
{
    nested_string_column = null;
    fieldSetFlags()[1] = false;
    return this;
} 
","/**
 * Default constructor.  Note that this does not initialize fields
 * to their default values from the schema.  If that is desired then
 * one should use <code>newBuilder()</code>.
 */
 
/**
 * All-args constructor.
 */
 
/**
 * Creates a new Builder
 */
 
/**
 * Creates a Builder by copying an existing Builder
 */
 
/**
 * Creates a Builder by copying an existing Nested instance
 */
 
","{
} 
{
    this.nested_ints_column = nested_ints_column;
    this.nested_string_column = nested_string_column;
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.SCHEMA$);
} 
{
    super(other);
    if (isValidValue(fields()[0], other.nested_ints_column)) {
        this.nested_ints_column = data().deepCopy(fields()[0].schema(), other.nested_ints_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.nested_string_column)) {
        this.nested_string_column = data().deepCopy(fields()[1].schema(), other.nested_string_column);
        fieldSetFlags()[1] = true;
    }
} 
{
    super(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.SCHEMA$);
    if (isValidValue(fields()[0], other.nested_ints_column)) {
        this.nested_ints_column = data().deepCopy(fields()[0].schema(), other.nested_ints_column);
        fieldSetFlags()[0] = true;
    }
    if (isValidValue(fields()[1], other.nested_string_column)) {
        this.nested_string_column = data().deepCopy(fields()[1].schema(), other.nested_string_column);
        fieldSetFlags()[1] = true;
    }
} 
",,,1,42
JavaTestUtils.java,16,23,0.6956521739130435,,,,,,,,1,0
JavaDatasetAggregatorSuite.java,19,86,0.22093023255813954,"
 * Suite for testing the aggregate functionality of Datasets in Java.
 ",,,,,,,1,69
JavaDatasetSuite.java,30,1240,0.024193548387096774,"
   * For testing error messages when creating an encoder on a private class. This is done
   * here since we cannot create truly private classes in Scala.
   ",,,,,,,2,154
MyDoubleSum.java,34,66,0.5151515151515151,"
 * An example {@link UserDefinedAggregateFunction} to calculate the sum of a
 * {@link org.apache.spark.sql.types.DoubleType} column.
 ",,,,,,,2,131
JavaDatasetAggregatorSuiteBase.java,20,34,0.5882352941176471,"
 * Common test base shared across this and Java8DatasetAggregatorSuite.
 ",,,,,,,1,71
JavaApplySchemaSuite.java,19,155,0.12258064516129032," The test suite itself is Serializable so that anonymous Function implementations can be| serialized, as an alternative to converting these anonymous classes to static inner classes;| see http://stackoverflow.com/questions/758570/.",,,,,,,1,231
JavaUDFSuite.java,21,102,0.20588235294117646," The test suite itself is Serializable so that anonymous Function implementations can be| serialized, as an alternative to converting these anonymous classes to static inner classes;| see http://stackoverflow.com/questions/758570/.",,,,,,,1,231
JavaSaveLoadSuite.java,16,70,0.22857142857142856,,,,,,,,1,0
JavaDataFrameSuite.java,37,412,0.08980582524271845,,"/**
 * See SPARK-5904. Abstract vararg methods defined in Scala do not work in Java.
 */
 
","{
    Dataset<Row> df = spark.table(""testData"");
    df.toDF(""key1"", ""value1"");
    df.select(""key"", ""value"");
    df.select(col(""key""), col(""value""));
    df.selectExpr(""key"", ""value + 1"");
    df.sort(""key"", ""value"");
    df.sort(col(""key""), col(""value""));
    df.orderBy(""key"", ""value"");
    df.orderBy(col(""key""), col(""value""));
    df.groupBy(""key"", ""value"").agg(col(""key""), col(""value""), sum(""value""));
    df.groupBy(col(""key""), col(""value"")).agg(col(""key""), col(""value""), sum(""value""));
    df.agg(first(""key""), sum(""value""));
    df.groupBy().avg(""key"");
    df.groupBy().mean(""key"");
    df.groupBy().max(""key"");
    df.groupBy().min(""key"");
    df.groupBy().sum(""key"");
    // Varargs in column expressions
    df.groupBy().agg(countDistinct(""key"", ""value""));
    df.groupBy().agg(countDistinct(col(""key""), col(""value"")));
    df.select(coalesce(col(""key"")));
    // Varargs with mathfunctions
    Dataset<Row> df2 = spark.table(""testData2"");
    df2.select(exp(""a""), exp(""b""));
    df2.select(exp(log(""a"")));
    df2.select(pow(""a"", ""a""), pow(""b"", 2.0));
    df2.select(pow(col(""a""), col(""b"")), exp(""b""));
    df2.select(sin(""a""), acos(""b""));
    df2.select(rand(), acos(""b""));
    df2.select(col(""*""), randn(5L));
} 
",,,,,1,0
JavaDataFrameReaderWriterSuite.java,20,122,0.16393442622950818,,"/**
 * This only tests whether API compiles, but does not run it as orc()
 * cannot be run without Hive classes.
 */
 
","{
    spark.read().schema(schema).orc();
    spark.read().schema(schema).orc(input);
    spark.read().schema(schema).orc(input, input, input);
    spark.read().schema(schema).orc(new String[] { input, input }).write().orc(output);
} 
",,,,,1,0
MyDoubleAvg.java,39,72,0.5416666666666666,"
 * An example {@link UserDefinedAggregateFunction} to calculate a special average value of a
 * {@link org.apache.spark.sql.types.DoubleType} column. This special average value is the sum
 * of the average value of input values and 100.0.
 ",,,,,,,3,234
JavaDataStreamReaderWriterSuite.java,16,62,0.25806451612903225,,,,,,,,1,0
JavaBeanDeserializationSuite.java,26,466,0.055793991416309016,,,,,,,,1,0
JavaColumnExpressionSuite.java,17,69,0.2463768115942029,,,,,,,,1,0
JavaHigherOrderFunctionsSuite.java,16,201,0.07960199004975124,,,,,,,,1,0
RecordBinaryComparatorSuite.java,26,236,0.11016949152542373,"
 * Test the RecordBinaryComparator, which compares two UnsafeRows by their binary form.
 ",,,,,,,1,87
JavaSimpleScanBuilder.java,16,24,0.6666666666666666,,,,,,,,1,0
JavaRangeInputPartition.java,16,10,1.6,,,,,,,,1,0
JavaReportStatisticsDataSource.java,16,42,0.38095238095238093,,,,,,,,1,0
JavaAdvancedDataSourceV2.java,17,138,0.12318840579710146,,,,,,,,1,0
JavaPartitionAwareDataSource.java,16,90,0.17777777777777778,,,,,,,,1,0
JavaSimpleBatchTable.java,16,26,0.6153846153846154,,,,,,,,1,0
JavaSchemaRequiredDataSource.java,16,40,0.4,,,,,,,,1,0
JavaSimpleDataSourceV2.java,16,26,0.6153846153846154,,,,,,,,1,0
JavaSimpleReaderFactory.java,16,27,0.5925925925925926,,,,,,,,1,0
JavaColumnarDataSourceV2.java,16,88,0.18181818181818182,,,,,,,,1,0
JavaUDAFSuite.java,16,30,0.5333333333333333,,,,,,,,1,0
Java8DatasetAggregatorSuite.java,19,42,0.4523809523809524,"
 * Suite that replicates tests in JavaDatasetAggregatorSuite using lambda syntax.
 ",,,,,,,1,81
JavaRowSuite.java,25,140,0.17857142857142858,,,,,,,,1,0
JavaStringLength.java,19,8,2.375,"
 * It is used for register Java UDF from PySpark
 ",,,,,,,1,48
JavaDataFrameWriterV2Suite.java,16,76,0.21052631578947367,,,,,,,,1,0
MapGroupsWithStateFunction.java,23,11,2.090909090909091,"
 * ::Experimental::
 * Base interface for a map function used in
 * {@link org.apache.spark.sql.KeyValueGroupedDataset#mapGroupsWithState(
 * MapGroupsWithStateFunction, org.apache.spark.sql.Encoder, org.apache.spark.sql.Encoder)}
 * @since 2.1.1
 ",,,,,,,5,238
FlatMapGroupsWithStateFunction.java,24,11,2.1818181818181817,"
 * ::Experimental::
 * Base interface for a map function used in
 * {@code org.apache.spark.sql.KeyValueGroupedDataset.flatMapGroupsWithState(
 * FlatMapGroupsWithStateFunction, org.apache.spark.sql.streaming.OutputMode,
 * org.apache.spark.sql.Encoder, org.apache.spark.sql.Encoder)}
 * @since 2.1.1
 ",,,,,,,6,290
NonClosableMutableURLClassLoader.java,19,13,1.4615384615384615,"
 * This class loader cannot be closed (its `close` method is a no-op).
 ",,,,,,,1,70
Trigger.java,102,37,2.7567567567567566,"
 * Policy used to indicate how often results should be produced by a [[StreamingQuery]].
 *
 * @since 2.0.0
 ","/**
 * A trigger policy that runs a query periodically based on an interval in processing time.
 * If `interval` is 0, the query will run as fast as possible.
 *
 * @since 2.2.0
 */
 
/**
 * (Java-friendly)
 * A trigger policy that runs a query periodically based on an interval in processing time.
 * If `interval` is 0, the query will run as fast as possible.
 *
 * {{{
 *    import java.util.concurrent.TimeUnit
 *    df.writeStream().trigger(Trigger.ProcessingTime(10, TimeUnit.SECONDS))
 * }}}
 *
 * @since 2.2.0
 */
 
/**
 * (Scala-friendly)
 * A trigger policy that runs a query periodically based on an interval in processing time.
 * If `duration` is 0, the query will run as fast as possible.
 *
 * {{{
 *    import scala.concurrent.duration._
 *    df.writeStream.trigger(Trigger.ProcessingTime(10.seconds))
 * }}}
 * @since 2.2.0
 */
 
/**
 * A trigger policy that runs a query periodically based on an interval in processing time.
 * If `interval` is effectively 0, the query will run as fast as possible.
 *
 * {{{
 *    df.writeStream.trigger(Trigger.ProcessingTime(""10 seconds""))
 * }}}
 * @since 2.2.0
 */
 
/**
 * A trigger that process only one batch of data in a streaming query then terminates
 * the query.
 *
 * @since 2.2.0
 */
 
/**
 * A trigger that continuously processes streaming data, asynchronously checkpointing at
 * the specified interval.
 *
 * @since 2.3.0
 */
 
/**
 * A trigger that continuously processes streaming data, asynchronously checkpointing at
 * the specified interval.
 *
 * {{{
 *    import java.util.concurrent.TimeUnit
 *    df.writeStream.trigger(Trigger.Continuous(10, TimeUnit.SECONDS))
 * }}}
 *
 * @since 2.3.0
 */
 
/**
 * (Scala-friendly)
 * A trigger that continuously processes streaming data, asynchronously checkpointing at
 * the specified interval.
 *
 * {{{
 *    import scala.concurrent.duration._
 *    df.writeStream.trigger(Trigger.Continuous(10.seconds))
 * }}}
 * @since 2.3.0
 */
 
/**
 * A trigger that continuously processes streaming data, asynchronously checkpointing at
 * the specified interval.
 *
 * {{{
 *    df.writeStream.trigger(Trigger.Continuous(""10 seconds""))
 * }}}
 * @since 2.3.0
 */
 
","{
    return ProcessingTimeTrigger.create(intervalMs, TimeUnit.MILLISECONDS);
} 
{
    return ProcessingTimeTrigger.create(interval, timeUnit);
} 
{
    return ProcessingTimeTrigger.apply(interval);
} 
{
    return ProcessingTimeTrigger.apply(interval);
} 
{
    return OneTimeTrigger$.MODULE$;
} 
{
    return ContinuousTrigger.apply(intervalMs);
} 
{
    return ContinuousTrigger.create(interval, timeUnit);
} 
{
    return ContinuousTrigger.apply(interval);
} 
{
    return ContinuousTrigger.apply(interval);
} 
",,,,,3,103
SchemaColumnConvertNotSupportedException.java,28,26,1.0769230769230769,"
 * Exception thrown when the parquet reader find column type mismatches.
 ",,,,,"/**
 * Name of the column which cannot be converted.
 */
 
/**
 * Physical column type in the actual parquet file.
 */
 
/**
 * Logical column type in the parquet schema the parquet reader use to parse all files.
 */
 
","Field column
Field physicalType
Field logicalType
",1,72
OrcColumnarBatchReader.java,54,128,0.421875,"
 * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.
 * After creating, `initialize` and `initBatch` should be called sequentially.
 ","/**
 * Initialize ORC file reader and batch record reader.
 * Please note that `initBatch` is needed to be called after this.
 */
 
/**
 * Initialize columnar batch by setting required schema and partition information.
 * With this information, this creates ColumnarBatch with the full schema.
 *
 * @param orcSchema Schema from ORC file reader.
 * @param requiredFields All the fields that are required to return, including partition fields.
 * @param requestedDataColIds Requested column ids from orcSchema. -1 if not existed.
 * @param requestedPartitionColIds Requested column ids from partition schema. -1 if not existed.
 * @param partitionValues Values of partition columns.
 */
 
/**
 * Return true if there exists more data in the next batch. If exists, prepare the next batch
 * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.
 */
 
","{
    FileSplit fileSplit = (FileSplit) inputSplit;
    Configuration conf = taskAttemptContext.getConfiguration();
    Reader reader = OrcFile.createReader(fileSplit.getPath(), OrcFile.readerOptions(conf).maxLength(OrcConf.MAX_FILE_LENGTH.getLong(conf)).filesystem(fileSplit.getPath().getFileSystem(conf)));
    Reader.Options options = OrcInputFormat.buildOptions(conf, reader, fileSplit.getStart(), fileSplit.getLength());
    recordReader = reader.rows(options);
} 
{
    wrap = new VectorizedRowBatchWrap(orcSchema.createRowBatch(capacity));
    // `selectedInUse` should be initialized with `false`.
    assert (!wrap.batch().selectedInUse);
    assert (requiredFields.length == requestedDataColIds.length);
    assert (requiredFields.length == requestedPartitionColIds.length);
    // If a required column is also partition column, use partition value and don't read from file.
    for (int i = 0; i < requiredFields.length; i++) {
        if (requestedPartitionColIds[i] != -1) {
            requestedDataColIds[i] = -1;
        }
    }
    this.requiredFields = requiredFields;
    this.requestedDataColIds = requestedDataColIds;
    StructType resultSchema = new StructType(requiredFields);
    // Just wrap the ORC column vector instead of copying it to Spark column vector.
    orcVectorWrappers = new org.apache.spark.sql.vectorized.ColumnVector[resultSchema.length()];
    for (int i = 0; i < requiredFields.length; i++) {
        DataType dt = requiredFields[i].dataType();
        if (requestedPartitionColIds[i] != -1) {
            OnHeapColumnVector partitionCol = new OnHeapColumnVector(capacity, dt);
            ColumnVectorUtils.populate(partitionCol, partitionValues, requestedPartitionColIds[i]);
            partitionCol.setIsConstant();
            orcVectorWrappers[i] = partitionCol;
        } else {
            int colId = requestedDataColIds[i];
            // Initialize the missing columns once.
            if (colId == -1) {
                OnHeapColumnVector missingCol = new OnHeapColumnVector(capacity, dt);
                missingCol.putNulls(0, capacity);
                missingCol.setIsConstant();
                orcVectorWrappers[i] = missingCol;
            } else {
                orcVectorWrappers[i] = new OrcColumnVector(dt, wrap.batch().cols[colId]);
            }
        }
    }
    columnarBatch = new ColumnarBatch(orcVectorWrappers);
} 
{
    recordReader.nextBatch(wrap.batch());
    int batchSize = wrap.batch().size;
    if (batchSize == 0) {
        return false;
    }
    columnarBatch.setNumRows(batchSize);
    for (int i = 0; i < requiredFields.length; i++) {
        if (requestedDataColIds[i] != -1) {
            ((OrcColumnVector) orcVectorWrappers[i]).setBatchSize(batchSize);
        }
    }
    return true;
} 
",,,"/**
 * The column IDs of the physical ORC file schema which are required by this reader.
 * -1 means this required column is partition column, or it doesn't exist in the ORC file.
 * Ideally partition column should never appear in the physical file, and should only appear
 * in the directory name. However, Spark allows partition columns inside physical file,
 * but Spark will discard the values from the file, and use the partition value got from
 * directory name. The column order will be reserved though.
 */
 
","Field requestedDataColIds
",2,161
VectorizedParquetRecordReader.java,96,183,0.5245901639344263,"
 * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the
 * Parquet column APIs. This is somewhat based on parquet-mr's ColumnReader.
 *
 * TODO: handle complex types, decimal requiring more than 8 bytes, INT96. Schema mismatch.
 * All of these can be handled efficiently and easily with codegen.
 *
 * This class can either return InternalRows or ColumnarBatches. With whole stage codegen
 * enabled, this class returns ColumnarBatches which offers significant performance gains.
 * TODO: make this always return ColumnarBatches.
 ","/**
 * Implementation of RecordReader API.
 */
 
/**
 * Utility API that will read all the data in path. This circumvents the need to create Hadoop
 * objects to use this class. `columns` can contain the list of columns to project.
 */
 
/**
 * Returns the ColumnarBatch object that will be used for all rows returned by this reader.
 * This object is reused. Calling this enables the vectorized reader. This should be called
 * before any calls to nextKeyValue/nextBatch.
 */
 
/**
 * Can be called before any rows are returned to enable returning columnar batches directly.
 */
 
/**
 * Advances to the next batch of rows. Returns false if there are no more.
 */
 
","{
    super.initialize(inputSplit, taskAttemptContext);
    initializeInternal();
} 
{
    super.initialize(path, columns);
    initializeInternal();
} 
{
    if (columnarBatch == null)
        initBatch();
    return columnarBatch;
} 
{
    returnColumnarBatch = true;
} 
{
    for (WritableColumnVector vector : columnVectors) {
        vector.reset();
    }
    columnarBatch.setNumRows(0);
    if (rowsReturned >= totalRowCount)
        return false;
    checkEndOfRowGroup();
    int num = (int) Math.min((long) capacity, totalCountLoadedSoFar - rowsReturned);
    for (int i = 0; i < columnReaders.length; ++i) {
        if (columnReaders[i] == null)
            continue;
        columnReaders[i].readBatch(num, columnVectors[i]);
    }
    rowsReturned += num;
    columnarBatch.setNumRows(num);
    numBatched = num;
    batchIdx = 0;
    return true;
} 
",,,"/**
 * Batch of rows that we assemble and the current index we've returned. Every time this
 * batch is used up (batchIdx == numBatched), we populated the batch.
 */
 
/**
 * For each request column, the reader to read this column. This is NULL if this column
 * is missing from the file, in which case we populate the attribute with NULL.
 */
 
/**
 * The number of rows that have been returned.
 */
 
/**
 * The number of rows that have been reading, including the current in flight row group.
 */
 
/**
 * For each column, true if the column is missing in the file and we'll instead return NULLs.
 */
 
/**
 * The timezone that timestamp INT96 values should be converted to. Null if no conversion. Here to
 * workaround incompatibilities between different engines when writing timestamp values.
 */
 
/**
 * columnBatch object that is used for batch decoding. This is created on first use and triggers
 * batched decoding. It is not valid to interleave calls to the batched interface with the row
 * by row RecordReader APIs.
 * This is only enabled with additional flags for development. This is still a work in progress
 * and currently unsupported cases will fail with potentially difficult to diagnose errors.
 * This should be only turned on for development to work on this feature.
 *
 * When this is set, the code will branch early on in the RecordReader APIs. There is no shared
 * code between the path that uses the MR decoders and the vectorized ones.
 *
 * TODOs:
 *  - Implement v2 page formats (just make sure we create the correct decoders).
 */
 
/**
 * If true, this class returns batches instead of rows.
 */
 
/**
 * The memory mode of the columnarBatch
 */
 
","Field batchIdx
Field columnReaders
Field rowsReturned
Field totalCountLoadedSoFar
Field missingColumns
Field convertTz
Field columnarBatch
Field returnColumnarBatch
Field MEMORY_MODE
",9,554
ParquetDictionary.java,16,28,0.5714285714285714,,,,,,,,1,0
VectorizedRleValuesReader.java,77,521,0.14779270633397312,"
 * A values reader for Parquet's run-length encoded data. This is based off of the version in
 * parquet-mr with these changes:
 *  - Supports the vectorized interface.
 *  - Works on byte arrays(byte[]) instead of making byte streams.
 *
 * This encoding is used in multiple places:
 *  - Definition/Repetition levels
 *  - Dictionary ids.
 | Current decoding mode. The encoded data contains groups of either run length encoded data| (RLE) or bit packed data. Each group contains a header that indicates which group it is and| the number of values in the group.| If true, the bit width is fixed. This decoder is used in different places and this also| The RLE reader implements the vectorized decoding interface when used to decode dictionary| IDs. This is different than the above APIs that decodes definitions levels along with values.| More details here: https://github.com/Parquet/parquet-format/blob/master/Encodings.md","/**
 * Initializes the internal state for decoding ints of `bitWidth`.
 */
 
/**
 * Reads `total` ints into `c` filling them in starting at `c[rowId]`. This reader
 * reads the definition levels and then will read from `data` for the non-null values.
 * If the value is null, c will be populated with `nullValue`. Note that `nullValue` is only
 * necessary for readIntegers because we also use it to decode dictionaryIds and want to make
 * sure it always has a value in range.
 *
 * This is a batched version of this logic:
 *  if (this.readInt() == level) {
 *    c[rowId] = data.readInteger();
 *  } else {
 *    c[rowId] = null;
 *  }
 */
 
/**
 * Decoding for dictionary ids. The IDs are populated into `values` and the nullability is
 * populated into `nulls`.
 */
 
/**
 * Reads the next varint encoded int.
 */
 
/**
 * Reads the next 4 byte little endian int.
 */
 
/**
 * Reads the next byteWidth little endian int.
 */
 
/**
 * Reads the next group.
 */
 
","{
    Preconditions.checkArgument(bitWidth >= 0 && bitWidth <= 32, ""bitWidth must be >= 0 and <= 32"");
    this.bitWidth = bitWidth;
    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bitWidth);
    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bitWidth);
} 
{
    int left = total;
    while (left > 0) {
        if (this.currentCount == 0)
            this.readNextGroup();
        int n = Math.min(left, this.currentCount);
        switch(mode) {
            case RLE:
                if (currentValue == level) {
                    data.readIntegers(n, c, rowId);
                } else {
                    c.putNulls(rowId, n);
                }
                break;
            case PACKED:
                for (int i = 0; i < n; ++i) {
                    if (currentBuffer[currentBufferIdx++] == level) {
                        c.putInt(rowId + i, data.readInteger());
                    } else {
                        c.putNull(rowId + i);
                    }
                }
                break;
        }
        rowId += n;
        left -= n;
        currentCount -= n;
    }
} 
{
    int left = total;
    while (left > 0) {
        if (this.currentCount == 0)
            this.readNextGroup();
        int n = Math.min(left, this.currentCount);
        switch(mode) {
            case RLE:
                if (currentValue == level) {
                    data.readIntegers(n, values, rowId);
                } else {
                    nulls.putNulls(rowId, n);
                }
                break;
            case PACKED:
                for (int i = 0; i < n; ++i) {
                    if (currentBuffer[currentBufferIdx++] == level) {
                        values.putInt(rowId + i, data.readInteger());
                    } else {
                        nulls.putNull(rowId + i);
                    }
                }
                break;
        }
        rowId += n;
        left -= n;
        currentCount -= n;
    }
} 
{
    int value = 0;
    int shift = 0;
    int b;
    do {
        b = in.read();
        value |= (b & 0x7F) << shift;
        shift += 7;
    } while ((b & 0x80) != 0);
    return value;
} 
{
    int ch4 = in.read();
    int ch3 = in.read();
    int ch2 = in.read();
    int ch1 = in.read();
    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));
} 
{
    switch(bytesWidth) {
        case 0:
            return 0;
        case 1:
            return in.read();
        case 2:
            {
                int ch2 = in.read();
                int ch1 = in.read();
                return (ch1 << 8) + ch2;
            }
        case 3:
            {
                int ch3 = in.read();
                int ch2 = in.read();
                int ch1 = in.read();
                return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);
            }
        case 4:
            {
                return readIntLittleEndian();
            }
    }
    throw new RuntimeException(""Unreachable"");
} 
{
    try {
        int header = readUnsignedVarInt();
        this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;
        switch(mode) {
            case RLE:
                this.currentCount = header >>> 1;
                this.currentValue = readIntLittleEndianPaddedOnBitWidth();
                return;
            case PACKED:
                int numGroups = header >>> 1;
                this.currentCount = numGroups * 8;
                if (this.currentBuffer.length < this.currentCount) {
                    this.currentBuffer = new int[this.currentCount];
                }
                currentBufferIdx = 0;
                int valueIndex = 0;
                while (valueIndex < this.currentCount) {
                    // values are bit packed 8 at a time, so reading bitWidth will always work
                    ByteBuffer buffer = in.slice(bitWidth);
                    this.packer.unpack8Values(buffer, buffer.position(), this.currentBuffer, valueIndex);
                    valueIndex += 8;
                }
                return;
            default:
                throw new ParquetDecodingException(""not a valid mode "" + this.mode);
        }
    } catch (IOException e) {
        throw new ParquetDecodingException(""Failed to read from input stream"", e);
    }
} 
",,,,,9,909
ParquetLogRedirector.java,30,34,0.8823529411764706," Redirects the JUL logging for parquet-mr versions <= 1.8 to SLF4J logging using| SLF4JBridgeHandler. Parquet-mr versions >= 1.9 use SLF4J directly",,,,,,,1,147
SpecificParquetRecordReaderBase.java,64,236,0.2711864406779661,"
 * Base class for custom RecordReaders for Parquet that directly materialize to `T`.
 * This class handles computing row groups, filtering on them, setting up the column readers,
 * etc.
 * This is heavily based on parquet-mr's RecordReader.
 * TODO: move this to the parquet-mr project. There are performance benefits of doing it
 * this way, albeit at a higher cost to implement. This base class is reusable.
 |
   * Utility classes to abstract over different way to read ints with different encodings.
   * TODO: remove this layer of abstraction?
   ","/**
 * Returns the list of files at 'path' recursively. This skips files that are ignored normally
 * by MapReduce.
 */
 
/**
 * Initializes the reader to read the file at `path` with `columns` projected. If columns is
 * null, all the columns are projected.
 *
 * This is exposed for testing to be able to create this reader without the rest of the Hadoop
 * split machinery. It is not intended for general use and those not support all the
 * configurations.
 */
 
/**
 * Creates a reader for definition and repetition levels, returning an optimized one if
 * the levels are not needed.
 */
 
/**
 * @param readSupportClass to instantiate
 * @return the configured read support
 */
 
","{
    List<String> result = new ArrayList<>();
    if (path.isDirectory()) {
        for (File f : path.listFiles()) {
            result.addAll(listDirectory(f));
        }
    } else {
        char c = path.getName().charAt(0);
        if (c != '.' && c != '_') {
            result.add(path.getAbsolutePath());
        }
    }
    return result;
} 
{
    Configuration config = new Configuration();
    config.set(""spark.sql.parquet.binaryAsString"", ""false"");
    config.set(""spark.sql.parquet.int96AsTimestamp"", ""false"");
    this.file = new Path(path);
    long length = this.file.getFileSystem(config).getFileStatus(this.file).getLen();
    ParquetMetadata footer = readFooter(config, file, range(0, length));
    List<BlockMetaData> blocks = footer.getBlocks();
    this.fileSchema = footer.getFileMetaData().getSchema();
    if (columns == null) {
        this.requestedSchema = fileSchema;
    } else {
        if (columns.size() > 0) {
            Types.MessageTypeBuilder builder = Types.buildMessage();
            for (String s : columns) {
                if (!fileSchema.containsField(s)) {
                    throw new IOException(""Can only project existing columns. Unknown field: "" + s + "" File schema:\n"" + fileSchema);
                }
                builder.addFields(fileSchema.getType(s));
            }
            this.requestedSchema = builder.named(ParquetSchemaConverter.SPARK_PARQUET_SCHEMA_NAME());
        } else {
            this.requestedSchema = ParquetSchemaConverter.EMPTY_MESSAGE();
        }
    }
    this.sparkSchema = new ParquetToSparkSchemaConverter(config).convert(requestedSchema);
    this.reader = new ParquetFileReader(config, footer.getFileMetaData(), file, blocks, requestedSchema.getColumns());
    // use the blocks from the reader in case some do not match filters and will not be read
    for (BlockMetaData block : reader.getRowGroups()) {
        this.totalRowCount += block.getRowCount();
    }
} 
{
    try {
        if (maxLevel == 0)
            return new NullIntIterator();
        return new RLEIntIterator(new RunLengthBitPackingHybridDecoder(BytesUtils.getWidthFromMaxInt(maxLevel), bytes.toInputStream()));
    } catch (IOException e) {
        throw new IOException(""could not read levels in page for col "" + descriptor, e);
    }
} 
{
    try {
        return readSupportClass.getConstructor().newInstance();
    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {
        throw new BadConfigurationException(""could not instantiate read support class"", e);
    }
} 
",,,"/**
 * The total number of rows this RecordReader will eventually read. The sum of the
 * rows of all the row groups.
 */
 
","Field totalRowCount
",9,536
VectorizedColumnReader.java,99,496,0.19959677419354838,"
 * Decoder to return values from a single column.
 ","/**
 * Advances to the next value. Returns true if the value is non-null.
 */
 
/**
 * Reads `total` values from this columnReader into column.
 */
 
/**
 * Helper function to construct exception for parquet schema mismatch.
 */
 
/**
 * Reads `num` values into column, decoding the values from `dictionaryIds` and `dictionary`.
 */
 
","{
    if (valuesRead >= endOfPageValueCount) {
        if (valuesRead >= totalValueCount) {
            // How do we get here? Throw end of stream exception?
            return false;
        }
        readPage();
    }
    ++valuesRead;
    // TODO: Don't read for flat schemas
    // repetitionLevel = repetitionLevelColumn.nextInt();
    return definitionLevelColumn.nextInt() == maxDefLevel;
} 
{
    int rowId = 0;
    WritableColumnVector dictionaryIds = null;
    if (dictionary != null) {
        // SPARK-16334: We only maintain a single dictionary per row batch, so that it can be used to
        // decode all previous dictionary encoded pages if we ever encounter a non-dictionary encoded
        // page.
        dictionaryIds = column.reserveDictionaryIds(total);
    }
    while (total > 0) {
        // Compute the number of values we want to read in this page.
        int leftInPage = (int) (endOfPageValueCount - valuesRead);
        if (leftInPage == 0) {
            readPage();
            leftInPage = (int) (endOfPageValueCount - valuesRead);
        }
        int num = Math.min(total, leftInPage);
        PrimitiveType.PrimitiveTypeName typeName = descriptor.getPrimitiveType().getPrimitiveTypeName();
        if (isCurrentPageDictionaryEncoded) {
            // Read and decode dictionary ids.
            defColumn.readIntegers(num, dictionaryIds, column, rowId, maxDefLevel, (VectorizedValuesReader) dataColumn);
            // TIMESTAMP_MILLIS encoded as INT64 can't be lazily decoded as we need to post process
            // the values to add microseconds precision.
            if (column.hasDictionary() || (rowId == 0 && (typeName == PrimitiveType.PrimitiveTypeName.INT32 || (typeName == PrimitiveType.PrimitiveTypeName.INT64 && originalType != OriginalType.TIMESTAMP_MILLIS) || typeName == PrimitiveType.PrimitiveTypeName.FLOAT || typeName == PrimitiveType.PrimitiveTypeName.DOUBLE || typeName == PrimitiveType.PrimitiveTypeName.BINARY))) {
                // Column vector supports lazy decoding of dictionary values so just set the dictionary.
                // We can't do this if rowId != 0 AND the column doesn't have a dictionary (i.e. some
                // non-dictionary encoded values have already been added).
                column.setDictionary(new ParquetDictionary(dictionary));
            } else {
                decodeDictionaryIds(rowId, num, column, dictionaryIds);
            }
        } else {
            if (column.hasDictionary() && rowId != 0) {
                // This batch already has dictionary encoded values but this new page is not. The batch
                // does not support a mix of dictionary and not so we will decode the dictionary.
                decodeDictionaryIds(0, rowId, column, column.getDictionaryIds());
            }
            column.setDictionary(null);
            switch(typeName) {
                case BOOLEAN:
                    readBooleanBatch(rowId, num, column);
                    break;
                case INT32:
                    readIntBatch(rowId, num, column);
                    break;
                case INT64:
                    readLongBatch(rowId, num, column);
                    break;
                case INT96:
                    readBinaryBatch(rowId, num, column);
                    break;
                case FLOAT:
                    readFloatBatch(rowId, num, column);
                    break;
                case DOUBLE:
                    readDoubleBatch(rowId, num, column);
                    break;
                case BINARY:
                    readBinaryBatch(rowId, num, column);
                    break;
                case FIXED_LEN_BYTE_ARRAY:
                    readFixedLenByteArrayBatch(rowId, num, column, descriptor.getPrimitiveType().getTypeLength());
                    break;
                default:
                    throw new IOException(""Unsupported type: "" + typeName);
            }
        }
        valuesRead += num;
        rowId += num;
        total -= num;
    }
} 
{
    return new SchemaColumnConvertNotSupportedException(Arrays.toString(descriptor.getPath()), descriptor.getPrimitiveType().getPrimitiveTypeName().toString(), column.dataType().catalogString());
} 
{
    switch(descriptor.getPrimitiveType().getPrimitiveTypeName()) {
        case INT32:
            if (column.dataType() == DataTypes.IntegerType || DecimalType.is32BitDecimalType(column.dataType())) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        column.putInt(i, dictionary.decodeToInt(dictionaryIds.getDictId(i)));
                    }
                }
            } else if (column.dataType() == DataTypes.ByteType) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        column.putByte(i, (byte) dictionary.decodeToInt(dictionaryIds.getDictId(i)));
                    }
                }
            } else if (column.dataType() == DataTypes.ShortType) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        column.putShort(i, (short) dictionary.decodeToInt(dictionaryIds.getDictId(i)));
                    }
                }
            } else {
                throw constructConvertNotSupportedException(descriptor, column);
            }
            break;
        case INT64:
            if (column.dataType() == DataTypes.LongType || DecimalType.is64BitDecimalType(column.dataType()) || originalType == OriginalType.TIMESTAMP_MICROS) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        column.putLong(i, dictionary.decodeToLong(dictionaryIds.getDictId(i)));
                    }
                }
            } else if (originalType == OriginalType.TIMESTAMP_MILLIS) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        column.putLong(i, DateTimeUtils.fromMillis(dictionary.decodeToLong(dictionaryIds.getDictId(i))));
                    }
                }
            } else {
                throw constructConvertNotSupportedException(descriptor, column);
            }
            break;
        case FLOAT:
            for (int i = rowId; i < rowId + num; ++i) {
                if (!column.isNullAt(i)) {
                    column.putFloat(i, dictionary.decodeToFloat(dictionaryIds.getDictId(i)));
                }
            }
            break;
        case DOUBLE:
            for (int i = rowId; i < rowId + num; ++i) {
                if (!column.isNullAt(i)) {
                    column.putDouble(i, dictionary.decodeToDouble(dictionaryIds.getDictId(i)));
                }
            }
            break;
        case INT96:
            if (column.dataType() == DataTypes.TimestampType) {
                if (!shouldConvertTimestamps()) {
                    for (int i = rowId; i < rowId + num; ++i) {
                        if (!column.isNullAt(i)) {
                            Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                            column.putLong(i, ParquetRowConverter.binaryToSQLTimestamp(v));
                        }
                    }
                } else {
                    for (int i = rowId; i < rowId + num; ++i) {
                        if (!column.isNullAt(i)) {
                            Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                            long rawTime = ParquetRowConverter.binaryToSQLTimestamp(v);
                            long adjTime = DateTimeUtils.convertTz(rawTime, convertTz, UTC);
                            column.putLong(i, adjTime);
                        }
                    }
                }
            } else {
                throw constructConvertNotSupportedException(descriptor, column);
            }
            break;
        case BINARY:
            // TODO: this is incredibly inefficient as it blows up the dictionary right here. We
            // need to do this better. We should probably add the dictionary data to the ColumnVector
            // and reuse it across batches. This should mean adding a ByteArray would just update
            // the length and offset.
            for (int i = rowId; i < rowId + num; ++i) {
                if (!column.isNullAt(i)) {
                    Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                    column.putByteArray(i, v.getBytes());
                }
            }
            break;
        case FIXED_LEN_BYTE_ARRAY:
            // DecimalType written in the legacy mode
            if (DecimalType.is32BitDecimalType(column.dataType())) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                        column.putInt(i, (int) ParquetRowConverter.binaryToUnscaledLong(v));
                    }
                }
            } else if (DecimalType.is64BitDecimalType(column.dataType())) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                        column.putLong(i, ParquetRowConverter.binaryToUnscaledLong(v));
                    }
                }
            } else if (DecimalType.isByteArrayDecimalType(column.dataType())) {
                for (int i = rowId; i < rowId + num; ++i) {
                    if (!column.isNullAt(i)) {
                        Binary v = dictionary.decodeToBinary(dictionaryIds.getDictId(i));
                        column.putByteArray(i, v.getBytes());
                    }
                }
            } else {
                throw constructConvertNotSupportedException(descriptor, column);
            }
            break;
        default:
            throw new UnsupportedOperationException(""Unsupported type: "" + descriptor.getPrimitiveType().getPrimitiveTypeName());
    }
} 
",,,"/**
 * Total number of values read.
 */
 
/**
 * value that indicates the end of the current page. That is,
 * if valuesRead == endOfPageValueCount, we are at the end of the page.
 */
 
/**
 * The dictionary, if this column has dictionary encoding.
 */
 
/**
 * If true, the current page is dictionary encoded.
 */
 
/**
 * Maximum definition level for this column.
 */
 
/**
 * Repetition/Definition/Value readers.
 */
 
/**
 * Total number of values in this column (in this row group).
 */
 
/**
 * Total values in the current page.
 */
 
","Field valuesRead
Field endOfPageValueCount
Field dictionary
Field isCurrentPageDictionaryEncoded
Field maxDefLevel
Field repetitionLevelColumn
Field totalValueCount
Field pageValueCount
",1,49
VectorizedValuesReader.java,23,19,1.2105263157894737,"
 * Interface for value decoding that supports vectorized (aka batched) decoding.
 * TODO: merge this into parquet-mr.
 ",,,,,,,2,115
VectorizedPlainValuesReader.java,25,160,0.15625,"
 * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.
 ",,,,,,,1,89
ColumnDictionary.java,16,32,0.5,,,,,,,,1,0
UnsafeExternalRowSorter.java,41,176,0.23295454545454544,,"/**
 * Computes prefix for the given row. For efficiency, the returned object may be reused in
 * further calls to a given PrefixComputer.
 */
 
/**
 * Forces spills to occur every `frequency` records. Only for use in tests.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * @return the total amount of time spent sorting data (in-memory only).
 */
 
","computePrefix 
{
    assert frequency > 0 : ""Frequency must be positive"";
    testSpillFrequency = frequency;
} 
{
    return sorter.getPeakMemoryUsedBytes();
} 
{
    return sorter.getSortTimeNanos();
} 
",,,"/**
 * If positive, forces records to be spilled to disk at the given frequency (measured in numbers
 * of records). This is only intended to be used in tests.
 */
 
/**
 * Key prefix value, or the null prefix value if isNull = true. *
 */
 
/**
 * Whether the key is null.
 */
 
","Field testSpillFrequency
Field value
Field isNull
",1,0
RecordBinaryComparator.java,24,42,0.5714285714285714,,,,,,,,1,0
BufferedRowIterator.java,48,36,1.3333333333333333,"
 * An iterator interface used to pull the output from generated function for multiple operators
 * (whole stage codegen).
 ","/**
 * Returns the elapsed time since this object is created. This object represents a pipeline so
 * this is a measure of how long the pipeline has been running.
 */
 
/**
 * Initializes from array of iterators of InternalRow.
 */
 
/**
 * Append a row to currentRows.
 */
 
/**
 * Returns whether `processNext()` should stop processing next row from `input` or not.
 *
 * If it returns true, the caller should exit the loop (return from processNext()).
 */
 
/**
 * Increase the peak execution memory for current task.
 */
 
/**
 * Processes the input until have a row as output (currentRow).
 *
 * After it's called, if currentRow is still null, it means no more rows left.
 */
 
","{
    return (System.nanoTime() - startTimeNs) / (1000 * 1000);
} 
init 
{
    currentRows.add(row);
} 
{
    return !currentRows.isEmpty();
} 
{
    TaskContext.get().taskMetrics().incPeakExecutionMemory(size);
} 
processNext 
",,,,,2,119
Offset.java,27,29,0.9310344827586207,"
 * This class is an alias of {@link org.apache.spark.sql.connector.read.streaming.Offset}. It's
 * internal and deprecated. New streaming data source implementations should use data source v2 API,
 * which will be supported in the long term.
 *
 * This class will be removed in a future release.
 ",,,,,,,5,287
MutableColumnarRow.java,23,216,0.10648148148148148,"
 * A mutable version of {@link ColumnarRow}, which is used in the vectorized hash map for hash
 * aggregate, and {@link ColumnarBatch} to save object creation.
 *
 * Note that this class intentionally has a lot of duplicated code with {@link ColumnarRow}, to
 * avoid java polymorphism overhead by keeping {@link ColumnarRow} and this class final classes.
 ",,,,,,,5,347
OffHeapColumnVector.java,70,414,0.16908212560386474,"
 * Column data backed using offheap memory.
 ","/**
 * Allocates columns to store elements of each field of the schema off heap.
 * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is
 * in number of elements, not number of bytes.
 */
 
/**
 * Allocates columns to store elements of each field off heap.
 * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is
 * in number of elements, not number of bytes.
 */
 
/**
 * Returns the off heap pointer for the values buffer.
 */
 
/**
 * Returns the dictionary Id for rowId.
 * This should only be called when the ColumnVector is dictionaryIds.
 * We have this separate method for dictionaryIds as per SPARK-16928.
 */
 
","{
    return allocateColumns(capacity, schema.fields());
} 
{
    OffHeapColumnVector[] vectors = new OffHeapColumnVector[fields.length];
    for (int i = 0; i < fields.length; i++) {
        vectors[i] = new OffHeapColumnVector(capacity, fields[i].dataType());
    }
    return vectors;
} 
{
    return data;
} 
{
    assert (dictionary == null) : ""A ColumnVector dictionary should not have a dictionary for itself."";
    return Platform.getInt(null, data + 4L * rowId);
} 
",,,,,1,43
OnHeapColumnVector.java,73,423,0.17257683215130024,"
 * A column backed by an in memory JVM array. This stores the NULLs as a byte per value
 * and a java array for the values.
 ","/**
 * Allocates columns to store elements of each field of the schema on heap.
 * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is
 * in number of elements, not number of bytes.
 */
 
/**
 * Allocates columns to store elements of each field on heap.
 * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is
 * in number of elements, not number of bytes.
 */
 
/**
 * Returns the dictionary Id for rowId.
 * This should only be called when the ColumnVector is dictionaryIds.
 * We have this separate method for dictionaryIds as per SPARK-16928.
 */
 
","{
    return allocateColumns(capacity, schema.fields());
} 
{
    OnHeapColumnVector[] vectors = new OnHeapColumnVector[fields.length];
    for (int i = 0; i < fields.length; i++) {
        vectors[i] = new OnHeapColumnVector(capacity, fields[i].dataType());
    }
    return vectors;
} 
{
    assert (dictionary == null) : ""A ColumnVector dictionary should not have a dictionary for itself."";
    return intData[rowId];
} 
",,,,,2,121
WritableColumnVector.java,223,423,0.5271867612293144,"
 * This class adds write APIs to ColumnVector.
 * It supports all the types and contains put APIs as well as their batched versions.
 * The batched versions are preferable whenever possible.
 *
 * Capacity: The data stored is dense but the arrays are not fixed capacity. It is the
 * responsibility of the caller to call reserve() to ensure there is enough room before adding
 * elements. This means that the put() APIs do not check as in common cases (i.e. flat schemas),
 * the lengths are known up front.
 *
 * A WritableColumnVector should be considered immutable once originally created. In other words,
 * it is not valid to call put APIs after reads until reset() is called.
 *
 * WritableColumnVector are intended to be reused.
 ","/**
 * Resets this column for writing. The currently stored values are no longer accessible.
 */
 
/**
 * Returns the dictionary Id for rowId.
 *
 * This should only be called when this `WritableColumnVector` represents dictionaryIds.
 * We have this separate method for dictionaryIds as per SPARK-16928.
 */
 
/**
 * Returns true if this column has a dictionary.
 */
 
/**
 * Returns the underlying integer column for ids of dictionary.
 */
 
/**
 * Update the dictionary.
 */
 
/**
 * Reserve a integer column for ids of dictionary.
 */
 
/**
 * Ensures that there is enough storage to store capacity elements. That is, the put() APIs
 * must work for all rowIds < capacity.
 */
 
/**
 * Sets null/not null to the value at rowId.
 */
 
/**
 * Sets null/not null to the values at [rowId, rowId + count).
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 2]) to [rowId, rowId + count)
 * The data in src must be 2-byte platform native endian shorts.
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 4]) to [rowId, rowId + count)
 * The data in src must be 4-byte platform native endian ints.
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 4]) to [rowId, rowId + count)
 * The data in src must be 4-byte little endian ints.
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 8]) to [rowId, rowId + count)
 * The data in src must be 8-byte platform native endian longs.
 */
 
/**
 * Sets values from [src + srcIndex, src + srcIndex + count * 8) to [rowId, rowId + count)
 * The data in src must be 8-byte little endian longs.
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 4]) to [rowId, rowId + count)
 * The data in src must be ieee formatted floats in platform native endian.
 */
 
/**
 * Sets `value` to the value at rowId.
 */
 
/**
 * Sets value to [rowId, rowId + count).
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)
 */
 
/**
 * Sets values from [src[srcIndex], src[srcIndex + count * 8]) to [rowId, rowId + count)
 * The data in src must be ieee formatted doubles in platform native endian.
 */
 
/**
 * Puts a byte array that already exists in this column.
 */
 
/**
 * Sets values from [value + offset, value + offset + count) to the values at rowId.
 */
 
/**
 * Gets the values of bytes from [rowId, rowId + count), as a UTF8String.
 * This method is similar to {@link ColumnVector#getBytes(int, int)}, but can save data copy as
 * UTF8String is used as a pointer.
 */
 
/**
 * Append APIs. These APIs all behave similarly and will append data to the current vector.  It
 * is not valid to mix the put and append APIs. The append APIs are slower and should only be
 * used if the sizes are not known up front.
 * In all these cases, the return value is the rowId for the first appended element.
 */
 
/**
 * Appends a NULL struct. This *has* to be used for structs instead of appendNull() as this
 * recursively appends a NULL to its children.
 * We don't have this logic as the general appendNull implementation to optimize the more
 * common non-struct case.
 */
 
/**
 * Returns the elements appended.
 */
 
/**
 * Marks this column as being constant.
 */
 
/**
 * Reserve a new column.
 */
 
","{
    if (isConstant)
        return;
    if (childColumns != null) {
        for (ColumnVector c : childColumns) {
            ((WritableColumnVector) c).reset();
        }
    }
    elementsAppended = 0;
    if (numNulls > 0) {
        putNotNulls(0, capacity);
        numNulls = 0;
    }
} 
getDictId 
{
    return this.dictionary != null;
} 
{
    return dictionaryIds;
} 
{
    this.dictionary = dictionary;
} 
{
    if (dictionaryIds == null) {
        dictionaryIds = reserveNewColumn(capacity, DataTypes.IntegerType);
    } else {
        dictionaryIds.reset();
        dictionaryIds.reserve(capacity);
    }
    return dictionaryIds;
} 
reserveInternal 
putNotNull 
putNulls 
putBoolean 
putBooleans 
putByte 
putBytes 
putBytes 
putShort 
putShorts 
putShorts 
putShorts 
putInt 
putInts 
putInts 
putInts 
putIntsLittleEndian 
putLong 
putLongs 
putLongs 
putLongs 
putLongsLittleEndian 
putFloat 
putFloats 
putFloats 
putFloats 
putDouble 
putDoubles 
putDoubles 
putDoubles 
putArray 
putByteArray 
getBytesAsUTF8String 
{
    // Use appendStruct()
    assert (!(dataType() instanceof StructType));
    reserve(elementsAppended + 1);
    putNull(elementsAppended);
    return elementsAppended++;
} 
{
    if (isNull) {
        // This is the same as appendNull but without the assertion for struct types
        reserve(elementsAppended + 1);
        putNull(elementsAppended);
        elementsAppended++;
        for (WritableColumnVector c : childColumns) {
            if (c.type instanceof StructType) {
                c.appendStruct(true);
            } else {
                c.appendNull();
            }
        }
    } else {
        appendNotNull();
    }
    return elementsAppended;
} 
{
    return elementsAppended;
} 
{
    isConstant = true;
} 
reserveNewColumn 
","/**
 * Sets up the common state and also handles creating the child columns if this is a nested
 * type.
 */
 
","{
    super(type);
    this.capacity = capacity;
    if (isArray()) {
        DataType childType;
        int childCapacity = capacity;
        if (type instanceof ArrayType) {
            childType = ((ArrayType) type).elementType();
        } else {
            childType = DataTypes.ByteType;
            childCapacity *= DEFAULT_ARRAY_LENGTH;
        }
        this.childColumns = new WritableColumnVector[1];
        this.childColumns[0] = reserveNewColumn(childCapacity, childType);
    } else if (type instanceof StructType) {
        StructType st = (StructType) type;
        this.childColumns = new WritableColumnVector[st.fields().length];
        for (int i = 0; i < childColumns.length; ++i) {
            this.childColumns[i] = reserveNewColumn(capacity, st.fields()[i].dataType());
        }
    } else if (type instanceof MapType) {
        MapType mapType = (MapType) type;
        this.childColumns = new WritableColumnVector[2];
        this.childColumns[0] = reserveNewColumn(capacity, mapType.keyType());
        this.childColumns[1] = reserveNewColumn(capacity, mapType.valueType());
    } else if (type instanceof CalendarIntervalType) {
        // Two columns. Months as int. Microseconds as Long.
        this.childColumns = new WritableColumnVector[2];
        this.childColumns[0] = reserveNewColumn(capacity, DataTypes.IntegerType);
        this.childColumns[1] = reserveNewColumn(capacity, DataTypes.LongType);
    } else {
        this.childColumns = null;
    }
} 
","/**
 * The Dictionary for this column.
 *
 * If it's not null, will be used to decode the value in getXXX().
 */
 
/**
 * Reusable column for ids of dictionary.
 */
 
/**
 * Maximum number of rows that can be stored in this column.
 */
 
/**
 * Upper limit for the maximum capacity for this column.
 */
 
/**
 * Number of nulls in this column. This is an optimization for the reader, to skip NULL checks.
 */
 
/**
 * True if this column's values are fixed. This means the column values never change, even
 * across resets.
 */
 
/**
 * Default size of each array length value. This grows as necessary.
 */
 
/**
 * Current write cursor (row index) when appending data.
 */
 
/**
 * If this is a nested type (array or struct), the column for the child data.
 */
 
","Field dictionary
Field dictionaryIds
Field capacity
Field MAX_CAPACITY
Field numNulls
Field isConstant
Field DEFAULT_ARRAY_LENGTH
Field elementsAppended
Field childColumns
",13,711
Dictionary.java,16,32,0.5,"
 * The interface for dictionary in ColumnVector to decode dictionary encoded values.
 ",,,,,,,1,84
AggregateHashMap.java,33,62,0.532258064516129,"
 * This is an illustrative implementation of an append-only single-key/single value aggregate hash
 * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates
 * (and fall back to the `BytesToBytesMap` if a given key isn't found). This can be potentially
 * 'codegened' in HashAggregate to speed up aggregates w/ key.
 *
 * It is backed by a power-of-2-sized array for index lookups and a columnar batch that stores the
 * key-value pairs. The index lookups in the array rely on linear probing (with a small number of
 * maximum tries) and use an inexpensive hash function which makes it really efficient for a
 * majority of lookups. However, using linear probing and an inexpensive hash function also makes it
 * less robust as compared to the `BytesToBytesMap` (especially for a large number of keys or even
 * for certain distribution of keys) and requires us to fall back on the latter for correctness.
 ",,,,,,,11,925
ColumnVectorUtils.java,32,187,0.1711229946524064,"
 * Utilities to help manipulate data associate with ColumnVectors. These should be used mostly
 * for debugging or other non-performance critical paths.
 * These utilities are mostly used to convert ColumnVectors into other formats.
 ","/**
 * Populates the entire `col` with `row[fieldIdx]`
 */
 
/**
 * Returns the array data as the java primitive array.
 * For example, an array of IntegerType will return an int[].
 * Throws exceptions for unhandled schemas.
 */
 
/**
 * Converts an iterator of rows into a single ColumnBatch.
 */
 
","{
    int capacity = col.capacity;
    DataType t = col.dataType();
    if (row.isNullAt(fieldIdx)) {
        col.putNulls(0, capacity);
    } else {
        if (t == DataTypes.BooleanType) {
            col.putBooleans(0, capacity, row.getBoolean(fieldIdx));
        } else if (t == DataTypes.ByteType) {
            col.putBytes(0, capacity, row.getByte(fieldIdx));
        } else if (t == DataTypes.ShortType) {
            col.putShorts(0, capacity, row.getShort(fieldIdx));
        } else if (t == DataTypes.IntegerType) {
            col.putInts(0, capacity, row.getInt(fieldIdx));
        } else if (t == DataTypes.LongType) {
            col.putLongs(0, capacity, row.getLong(fieldIdx));
        } else if (t == DataTypes.FloatType) {
            col.putFloats(0, capacity, row.getFloat(fieldIdx));
        } else if (t == DataTypes.DoubleType) {
            col.putDoubles(0, capacity, row.getDouble(fieldIdx));
        } else if (t == DataTypes.StringType) {
            UTF8String v = row.getUTF8String(fieldIdx);
            byte[] bytes = v.getBytes();
            for (int i = 0; i < capacity; i++) {
                col.putByteArray(i, bytes);
            }
        } else if (t instanceof DecimalType) {
            DecimalType dt = (DecimalType) t;
            Decimal d = row.getDecimal(fieldIdx, dt.precision(), dt.scale());
            if (dt.precision() <= Decimal.MAX_INT_DIGITS()) {
                col.putInts(0, capacity, (int) d.toUnscaledLong());
            } else if (dt.precision() <= Decimal.MAX_LONG_DIGITS()) {
                col.putLongs(0, capacity, d.toUnscaledLong());
            } else {
                final BigInteger integer = d.toJavaBigDecimal().unscaledValue();
                byte[] bytes = integer.toByteArray();
                for (int i = 0; i < capacity; i++) {
                    col.putByteArray(i, bytes, 0, bytes.length);
                }
            }
        } else if (t instanceof CalendarIntervalType) {
            CalendarInterval c = (CalendarInterval) row.get(fieldIdx, t);
            col.getChild(0).putInts(0, capacity, c.months);
            col.getChild(1).putLongs(0, capacity, c.microseconds);
        } else if (t instanceof DateType) {
            col.putInts(0, capacity, row.getInt(fieldIdx));
        } else if (t instanceof TimestampType) {
            col.putLongs(0, capacity, row.getLong(fieldIdx));
        }
    }
} 
{
    for (int i = 0; i < array.numElements(); i++) {
        if (array.isNullAt(i)) {
            throw new RuntimeException(""Cannot handle NULL values."");
        }
    }
    return array.toIntArray();
} 
{
    int capacity = 4 * 1024;
    WritableColumnVector[] columnVectors;
    if (memMode == MemoryMode.OFF_HEAP) {
        columnVectors = OffHeapColumnVector.allocateColumns(capacity, schema);
    } else {
        columnVectors = OnHeapColumnVector.allocateColumns(capacity, schema);
    }
    int n = 0;
    while (row.hasNext()) {
        Row r = row.next();
        for (int i = 0; i < schema.fields().length; i++) {
            appendValue(columnVectors[i], schema.fields()[i].dataType(), r, i);
        }
        n++;
    }
    ColumnarBatch batch = new ColumnarBatch(columnVectors);
    batch.setNumRows(n);
    return batch;
} 
",,,,,3,228
UnsafeKVExternalSorter.java,79,210,0.3761904761904762,"
 * A class for performing external sorting on key-value records. Both key and value are UnsafeRows.
 *
 * Note that this class allows optionally passing in a {@link BytesToBytesMap} directly in order
 * to perform in-place sorting of records in the map.
 ","/**
 * Inserts a key-value record into the sorter. If the sorter no longer has enough memory to hold
 * the record, the sorter sorts the existing records in-memory, writes them out as partially
 * sorted runs, and then reallocates memory to hold the new record.
 */
 
/**
 * Merges another UnsafeKVExternalSorter into `this`, the other one will be emptied.
 *
 * @throws IOException
 */
 
/**
 * Returns a sorted iterator. It is the caller's responsibility to call `cleanupResources()`
 * after consuming this iterator.
 */
 
/**
 * Return the total number of bytes that has been spilled into disk so far.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * Marks the current page as no-more-space-available, and as a result, either allocate a
 * new page or spill when we see the next record.
 */
 
/**
 * Frees this sorter's in-memory data structures and cleans up its spill files.
 */
 
","{
    final UnsafeExternalRowSorter.PrefixComputer.Prefix prefix = prefixComputer.computePrefix(key);
    sorter.insertKVRecord(key.getBaseObject(), key.getBaseOffset(), key.getSizeInBytes(), value.getBaseObject(), value.getBaseOffset(), value.getSizeInBytes(), prefix.value, prefix.isNull);
} 
{
    sorter.merge(other.sorter);
} 
{
    try {
        final UnsafeSorterIterator underlying = sorter.getSortedIterator();
        if (!underlying.hasNext()) {
            // Since we won't ever call next() on an empty iterator, we need to clean up resources
            // here in order to prevent memory leaks.
            cleanupResources();
        }
        return new KVSorterIterator(underlying);
    } catch (IOException e) {
        cleanupResources();
        throw e;
    }
} 
{
    return sorter.getSpillSize();
} 
{
    return sorter.getPeakMemoryUsedBytes();
} 
{
    sorter.closeCurrentPage();
} 
{
    sorter.cleanupResources();
} 
",,,,,4,247
UnsafeFixedWidthAggregationMap.java,84,139,0.60431654676259,"
 * Unsafe-based HashMap for performing aggregations where the aggregated values are fixed-width.
 *
 * This map supports a maximum of 2 billion keys.
 ","/**
 * @return true if UnsafeFixedWidthAggregationMap supports aggregation buffers with the given
 *         schema, false otherwise.
 */
 
/**
 * Return the aggregation buffer for the current group. For efficiency, all calls to this method
 * return the same object. If additional memory could not be allocated, then this method will
 * signal an error by returning null.
 */
 
/**
 * Returns an iterator over the keys and values in this map. This uses destructive iterator of
 * BytesToBytesMap. So it is illegal to call any other method on this map after `iterator()` has
 * been called.
 *
 * For efficiency, each call returns the same object.
 */
 
/**
 * Return the peak memory used so far, in bytes.
 */
 
/**
 * Free the memory associated with this map. This is idempotent and can be called multiple times.
 */
 
/**
 * Gets the average bucket list iterations per lookup in the underlying `BytesToBytesMap`.
 */
 
/**
 * Sorts the map's records in place, spill them to disk, and returns an [[UnsafeKVExternalSorter]]
 *
 * Note that the map will be reset for inserting new records, and the returned sorter can NOT be
 * used to insert records.
 */
 
","{
    for (StructField field : schema.fields()) {
        if (!UnsafeRow.isMutable(field.dataType())) {
            return false;
        }
    }
    return true;
} 
{
    final UnsafeRow unsafeGroupingKeyRow = this.groupingKeyProjection.apply(groupingKey);
    return getAggregationBufferFromUnsafeRow(unsafeGroupingKeyRow);
} 
{
    return new KVIterator<UnsafeRow, UnsafeRow>() {

        private final BytesToBytesMap.MapIterator mapLocationIterator = map.destructiveIterator();

        private final UnsafeRow key = new UnsafeRow(groupingKeySchema.length());

        private final UnsafeRow value = new UnsafeRow(aggregationBufferSchema.length());

        @Override
        public boolean next() {
            if (mapLocationIterator.hasNext()) {
                final BytesToBytesMap.Location loc = mapLocationIterator.next();
                key.pointTo(loc.getKeyBase(), loc.getKeyOffset(), loc.getKeyLength());
                value.pointTo(loc.getValueBase(), loc.getValueOffset(), loc.getValueLength());
                return true;
            } else {
                return false;
            }
        }

        @Override
        public UnsafeRow getKey() {
            return key;
        }

        @Override
        public UnsafeRow getValue() {
            return value;
        }

        @Override
        public void close() {
        // Do nothing.
        }
    };
} 
{
    return map.getPeakMemoryUsedBytes();
} 
{
    map.free();
} 
{
    return map.getAvgHashProbeBucketListIterations();
} 
{
    return new UnsafeKVExternalSorter(groupingKeySchema, aggregationBufferSchema, SparkEnv.get().blockManager(), SparkEnv.get().serializerManager(), map.getPageSizeBytes(), (int) SparkEnv.get().conf().get(package$.MODULE$.SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD()), map);
} 
","/**
 * Create a new UnsafeFixedWidthAggregationMap.
 *
 * @param emptyAggregationBuffer the default value for new keys (a ""zero"" of the agg. function)
 * @param aggregationBufferSchema the schema of the aggregation buffer, used for row conversion.
 * @param groupingKeySchema the schema of the grouping key, used for row conversion.
 * @param taskContext the current task context.
 * @param initialCapacity the initial capacity of the map (a sizing hint to avoid re-hashing).
 * @param pageSizeBytes the data page size, in bytes; limits the maximum record size.
 */
 
","{
    this.aggregationBufferSchema = aggregationBufferSchema;
    this.currentAggregationBuffer = new UnsafeRow(aggregationBufferSchema.length());
    this.groupingKeyProjection = UnsafeProjection.create(groupingKeySchema);
    this.groupingKeySchema = groupingKeySchema;
    this.map = new BytesToBytesMap(taskContext.taskMemoryManager(), initialCapacity, pageSizeBytes);
    // Initialize the buffer for aggregation value
    final UnsafeProjection valueProjection = UnsafeProjection.create(aggregationBufferSchema);
    this.emptyAggregationBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();
    // Register a cleanup task with TaskContext to ensure that memory is guaranteed to be freed at
    // the end of the task. This is necessary to avoid memory leaks in when the downstream operator
    // does not fully consume the aggregation map's output (e.g. aggregate followed by limit).
    taskContext.addTaskCompletionListener(context -> {
        free();
    });
} 
","/**
 * An empty aggregation buffer, encoded in UnsafeRow format. When inserting a new key into the
 * map, we copy this buffer and use it as the value.
 */
 
/**
 * Encodes grouping keys as UnsafeRows.
 */
 
/**
 * A hashmap which maps from opaque bytearray keys to bytearray values.
 */
 
/**
 * Re-used pointer to the current aggregation buffer
 */
 
","Field emptyAggregationBuffer
Field groupingKeyProjection
Field map
Field currentAggregationBuffer
",3,145
typed.java,45,22,2.0454545454545454,"
 * Type-safe functions available for {@link org.apache.spark.sql.Dataset} operations in Java.
 *
 * Scala users should use {@link org.apache.spark.sql.expressions.scalalang.typed}.
 *
 * @since 2.0.0
 * @deprecated As of release 3.0.0, please use the untyped builtin aggregate functions.
 ","/**
 * Average aggregate function.
 *
 * @since 2.0.0
 */
 
/**
 * Count aggregate function.
 *
 * @since 2.0.0
 */
 
/**
 * Sum aggregate function for floating point (double) type.
 *
 * @since 2.0.0
 */
 
/**
 * Sum aggregate function for integral (long, i.e. 64 bit integer) type.
 *
 * @since 2.0.0
 */
 
","{
    return new TypedAverage<T>(f).toColumnJava();
} 
{
    return new TypedCount<T>(f).toColumnJava();
} 
{
    return new TypedSumDouble<T>(f).toColumnJava();
} 
{
    return new TypedSumLong<T>(f).toColumnJava();
} 
",,,,,6,277
UDF4.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 4 arguments.
 ",,,,,,,1,40
UDF10.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 10 arguments.
 ",,,,,,,1,41
UDF8.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 8 arguments.
 ",,,,,,,1,40
UDF9.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 9 arguments.
 ",,,,,,,1,40
UDF11.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 11 arguments.
 ",,,,,,,1,41
UDF5.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 5 arguments.
 ",,,,,,,1,40
UDF20.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 20 arguments.
 ",,,,,,,1,41
UDF2.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 2 arguments.
 ",,,,,,,1,40
UDF16.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 16 arguments.
 ",,,,,,,1,41
UDF17.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 17 arguments.
 ",,,,,,,1,41
UDF3.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 3 arguments.
 ",,,,,,,1,40
UDF21.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 21 arguments.
 ",,,,,,,1,41
UDF18.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 18 arguments.
 ",,,,,,,1,41
UDF22.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 22 arguments.
 ",,,,,,,1,41
UDF14.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 14 arguments.
 ",,,,,,,1,41
UDF0.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 0 arguments.
 ",,,,,,,1,40
UDF1.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 1 arguments.
 ",,,,,,,1,40
UDF15.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 15 arguments.
 ",,,,,,,1,41
UDF19.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 19 arguments.
 ",,,,,,,1,41
UDF12.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 12 arguments.
 ",,,,,,,1,41
UDF6.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 6 arguments.
 ",,,,,,,1,40
UDF7.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 7 arguments.
 ",,,,,,,1,40
UDF13.java,19,7,2.7142857142857144,"
 * A Spark SQL UDF that has 13 arguments.
 ",,,,,,,1,41
SaveMode.java,47,9,5.222222222222222,"
 * SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.
 *
 * @since 1.3.0
 ",,,,,,,3,107
HiveHasherSuite.java,22,84,0.2619047619047619,,,,,,,,1,0
XXH64Suite.java,28,128,0.21875,"
 * Test the XXH64 function.
 * <p/>
 * Test constants were taken from the original implementation and the airlift/slice implementation.
 ",,,,,,,3,131
RowBasedKeyValueBatchSuite.java,23,333,0.06906906906906907,,,,,,,,1,0
JavaOutputModeSuite.java,16,12,1.3333333333333333,,,,,,,,1,0
JavaGroupStateTimeoutSuite.java,16,13,1.2307692307692308,,,,,,,,1,0
CatalogLoadingSuite.java,17,157,0.10828025477707007,,,,,,,,1,0
SQLUserDefinedType.java,26,9,2.888888888888889,"
 * ::DeveloperApi::
 * A user-defined type which can be automatically recognized by a SQLContext and registered.
 * WARNING: UDTs are currently only supported from Scala.
 | TODO: Should I used @Documented ?",,,,,,,4,201
DataTypes.java,99,97,1.0206185567010309,"
 * To get/create specific data type, users should use singleton objects and factory methods
 * provided by this class.
 *
 * @since 1.3.0
 ","/**
 * Creates an ArrayType by specifying the data type of elements ({@code elementType}).
 * The field of {@code containsNull} is set to {@code true}.
 */
 
/**
 * Creates an ArrayType by specifying the data type of elements ({@code elementType}) and
 * whether the array contains null values ({@code containsNull}).
 */
 
/**
 * Creates a DecimalType by specifying the precision and scale.
 */
 
/**
 * Creates a DecimalType with default precision and scale, which are 10 and 0.
 */
 
/**
 * Creates a MapType by specifying the data type of keys ({@code keyType}) and values
 * ({@code keyType}). The field of {@code valueContainsNull} is set to {@code true}.
 */
 
/**
 * Creates a MapType by specifying the data type of keys ({@code keyType}), the data type of
 * values ({@code keyType}), and whether values contain any null value
 * ({@code valueContainsNull}).
 */
 
/**
 * Creates a StructField by specifying the name ({@code name}), data type ({@code dataType}) and
 * whether values of this field can be null values ({@code nullable}).
 */
 
/**
 * Creates a StructField with empty metadata.
 *
 * @see #createStructField(String, DataType, boolean, Metadata)
 */
 
/**
 * Creates a StructType with the given list of StructFields ({@code fields}).
 */
 
/**
 * Creates a StructType with the given StructField array ({@code fields}).
 */
 
","{
    if (elementType == null) {
        throw new IllegalArgumentException(""elementType should not be null."");
    }
    return new ArrayType(elementType, true);
} 
{
    if (elementType == null) {
        throw new IllegalArgumentException(""elementType should not be null."");
    }
    return new ArrayType(elementType, containsNull);
} 
{
    return DecimalType$.MODULE$.apply(precision, scale);
} 
{
    return DecimalType$.MODULE$.USER_DEFAULT();
} 
{
    if (keyType == null) {
        throw new IllegalArgumentException(""keyType should not be null."");
    }
    if (valueType == null) {
        throw new IllegalArgumentException(""valueType should not be null."");
    }
    return new MapType(keyType, valueType, true);
} 
{
    if (keyType == null) {
        throw new IllegalArgumentException(""keyType should not be null."");
    }
    if (valueType == null) {
        throw new IllegalArgumentException(""valueType should not be null."");
    }
    return new MapType(keyType, valueType, valueContainsNull);
} 
{
    if (name == null) {
        throw new IllegalArgumentException(""name should not be null."");
    }
    if (dataType == null) {
        throw new IllegalArgumentException(""dataType should not be null."");
    }
    if (metadata == null) {
        throw new IllegalArgumentException(""metadata should not be null."");
    }
    return new StructField(name, dataType, nullable, metadata);
} 
{
    return createStructField(name, dataType, nullable, (new MetadataBuilder()).build());
} 
{
    return createStructType(fields.toArray(new StructField[fields.size()]));
} 
{
    if (fields == null) {
        throw new IllegalArgumentException(""fields should not be null."");
    }
    Set<String> distinctNames = new HashSet<>();
    for (StructField field : fields) {
        if (field == null) {
            throw new IllegalArgumentException(""fields should not contain any null."");
        }
        distinctNames.add(field.name());
    }
    if (distinctNames.size() != fields.length) {
        throw new IllegalArgumentException(""fields should have distinct names."");
    }
    return StructType$.MODULE$.apply(fields);
} 
",,,"/**
 * Gets the StringType object.
 */
 
/**
 * Gets the BinaryType object.
 */
 
/**
 * Gets the BooleanType object.
 */
 
/**
 * Gets the DateType object.
 */
 
/**
 * Gets the TimestampType object.
 */
 
/**
 * Gets the CalendarIntervalType object.
 */
 
/**
 * Gets the DoubleType object.
 */
 
/**
 * Gets the FloatType object.
 */
 
/**
 * Gets the ByteType object.
 */
 
/**
 * Gets the IntegerType object.
 */
 
/**
 * Gets the LongType object.
 */
 
/**
 * Gets the ShortType object.
 */
 
/**
 * Gets the NullType object.
 */
 
","Field StringType
Field BinaryType
Field BooleanType
Field DateType
Field TimestampType
Field CalendarIntervalType
Field DoubleType
Field FloatType
Field ByteType
Field IntegerType
Field LongType
Field ShortType
Field NullType
",4,131
UnsafeRow.java,119,444,0.268018018018018,"
 * An Unsafe implementation of Row which is backed by raw memory instead of Java objects.
 *
 * Each tuple has three parts: [null bit set] [values] [variable length portion]
 *
 * The bit set is used for null tracking and is aligned to 8-byte word boundaries.  It stores
 * one bit per field.
 *
 * In the `values` region, we store one 8-byte word per field. For fields that hold fixed-length
 * primitive types, such as long, double, or int, we store the value directly in the word. For
 * fields with non-primitive or variable-length values, we store a relative offset (w.r.t. the
 * base address of the row) that points to the beginning of the variable-length field, and length
 * (they are combined into a long).
 *
 * Instances of `UnsafeRow` act as pointers to row data stored in this format.
 ","/**
 * Update this UnsafeRow to point to different backing data.
 *
 * @param baseObject the base object
 * @param baseOffset the offset within the base object
 * @param sizeInBytes the size of this row's backing data, in bytes
 */
 
/**
 * Update this UnsafeRow to point to the underlying byte array.
 *
 * @param buf byte array to point to
 * @param sizeInBytes the number of bytes valid in the byte array
 */
 
/**
 * Updates the decimal column.
 *
 * Note: In order to support update a decimal with precision > 18, CAN NOT call
 * setNullAt() for this column.
 */
 
/**
 * Copies this row, returning a self-contained UnsafeRow that stores its data in an internal
 * byte array rather than referencing data stored in a data page.
 */
 
/**
 * Creates an empty UnsafeRow from a byte array with specified numBytes and numFields.
 * The returned row is invalid until we call copyFrom on it.
 */
 
/**
 * Copies the input UnsafeRow to this UnsafeRow, and resize the underlying byte[] when the
 * input row is larger than this row.
 */
 
/**
 * Write this UnsafeRow's underlying bytes to the given OutputStream.
 *
 * @param out the stream to write to.
 * @param writeBuffer a byte array for buffering chunks of off-heap data while writing to the
 *                    output stream. If this row is backed by an on-heap byte array, then this
 *                    buffer will not be used and may be null.
 */
 
/**
 * Returns the underlying bytes for this UnsafeRow.
 */
 
/**
 * Writes the content of this row into a memory address, identified by an object and an offset.
 * The target memory address must already been allocated, and have enough space to hold all the
 * bytes in this string.
 */
 
/**
 * Write the bytes of var-length field into ByteBuffer
 *
 * Note: only work with HeapByteBuffer
 */
 
","{
    assert numFields >= 0 : ""numFields ("" + numFields + "") should >= 0"";
    assert sizeInBytes % 8 == 0 : ""sizeInBytes ("" + sizeInBytes + "") should be a multiple of 8"";
    this.baseObject = baseObject;
    this.baseOffset = baseOffset;
    this.sizeInBytes = sizeInBytes;
} 
{
    pointTo(buf, Platform.BYTE_ARRAY_OFFSET, sizeInBytes);
} 
{
    assertIndexIsValid(ordinal);
    if (precision <= Decimal.MAX_LONG_DIGITS()) {
        // compact format
        if (value == null) {
            setNullAt(ordinal);
        } else {
            setLong(ordinal, value.toUnscaledLong());
        }
    } else {
        // fixed length
        long cursor = getLong(ordinal) >>> 32;
        assert cursor > 0 : ""invalid cursor "" + cursor;
        // zero-out the bytes
        Platform.putLong(baseObject, baseOffset + cursor, 0L);
        Platform.putLong(baseObject, baseOffset + cursor + 8, 0L);
        if (value == null) {
            setNullAt(ordinal);
            // keep the offset for future update
            Platform.putLong(baseObject, getFieldOffset(ordinal), cursor << 32);
        } else {
            final BigInteger integer = value.toJavaBigDecimal().unscaledValue();
            byte[] bytes = integer.toByteArray();
            assert (bytes.length <= 16);
            // Write the bytes to the variable length portion.
            Platform.copyMemory(bytes, Platform.BYTE_ARRAY_OFFSET, baseObject, baseOffset + cursor, bytes.length);
            setLong(ordinal, (cursor << 32) | ((long) bytes.length));
        }
    }
} 
{
    UnsafeRow rowCopy = new UnsafeRow(numFields);
    final byte[] rowDataCopy = new byte[sizeInBytes];
    Platform.copyMemory(baseObject, baseOffset, rowDataCopy, Platform.BYTE_ARRAY_OFFSET, sizeInBytes);
    rowCopy.pointTo(rowDataCopy, Platform.BYTE_ARRAY_OFFSET, sizeInBytes);
    return rowCopy;
} 
{
    final UnsafeRow row = new UnsafeRow(numFields);
    row.pointTo(new byte[numBytes], numBytes);
    return row;
} 
{
    // copyFrom is only available for UnsafeRow created from byte array.
    assert (baseObject instanceof byte[]) && baseOffset == Platform.BYTE_ARRAY_OFFSET;
    if (row.sizeInBytes > this.sizeInBytes) {
        // resize the underlying byte[] if it's not large enough.
        this.baseObject = new byte[row.sizeInBytes];
    }
    Platform.copyMemory(row.baseObject, row.baseOffset, this.baseObject, this.baseOffset, row.sizeInBytes);
    // update the sizeInBytes.
    this.sizeInBytes = row.sizeInBytes;
} 
{
    if (baseObject instanceof byte[]) {
        int offsetInByteArray = (int) (baseOffset - Platform.BYTE_ARRAY_OFFSET);
        out.write((byte[]) baseObject, offsetInByteArray, sizeInBytes);
    } else {
        int dataRemaining = sizeInBytes;
        long rowReadPosition = baseOffset;
        while (dataRemaining > 0) {
            int toTransfer = Math.min(writeBuffer.length, dataRemaining);
            Platform.copyMemory(baseObject, rowReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);
            out.write(writeBuffer, 0, toTransfer);
            rowReadPosition += toTransfer;
            dataRemaining -= toTransfer;
        }
    }
} 
{
    return UnsafeDataUtils.getBytes(baseObject, baseOffset, sizeInBytes);
} 
{
    Platform.copyMemory(baseObject, baseOffset, target, targetOffset, sizeInBytes);
} 
{
    final long offsetAndSize = getLong(ordinal);
    final int offset = (int) (offsetAndSize >> 32);
    final int size = (int) offsetAndSize;
    buffer.putInt(size);
    int pos = buffer.position();
    buffer.position(pos + size);
    Platform.copyMemory(baseObject, baseOffset + offset, buffer.array(), Platform.BYTE_ARRAY_OFFSET + buffer.arrayOffset() + pos, size);
} 
","/**
 * Construct a new UnsafeRow. The resulting row won't be usable until `pointTo()` has been called,
 * since the value returned by this constructor is equivalent to a null pointer.
 *
 * @param numFields the number of fields in this row
 */
 
","{
    this.numFields = numFields;
    this.bitSetWidthInBytes = calculateBitSetWidthInBytes(numFields);
} 
","/**
 * Field types that can be updated in place in UnsafeRows (e.g. we support set() for these types)
 */
 
/**
 * The number of fields in this row, used for calculating the bitset width (and in assertions)
 */
 
/**
 * The size of this row's backing data, in bytes)
 */
 
/**
 * The width of the null tracking bit set, in bytes
 */
 
","Field mutableFieldTypes
Field numFields
Field sizeInBytes
Field bitSetWidthInBytes
",14,772
RowBasedKeyValueBatch.java,47,114,0.41228070175438597,"
 * RowBasedKeyValueBatch stores key value pairs in contiguous memory region.
 *
 * Each key or value is stored as a single UnsafeRow. Each record contains one key and one value
 * and some auxiliary data, which differs based on implementation:
 * i.e., `FixedLengthRowBasedKeyValueBatch` and `VariableLengthRowBasedKeyValueBatch`.
 *
 * We use `FixedLengthRowBasedKeyValueBatch` if all fields in the key and the value are fixed-length
 * data types. Otherwise we use `VariableLengthRowBasedKeyValueBatch`.
 *
 * RowBasedKeyValueBatch is backed by a single page / MemoryBlock (ranges from 1 to 64MB depending
 * on the system configuration). If the page is full, the aggregate logic should fallback to a
 * second level, larger hash map. We intentionally use the single-page design because it simplifies
 * memory address encoding & decoding for each key-value pair. Because the maximum capacity for
 * RowBasedKeyValueBatch is only 2^16, it is unlikely we need a second page anyway. Filling the
 * page requires an average size for key value pairs to be larger than 1024 bytes.
 *
 ","/**
 * Append a key value pair.
 * It copies data into the backing MemoryBlock.
 * Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.
 */
 
/**
 * Returns the key row in this batch at `rowId`. Returned key row is reused across calls.
 */
 
/**
 * Returns the value row in this batch at `rowId`. Returned value row is reused across calls.
 * Because `getValueRow(id)` is always called after `getKeyRow(id)` with the same id, we use
 * `getValueFromKey(id) to retrieve value row, which reuses metadata from the cached key.
 */
 
/**
 * Returns the value row by two steps:
 * 1) looking up the key row with the same id (skipped if the key row is cached)
 * 2) retrieve the value row by reusing the metadata from step 1)
 * In most times, 1) is skipped because `getKeyRow(id)` is often called before `getValueRow(id)`.
 */
 
/**
 * Sometimes the TaskMemoryManager may call spill() on its associated MemoryConsumers to make
 * space for new consumers. For RowBasedKeyValueBatch, we do not actually spill and return 0.
 * We should not throw OutOfMemory exception here because other associated consumers might spill
 */
 
/**
 * Returns an iterator to go through all rows
 */
 
","appendRow 
getKeyRow 
{
    return getValueFromKey(rowId);
} 
getValueFromKey 
{
    logger.warn(""Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0."");
    return 0;
} 
rowIterator 
",,,,,16,1050
UnsafeDataUtils.java,19,17,1.1176470588235294,"
 * General utilities available for unsafe data
 ",,,,,,,1,46
ExpressionDescription.java,83,16,5.1875,"
 * ::DeveloperApi::
 *
 * A function description type which can be recognized by FunctionRegistry, and will be used to
 * show the usage of the function in human language.
 *
 * `usage()` will be used for the function usage in brief way.
 *
 * These below are concatenated and used for the function usage in verbose way, suppose arguments,
 * examples, note, since and deprecated will be provided.
 *
 * `arguments()` describes arguments for the expression.
 *
 * `examples()` describes examples for the expression.
 *
 * `note()` contains some notes for the expression optionally.
 *
 * `since()` contains version information for the expression. Version is specified by,
 * for example, ""2.2.0"".
 *
 * `deprecated()` contains deprecation information for the expression optionally, for example,
 * ""Deprecated since 2.2.0. Use something else instead"".
 *
 * The format, in particular for `arguments()`, `examples()`,`note()`, `since()` and
 * `deprecated()`, should strictly be as follows.
 *
 * <pre>
 * <code>@ExpressionDescription(
 *   ...
 *   arguments = """"""
 *     Arguments:
 *       * arg0 - ...
 *           ....
 *       * arg1 - ...
 *           ....
 *   """""",
 *   examples = """"""
 *     Examples:
 *       > SELECT ...;
 *        ...
 *       > SELECT ...;
 *        ...
 *   """""",
 *   note = """"""
 *     ...
 *   """""",
 *   since = ""3.0.0"",
 *   deprecated = """"""
 *     ...
 *   """""")
 * </code>
 * </pre>
 *
 *  We can refer the function name by `_FUNC_`, in `usage()`, `arguments()` and `examples()` as
 *  it is registered in `FunctionRegistry`.
 *
 *  Note that, if `extended()` is defined, `arguments()`, `examples()`, `note()`, `since()` and
 *  `deprecated()` should be not defined together. `extended()` exists for backward compatibility.
 *
 *  Note this contents are used in the SparkSQL documentation for built-in functions. The contents
 *  here are considered as a Markdown text and then rendered.
 ",,,,,,,60,1801
UnsafeMapData.java,43,101,0.42574257425742573,"
 * An Unsafe implementation of Map which is backed by raw memory instead of Java objects.
 *
 * Currently we just use 2 UnsafeArrayData to represent UnsafeMapData, with extra 8 bytes at head
 * to indicate the number of bytes of the unsafe key array.
 * [unsafe key array numBytes] [unsafe key array] [unsafe value array]
 *
 * Note that, user is responsible to guarantee that the key array does not have duplicated
 * elements, otherwise the behavior is undefined.
 | TODO: Use a more efficient format which doesn't depend on unsafe array.","/**
 * Update this UnsafeMapData to point to different backing data.
 *
 * @param baseObject the base object
 * @param baseOffset the offset within the base object
 * @param sizeInBytes the size of this map's backing data, in bytes
 */
 
","{
    // Read the numBytes of key array from the first 8 bytes.
    final long keyArraySize = Platform.getLong(baseObject, baseOffset);
    assert keyArraySize >= 0 : ""keyArraySize ("" + keyArraySize + "") should >= 0"";
    assert keyArraySize <= Integer.MAX_VALUE : ""keyArraySize ("" + keyArraySize + "") should <= Integer.MAX_VALUE"";
    final int valueArraySize = sizeInBytes - (int) keyArraySize - 8;
    assert valueArraySize >= 0 : ""valueArraySize ("" + valueArraySize + "") should >= 0"";
    keys.pointTo(baseObject, baseOffset + 8, (int) keyArraySize);
    values.pointTo(baseObject, baseOffset + 8 + keyArraySize, valueArraySize);
    assert keys.numElements() == values.numElements();
    this.baseObject = baseObject;
    this.baseOffset = baseOffset;
    this.sizeInBytes = sizeInBytes;
} 
","/**
 * Construct a new UnsafeMapData. The resulting UnsafeMapData won't be usable until
 * `pointTo()` has been called, since the value returned by this constructor is equivalent
 * to a null pointer.
 */
 
","{
    keys = new UnsafeArrayData();
    values = new UnsafeArrayData();
} 
",,,9,524
BufferHolder.java,31,67,0.4626865671641791,"
 * A helper class to manage the data buffer for an unsafe row.  The data buffer can grow and
 * automatically re-point the unsafe row to it.
 *
 * This class can be used to build a one-pass unsafe row writing program, i.e. data will be written
 * to the data buffer directly and no extra copy is needed.  There should be only one instance of
 * this class per writing program, so that the memory segment/data buffer can be reused.  Note that
 * for each incoming record, we should call `reset` of BufferHolder instance before write the record
 * and reuse the data buffer.
 ","/**
 * Grows the buffer by at least neededSize and points the row to the buffer.
 */
 
","{
    if (neededSize < 0) {
        throw new IllegalArgumentException(""Cannot grow BufferHolder by size "" + neededSize + "" because the size is negative"");
    }
    if (neededSize > ARRAY_MAX - totalSize()) {
        throw new IllegalArgumentException(""Cannot grow BufferHolder by size "" + neededSize + "" because the size after growing "" + ""exceeds size limitation "" + ARRAY_MAX);
    }
    final int length = totalSize() + neededSize;
    if (buffer.length < length) {
        // This will not happen frequently, because the buffer is re-used.
        int newLength = length < ARRAY_MAX / 2 ? length * 2 : ARRAY_MAX;
        int roundedSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(newLength);
        final byte[] tmp = new byte[roundedSize];
        Platform.copyMemory(buffer, Platform.BYTE_ARRAY_OFFSET, tmp, Platform.BYTE_ARRAY_OFFSET, totalSize());
        buffer = tmp;
        row.pointTo(buffer, buffer.length);
    }
} 
",,,,,8,558
UnsafeRowWriter.java,49,137,0.35766423357664234,"
 * A helper class to write data into global row buffer using `UnsafeRow` format.
 *
 * It will remember the offset of row buffer which it starts to write, and move the cursor of row
 * buffer while writing.  If new data(can be the input record if this is the outermost writer, or
 * nested struct if this is an inner writer) comes, the starting cursor of row buffer may be
 * changed, so we need to call `UnsafeRowWriter.resetRowWriter` before writing, to update the
 * `startingOffset` and clear out null bits.
 *
 * Note that if this is the outermost writer, which means we will always write from the very
 * beginning of the global row buffer, we don't need to update `startingOffset` and can just call
 * `zeroOutNullBytes` before writing new data.
 ","/**
 * Updates total size of the UnsafeRow using the size collected by BufferHolder, and returns
 * the UnsafeRow created at a constructor
 */
 
/**
 * Resets the `startingOffset` according to the current cursor of row buffer, and clear out null
 * bits.  This should be called before we write a new nested struct to the row buffer.
 */
 
/**
 * Clears out null bits.  This should be called before we write a new row to row buffer.
 */
 
","{
    row.setTotalSize(totalSize());
    return row;
} 
{
    this.startingOffset = cursor();
    // grow the global buffer to make sure it has enough space to write fixed-length data.
    grow(fixedSize);
    increaseCursor(fixedSize);
    zeroOutNullBytes();
} 
{
    for (int i = 0; i < nullBitsSize; i += 8) {
        Platform.putLong(getBuffer(), startingOffset + i, 0L);
    }
} 
",,,,,11,732
UnsafeArrayWriter.java,33,120,0.275,"
 * A helper class to write data into global row buffer using `UnsafeArrayData` format,
 * used by {@link org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection}.
 ",,,,,,,2,179
UnsafeWriter.java,30,141,0.2127659574468085,"
 * Base class for writing Unsafe* structures.
 ","/**
 * Accessor methods are delegated from BufferHolder class
 */
 
","{
    return holder;
} 
",,,,,1,44
UDFXPathUtil.java,26,163,0.15950920245398773,"
 * Utility class for all XPath UDFs. Each UDF instance should keep an instance of this class.
 *
 * This is based on Hive's UDFXPathUtil implementation.
 |
   * Reusable, non-threadsafe version of {@link java.io.StringReader}.
   ","/**
 * Check to make sure that the stream has not been closed
 */
 
","{
    if (str == null) {
        throw new IOException(""Stream closed"");
    }
} 
",,,,,5,221
SpecializedGetters.java,16,26,0.6153846153846154,,,,,,,,1,0
UnsafeArrayData.java,60,396,0.15151515151515152,,"/**
 * Update this UnsafeArrayData to point to different backing data.
 *
 * @param baseObject the base object
 * @param baseOffset the offset within the base object
 * @param sizeInBytes the size of this array's backing data, in bytes
 */
 
","{
    // Read the number of elements from the first 8 bytes.
    final long numElements = Platform.getLong(baseObject, baseOffset);
    assert numElements >= 0 : ""numElements ("" + numElements + "") should >= 0"";
    assert numElements <= Integer.MAX_VALUE : ""numElements ("" + numElements + "") should <= Integer.MAX_VALUE"";
    this.numElements = (int) numElements;
    this.baseObject = baseObject;
    this.baseOffset = baseOffset;
    this.sizeInBytes = sizeInBytes;
    this.elementOffset = baseOffset + calculateHeaderPortionInBytes(this.numElements);
} 
","/**
 * Construct a new UnsafeArrayData. The resulting UnsafeArrayData won't be usable until
 * `pointTo()` has been called, since the value returned by this constructor is equivalent
 * to a null pointer.
 */
 
","{
} 
","/**
 * The position to start storing array elements,
 */
 
","Field elementOffset
",1,0
ExpressionInfo.java,26,121,0.21487603305785125,"
 * Expression information, will be used to describe a expression.
 ",,,"/**
 * @deprecated This constructor is deprecated as of Spark 3.0. Use other constructors to fully
 *   specify each argument for extended usage.
 */
 
","{
    // `arguments` and `examples` are concatenated for the extended description. So, here
    // simply pass the `extended` as `arguments` and an empty string for `examples`.
    this(className, db, name, usage, extended, """", """", """", """");
} 
",,,1,65
SpecializedGettersReader.java,16,68,0.23529411764705882,,,,,,,,1,0
VariableLengthRowBasedKeyValueBatch.java,47,114,0.41228070175438597,"
 * An implementation of `RowBasedKeyValueBatch` in which key-value records have variable lengths.
 *
 *  The format for each record looks like this:
 * [4 bytes total size = (klen + vlen + 4)] [4 bytes key size = klen]
 * [UnsafeRow for key of length klen] [UnsafeRow for Value of length vlen]
 * [8 bytes pointer to next]
 * Thus, record length = 4 + 4 + klen + vlen + 8
 ","/**
 * Append a key value pair.
 * It copies data into the backing MemoryBlock.
 * Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.
 */
 
/**
 * Returns the key row in this batch at `rowId`. Returned key row is reused across calls.
 */
 
/**
 * Returns the value row by two steps:
 * 1) looking up the key row with the same id (skipped if the key row is cached)
 * 2) retrieve the value row by reusing the metadata from step 1)
 * In most times, 1) is skipped because `getKeyRow(id)` is often called before `getValueRow(id)`.
 */
 
/**
 * Returns an iterator to go through all rows
 */
 
","{
    final long recordLength = 8L + klen + vlen + 8;
    // if run out of max supported rows or page size, return null
    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {
        return null;
    }
    long offset = page.getBaseOffset() + pageCursor;
    final long recordOffset = offset;
    Platform.putInt(base, offset, klen + vlen + 4);
    Platform.putInt(base, offset + 4, klen);
    offset += 8;
    Platform.copyMemory(kbase, koff, base, offset, klen);
    offset += klen;
    Platform.copyMemory(vbase, voff, base, offset, vlen);
    offset += vlen;
    Platform.putLong(base, offset, 0);
    pageCursor += recordLength;
    keyOffsets[numRows] = recordOffset + 8;
    keyRowId = numRows;
    keyRow.pointTo(base, recordOffset + 8, klen);
    valueRow.pointTo(base, recordOffset + 8 + klen, vlen);
    numRows++;
    return valueRow;
} 
{
    assert (rowId >= 0);
    assert (rowId < numRows);
    if (keyRowId != rowId) {
        // if keyRowId == rowId, desired keyRow is already cached
        long offset = keyOffsets[rowId];
        int klen = Platform.getInt(base, offset - 4);
        keyRow.pointTo(base, offset, klen);
        // set keyRowId so we can check if desired row is cached
        keyRowId = rowId;
    }
    return keyRow;
} 
{
    if (keyRowId != rowId) {
        getKeyRow(rowId);
    }
    assert (rowId >= 0);
    long offset = keyRow.getBaseOffset();
    int klen = keyRow.getSizeInBytes();
    int vlen = Platform.getInt(base, offset - 8) - klen - 4;
    valueRow.pointTo(base, offset + klen, vlen);
    return valueRow;
} 
{
    return new org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow>() {

        private final UnsafeRow key = new UnsafeRow(keySchema.length());

        private final UnsafeRow value = new UnsafeRow(valueSchema.length());

        private long offsetInPage = 0;

        private int recordsInPage = 0;

        private int currentklen;

        private int currentvlen;

        private int totalLength;

        private boolean initialized = false;

        private void init() {
            if (page != null) {
                offsetInPage = page.getBaseOffset();
                recordsInPage = numRows;
            }
            initialized = true;
        }

        @Override
        public boolean next() {
            if (!initialized)
                init();
            // searching for the next non empty page is records is now zero
            if (recordsInPage == 0) {
                freeCurrentPage();
                return false;
            }
            totalLength = Platform.getInt(base, offsetInPage) - 4;
            currentklen = Platform.getInt(base, offsetInPage + 4);
            currentvlen = totalLength - currentklen;
            key.pointTo(base, offsetInPage + 8, currentklen);
            value.pointTo(base, offsetInPage + 8 + currentklen, currentvlen);
            offsetInPage += 8 + totalLength + 8;
            recordsInPage -= 1;
            return true;
        }

        @Override
        public UnsafeRow getKey() {
            return key;
        }

        @Override
        public UnsafeRow getValue() {
            return value;
        }

        @Override
        public void close() {
        // do nothing
        }

        private void freeCurrentPage() {
            if (page != null) {
                freePage(page);
                page = null;
            }
        }
    };
} 
",,,,,7,359
XXH64.java,27,137,0.19708029197080293," scalastyle: off|
 * xxHash64. A high quality and fast 64 bit hash code by Yann Colet and Mathias Westerdahl. The
 * class below is modelled like its Murmur3_x86_32 cousin.
 * <p/>
 * This was largely based on the following (original) C and Java implementations:
 * https://github.com/Cyan4973/xxHash/blob/master/xxhash.c
 * https://github.com/OpenHFT/Zero-Allocation-Hashing/blob/master/src/main/java/net/openhft/hashing/XxHash_r39.java
 * https://github.com/airlift/slice/blob/master/src/main/java/io/airlift/slice/XxHash64.java
 | scalastyle: on",,,,,,,9,533
FixedLengthRowBasedKeyValueBatch.java,45,108,0.4166666666666667,"
 * An implementation of `RowBasedKeyValueBatch` in which all key-value records have same length.
 *
 * The format for each record looks like this:
 * [UnsafeRow for key of length klen] [UnsafeRow for Value of length vlen]
 * [8 bytes pointer to next]
 * Thus, record length = klen + vlen + 8
 ","/**
 * Append a key value pair.
 * It copies data into the backing MemoryBlock.
 * Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.
 */
 
/**
 * Returns the key row in this batch at `rowId`. Returned key row is reused across calls.
 */
 
/**
 * Returns the value row by two steps:
 * 1) looking up the key row with the same id (skipped if the key row is cached)
 * 2) retrieve the value row by reusing the metadata from step 1)
 * In most times, 1) is skipped because `getKeyRow(id)` is often called before `getValueRow(id)`.
 */
 
/**
 * Returns an iterator to go through all rows
 */
 
","{
    // if run out of max supported rows or page size, return null
    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {
        return null;
    }
    long offset = page.getBaseOffset() + pageCursor;
    final long recordOffset = offset;
    Platform.copyMemory(kbase, koff, base, offset, klen);
    offset += klen;
    Platform.copyMemory(vbase, voff, base, offset, vlen);
    offset += vlen;
    Platform.putLong(base, offset, 0);
    pageCursor += recordLength;
    keyRowId = numRows;
    keyRow.pointTo(base, recordOffset, klen);
    valueRow.pointTo(base, recordOffset + klen, vlen);
    numRows++;
    return valueRow;
} 
{
    assert (rowId >= 0);
    assert (rowId < numRows);
    if (keyRowId != rowId) {
        // if keyRowId == rowId, desired keyRow is already cached
        long offset = getKeyOffsetForFixedLengthRecords(rowId);
        keyRow.pointTo(base, offset, klen);
        // set keyRowId so we can check if desired row is cached
        keyRowId = rowId;
    }
    return keyRow;
} 
{
    if (keyRowId != rowId) {
        getKeyRow(rowId);
    }
    assert (rowId >= 0);
    valueRow.pointTo(base, keyRow.getBaseOffset() + klen, vlen);
    return valueRow;
} 
{
    return new org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow>() {

        private final UnsafeRow key = new UnsafeRow(keySchema.length());

        private final UnsafeRow value = new UnsafeRow(valueSchema.length());

        private long offsetInPage = 0;

        private int recordsInPage = 0;

        private boolean initialized = false;

        private void init() {
            if (page != null) {
                offsetInPage = page.getBaseOffset();
                recordsInPage = numRows;
            }
            initialized = true;
        }

        @Override
        public boolean next() {
            if (!initialized)
                init();
            // searching for the next non empty page is records is now zero
            if (recordsInPage == 0) {
                freeCurrentPage();
                return false;
            }
            key.pointTo(base, offsetInPage, klen);
            value.pointTo(base, offsetInPage + klen, vlen);
            offsetInPage += recordLength;
            recordsInPage -= 1;
            return true;
        }

        @Override
        public UnsafeRow getKey() {
            return key;
        }

        @Override
        public UnsafeRow getValue() {
            return value;
        }

        @Override
        public void close() {
        // do nothing
        }

        private void freeCurrentPage() {
            if (page != null) {
                freePage(page);
                page = null;
            }
        }
    };
} 
",,,,,6,281
CaseInsensitiveStringMap.java,44,126,0.3492063492063492,"
 * Case-insensitive map of string keys to string values.
 * <p>
 * This is used to pass options to v2 implementations to ensure consistent case insensitivity.
 * <p>
 * Methods that return keys in this map, like {@link #entrySet()} and {@link #keySet()}, return
 * keys converted to lower case. This map doesn't allow null key.
 ","/**
 * Returns the boolean value to which the specified key is mapped,
 * or defaultValue if there is no mapping for the key. The key match is case-insensitive.
 */
 
/**
 * Returns the integer value to which the specified key is mapped,
 * or defaultValue if there is no mapping for the key. The key match is case-insensitive.
 */
 
/**
 * Returns the long value to which the specified key is mapped,
 * or defaultValue if there is no mapping for the key. The key match is case-insensitive.
 */
 
/**
 * Returns the double value to which the specified key is mapped,
 * or defaultValue if there is no mapping for the key. The key match is case-insensitive.
 */
 
/**
 * Returns the original case-sensitive map.
 */
 
","{
    String value = get(key);
    // We can't use `Boolean.parseBoolean` here, as it returns false for invalid strings.
    if (value == null) {
        return defaultValue;
    } else if (value.equalsIgnoreCase(""true"")) {
        return true;
    } else if (value.equalsIgnoreCase(""false"")) {
        return false;
    } else {
        throw new IllegalArgumentException(value + "" is not a boolean string."");
    }
} 
{
    String value = get(key);
    return value == null ? defaultValue : Integer.parseInt(value);
} 
{
    String value = get(key);
    return value == null ? defaultValue : Long.parseLong(value);
} 
{
    String value = get(key);
    return value == null ? defaultValue : Double.parseDouble(value);
} 
{
    return Collections.unmodifiableMap(original);
} 
",,,,,6,317
GroupStateTimeout.java,37,13,2.8461538461538463,"
 * Represents the type of timeouts possible for the Dataset operations
 * `mapGroupsWithState` and `flatMapGroupsWithState`. See documentation on
 * `GroupState` for more details.
 *
 * @since 2.2.0
 ","/**
 * Timeout based on processing time. The duration of timeout can be set for each group in
 * `map/flatMapGroupsWithState` by calling `GroupState.setTimeoutDuration()`. See documentation
 * on `GroupState` for more details.
 */
 
/**
 * Timeout based on event-time. The event-time timestamp for timeout can be set for each
 * group in `map/flatMapGroupsWithState` by calling `GroupState.setTimeoutTimestamp()`.
 * In addition, you have to define the watermark in the query using `Dataset.withWatermark`.
 * When the watermark advances beyond the set timestamp of a group and the group has not
 * received any data, then the group times out. See documentation on
 * `GroupState` for more details.
 */
 
/**
 * No timeout.
 */
 
","{
    return ProcessingTimeTimeout$.MODULE$;
} 
{
    return EventTimeTimeout$.MODULE$;
} 
{
    return NoTimeout$.MODULE$;
} 
",,,,,5,190
OutputMode.java,43,15,2.8666666666666667,"
 * OutputMode describes what data will be written to a streaming sink when there is
 * new data available in a streaming DataFrame/Dataset.
 *
 * @since 2.0.0
 ","/**
 * OutputMode in which only the new rows in the streaming DataFrame/Dataset will be
 * written to the sink. This output mode can be only be used in queries that do not
 * contain any aggregation.
 *
 * @since 2.0.0
 */
 
/**
 * OutputMode in which all the rows in the streaming DataFrame/Dataset will be written
 * to the sink every time there are some updates. This output mode can only be used in queries
 * that contain aggregations.
 *
 * @since 2.0.0
 */
 
/**
 * OutputMode in which only the rows that were updated in the streaming DataFrame/Dataset will
 * be written to the sink every time there are some updates. If the query doesn't contain
 * aggregations, it will be equivalent to `Append` mode.
 *
 * @since 2.1.1
 */
 
","{
    return InternalOutputModes.Append$.MODULE$;
} 
{
    return InternalOutputModes.Complete$.MODULE$;
} 
{
    return InternalOutputModes.Update$.MODULE$;
} 
",,,,,4,152
SupportsDynamicOverwrite.java,31,4,7.75,"
 * Write builder trait for tables that support dynamic partition overwrite.
 * <p>
 * A write that dynamically overwrites partitions removes all existing data in each logical
 * partition for which the write will commit new data. Any existing logical partition for which the
 * write does not contain data will remain unchanged.
 * <p>
 * This is provided to implement SQL compatible with Hive table operations but is not recommended.
 * Instead, use the {@link SupportsOverwrite overwrite by filter API} to explicitly replace data.
 ","/**
 * Configures a write to dynamically replace partitions with data committed in the write.
 *
 * @return this write builder for method chaining
 */
 
","overwriteDynamicPartitions 
",,,,,8,518
BatchWrite.java,81,12,6.75,"
 * An interface that defines how to write the data to data source for batch processing.
 *
 * The writing procedure is:
 *   1. Create a writer factory by {@link #createBatchWriterFactory()}, serialize and send it to all
 *      the partitions of the input data(RDD).
 *   2. For each partition, create the data writer, and write the data of the partition with this
 *      writer. If all the data are written successfully, call {@link DataWriter#commit()}. If
 *      exception happens during the writing, call {@link DataWriter#abort()}.
 *   3. If all writers are successfully committed, call {@link #commit(WriterCommitMessage[])}. If
 *      some writers are aborted, or the job failed with an unknown reason, call
 *      {@link #abort(WriterCommitMessage[])}.
 *
 * While Spark will retry failed writing tasks, Spark won't retry failed writing jobs. Users should
 * do it manually in their Spark applications if they want to retry.
 *
 * Please refer to the documentation of commit/abort methods for detailed specifications.
 ","/**
 * Creates a writer factory which will be serialized and sent to executors.
 *
 * If this method fails (by throwing an exception), the action will fail and no Spark job will be
 * submitted.
 */
 
/**
 * Returns whether Spark should use the commit coordinator to ensure that at most one task for
 * each partition commits.
 *
 * @return true if commit coordinator should be used, false otherwise.
 */
 
/**
 * Handles a commit message on receiving from a successful data writer.
 *
 * If this method fails (by throwing an exception), this writing job is considered to to have been
 * failed, and {@link #abort(WriterCommitMessage[])} would be called.
 */
 
/**
 * Commits this writing job with a list of commit messages. The commit messages are collected from
 * successful data writers and are produced by {@link DataWriter#commit()}.
 *
 * If this method fails (by throwing an exception), this writing job is considered to to have been
 * failed, and {@link #abort(WriterCommitMessage[])} would be called. The state of the destination
 * is undefined and @{@link #abort(WriterCommitMessage[])} may not be able to deal with it.
 *
 * Note that speculative execution may cause multiple tasks to run for a partition. By default,
 * Spark uses the commit coordinator to allow at most one task to commit. Implementations can
 * disable this behavior by overriding {@link #useCommitCoordinator()}. If disabled, multiple
 * tasks may have committed successfully and one successful commit message per task will be
 * passed to this commit method. The remaining commit messages are ignored by Spark.
 */
 
/**
 * Aborts this writing job because some data writers are failed and keep failing when retry,
 * or the Spark job fails with some unknown reasons,
 * or {@link #onDataWriterCommit(WriterCommitMessage)} fails,
 * or {@link #commit(WriterCommitMessage[])} fails.
 *
 * If this method fails (by throwing an exception), the underlying data source may require manual
 * cleanup.
 *
 * Unless the abort is triggered by the failure of commit, the given messages should have some
 * null slots as there maybe only a few data writers that are committed before the abort
 * happens, or some data writers were committed but their commit messages haven't reached the
 * driver when the abort is triggered. So this is just a ""best effort"" for data sources to
 * clean up the data left by data writers.
 */
 
","createBatchWriterFactory 
{
    return true;
} 
{
} 
commit 
abort 
",,,,,16,1001
SupportsOverwrite.java,31,10,3.1,"
 * Write builder trait for tables that support overwrite by filter.
 * <p>
 * Overwriting data by filter will delete any data that matches the filter and replace it with data
 * that is committed in the write.
 ","/**
 * Configures a write to replace data matching the filters with data committed in the write.
 * <p>
 * Rows must be deleted from the data source if and only if all of the filters match. That is,
 * filters must be interpreted as ANDed together.
 *
 * @param filters filters used to match data to overwrite
 * @return this write builder for method chaining
 */
 
","overwrite 
",,,,,4,203
DataWriter.java,86,9,9.555555555555555,"
 * A data writer returned by {@link DataWriterFactory#createWriter(int, long)} and is
 * responsible for writing data for an input RDD partition.
 *
 * One Spark task has one exclusive data writer, so there is no thread-safe concern.
 *
 * {@link #write(Object)} is called for each record in the input RDD partition. If one record fails
 * the {@link #write(Object)}, {@link #abort()} is called afterwards and the remaining records will
 * not be processed. If all records are successfully written, {@link #commit()} is called.
 *
 * Once a data writer returns successfully from {@link #commit()} or {@link #abort()}, its lifecycle
 * is over and Spark will not use it again.
 *
 * If this data writer succeeds(all records are successfully written and {@link #commit()}
 * succeeds), a {@link WriterCommitMessage} will be sent to the driver side and pass to
 * {@link BatchWrite#commit(WriterCommitMessage[])} with commit messages from other data
 * writers. If this data writer fails(one record fails to write or {@link #commit()} fails), an
 * exception will be sent to the driver side, and Spark may retry this writing task a few times.
 * In each retry, {@link DataWriterFactory#createWriter(int, long)} will receive a
 * different `taskId`. Spark will call {@link BatchWrite#abort(WriterCommitMessage[])}
 * when the configured number of retries is exhausted.
 *
 * Besides the retry mechanism, Spark may launch speculative tasks if the existing writing task
 * takes too long to finish. Different from retried tasks, which are launched one by one after the
 * previous one fails, speculative tasks are running simultaneously. It's possible that one input
 * RDD partition has multiple data writers with different `taskId` running at the same time,
 * and data sources should guarantee that these data writers don't conflict and can work together.
 * Implementations can coordinate with driver during {@link #commit()} to make sure only one of
 * these data writers can commit successfully. Or implementations can allow all of them to commit
 * successfully, and have a way to revert committed data writers without the commit message, because
 * Spark only accepts the commit message that arrives first and ignore others.
 *
 * Note that, Currently the type `T` can only be {@link org.apache.spark.sql.catalyst.InternalRow}.
 ","/**
 * Writes one record.
 *
 * If this method fails (by throwing an exception), {@link #abort()} will be called and this
 * data writer is considered to have been failed.
 *
 * @throws IOException if failure happens during disk/network IO like writing files.
 */
 
/**
 * Commits this writer after all records are written successfully, returns a commit message which
 * will be sent back to driver side and passed to
 * {@link BatchWrite#commit(WriterCommitMessage[])}.
 *
 * The written data should only be visible to data source readers after
 * {@link BatchWrite#commit(WriterCommitMessage[])} succeeds, which means this method
 * should still ""hide"" the written data and ask the {@link BatchWrite} at driver side to
 * do the final commit via {@link WriterCommitMessage}.
 *
 * If this method fails (by throwing an exception), {@link #abort()} will be called and this
 * data writer is considered to have been failed.
 *
 * @throws IOException if failure happens during disk/network IO like writing files.
 */
 
/**
 * Aborts this writer if it is failed. Implementations should clean up the data for already
 * written records.
 *
 * This method will only be called if there is one record failed to write, or {@link #commit()}
 * failed.
 *
 * If this method fails(by throwing an exception), the underlying data source may have garbage
 * that need to be cleaned by {@link BatchWrite#abort(WriterCommitMessage[])} or manually,
 * but these garbage should not be visible to data source readers.
 *
 * @throws IOException if failure happens during disk/network IO like writing files.
 */
 
","write 
commit 
abort 
",,,,,32,2267
StreamingDataWriterFactory.java,44,10,4.4,"
 * A factory of {@link DataWriter} returned by
 * {@link StreamingWrite#createStreamingWriterFactory()}, which is responsible for creating
 * and initializing the actual data writer at executor side.
 *
 * Note that, the writer factory will be serialized and sent to executors, then the data writer
 * will be created on executors and do the actual writing. So this interface must be
 * serializable and {@link DataWriter} doesn't need to be.
 ","/**
 * Returns a data writer to do the actual writing work. Note that, Spark will reuse the same data
 * object instance when sending data to the data writer, for better performance. Data writers
 * are responsible for defensive copies if necessary, e.g. copy the data before buffer it in a
 * list.
 *
 * If this method fails (by throwing an exception), the corresponding Spark write task would fail
 * and get retried until hitting the maximum retry times.
 *
 * @param partitionId A unique id of the RDD partition that the returned writer will process.
 *                    Usually Spark processes many RDD partitions at the same time,
 *                    implementations should use the partition id to distinguish writers for
 *                    different partitions.
 * @param taskId The task id returned by {@link TaskContext#taskAttemptId()}. Spark may run
 *               multiple tasks for the same partition (due to speculation or task failures,
 *               for example).
 * @param epochId A monotonically increasing id for streaming queries that are split in to
 *                discrete periods of execution.
 */
 
","createWriter 
",,,,,7,430
StreamingWrite.java,68,10,6.8,"
 * An interface that defines how to write the data to data source in streaming queries.
 *
 * The writing procedure is:
 *   1. Create a writer factory by {@link #createStreamingWriterFactory()}, serialize and send it to
 *      all the partitions of the input data(RDD).
 *   2. For each epoch in each partition, create the data writer, and write the data of the epoch in
 *      the partition with this writer. If all the data are written successfully, call
 *      {@link DataWriter#commit()}. If exception happens during the writing, call
 *      {@link DataWriter#abort()}.
 *   3. If writers in all partitions of one epoch are successfully committed, call
 *      {@link #commit(long, WriterCommitMessage[])}. If some writers are aborted, or the job failed
 *      with an unknown reason, call {@link #abort(long, WriterCommitMessage[])}.
 *
 * While Spark will retry failed writing tasks, Spark won't retry failed writing jobs. Users should
 * do it manually in their Spark applications if they want to retry.
 *
 * Please refer to the documentation of commit/abort methods for detailed specifications.
 ","/**
 * Creates a writer factory which will be serialized and sent to executors.
 *
 * If this method fails (by throwing an exception), the action will fail and no Spark job will be
 * submitted.
 */
 
/**
 * Commits this writing job for the specified epoch with a list of commit messages. The commit
 * messages are collected from successful data writers and are produced by
 * {@link DataWriter#commit()}.
 *
 * If this method fails (by throwing an exception), this writing job is considered to have been
 * failed, and the execution engine will attempt to call
 * {@link #abort(long, WriterCommitMessage[])}.
 *
 * The execution engine may call `commit` multiple times for the same epoch in some circumstances.
 * To support exactly-once data semantics, implementations must ensure that multiple commits for
 * the same epoch are idempotent.
 */
 
/**
 * Aborts this writing job because some data writers are failed and keep failing when retried, or
 * the Spark job fails with some unknown reasons, or {@link #commit(long, WriterCommitMessage[])}
 * fails.
 *
 * If this method fails (by throwing an exception), the underlying data source may require manual
 * cleanup.
 *
 * Unless the abort is triggered by the failure of commit, the given messages will have some
 * null slots, as there may be only a few data writers that were committed before the abort
 * happens, or some data writers were committed but their commit messages haven't reached the
 * driver when the abort is triggered. So this is just a ""best effort"" for data sources to
 * clean up the data left by data writers.
 */
 
","createStreamingWriterFactory 
commit 
abort 
",,,,,17,1077
WriteBuilder.java,49,23,2.130434782608696,"
 * An interface for building the {@link BatchWrite}. Implementations can mix in some interfaces to
 * support different ways to write data to data sources.
 *
 * Unless modified by a mixin interface, the {@link BatchWrite} configured by this builder is to
 * append data without affecting existing data.
 ","/**
 * Passes the `queryId` from Spark to data source. `queryId` is a unique string of the query. It's
 * possible that there are many queries running at the same time, or a query is restarted and
 * resumed. {@link BatchWrite} can use this id to identify the query.
 *
 * @return a new builder with the `queryId`. By default it returns `this`, which means the given
 *         `queryId` is ignored. Please override this method to take the `queryId`.
 */
 
/**
 * Passes the schema of the input data from Spark to data source.
 *
 * @return a new builder with the `schema`. By default it returns `this`, which means the given
 *         `schema` is ignored. Please override this method to take the `schema`.
 */
 
/**
 * Returns a {@link BatchWrite} to write data to batch source. By default this method throws
 * exception, data sources must overwrite this method to provide an implementation, if the
 * {@link Table} that creates this write returns {@link TableCapability#BATCH_WRITE} support in
 * its {@link Table#capabilities()}.
 */
 
/**
 * Returns a {@link StreamingWrite} to write data to streaming source. By default this method
 * throws exception, data sources must overwrite this method to provide an implementation, if the
 * {@link Table} that creates this write returns {@link TableCapability#STREAMING_WRITE} support
 * in its {@link Table#capabilities()}.
 */
 
","{
    return this;
} 
{
    return this;
} 
{
    throw new UnsupportedOperationException(getClass().getName() + "" does not support batch write"");
} 
{
    throw new UnsupportedOperationException(getClass().getName() + "" does not support streaming write"");
} 
",,,,,5,295
WriterCommitMessage.java,24,6,4.0,"
 * A commit message returned by {@link DataWriter#commit()} and will be sent back to the driver side
 * as the input parameter of {@link BatchWrite#commit(WriterCommitMessage[])} or
 * {@link StreamingWrite#commit(long, WriterCommitMessage[])}.
 *
 * This is an empty interface, data sources should define their own message class and use it when
 * generating messages at executor side and handling the messages at driver side.
 ",,,,,,,6,417
SupportsTruncate.java,26,4,6.5,"
 * Write builder trait for tables that support truncation.
 * <p>
 * Truncation removes all data in a table and replaces it with data that is committed in the write.
 ","/**
 * Configures a write to replace all existing data with data committed in the write.
 *
 * @return this write builder for method chaining
 */
 
","truncate 
",,,,,3,161
DataWriterFactory.java,44,10,4.4,"
 * A factory of {@link DataWriter} returned by {@link BatchWrite#createBatchWriterFactory()},
 * which is responsible for creating and initializing the actual data writer at executor side.
 *
 * Note that, the writer factory will be serialized and sent to executors, then the data writer
 * will be created on executors and do the actual writing. So this interface must be
 * serializable and {@link DataWriter} doesn't need to be.
 ","/**
 * Returns a data writer to do the actual writing work. Note that, Spark will reuse the same data
 * object instance when sending data to the data writer, for better performance. Data writers
 * are responsible for defensive copies if necessary, e.g. copy the data before buffer it in a
 * list.
 *
 * If this method fails (by throwing an exception), the corresponding Spark write task would fail
 * and get retried until hitting the maximum retry times.
 *
 * @param partitionId A unique id of the RDD partition that the returned writer will process.
 *                    Usually Spark processes many RDD partitions at the same time,
 *                    implementations should use the partition id to distinguish writers for
 *                    different partitions.
 * @param taskId The task id returned by {@link TaskContext#taskAttemptId()}. Spark may run
 *               multiple tasks for the same partition (due to speculation or task failures,
 *               for example).
 */
 
","createWriter 
",,,,,6,421
Identifier.java,26,404,0.06435643564356436,"
 * Identifies an object in a catalog.
 ","/**
 * @return the namespace in the catalog
 */
 
/**
 * @return the object name
 */
 
","namespace 
name 
",,,,,1,37
SupportsWrite.java,25,9,2.7777777777777777,"
 * A mix-in interface of {@link Table}, to indicate that it's writable. This adds
 * {@link #newWriteBuilder(CaseInsensitiveStringMap)} that is used to create a write
 * for batch or streaming.
 ","/**
 * Returns a {@link WriteBuilder} which can be used to create {@link BatchWrite}. Spark will call
 * this method to configure each data source write.
 */
 
","newWriteBuilder 
",,,,,3,189
TableCatalog.java,114,33,3.4545454545454546,"
 * Catalog methods for working with Tables.
 * <p>
 * TableCatalog implementations may be case sensitive or case insensitive. Spark will pass
 * {@link Identifier table identifiers} without modification. Field names passed to
 * {@link #alterTable(Identifier, TableChange...)} will be normalized to match the case used in the
 * table schema when updating, renaming, or dropping existing columns when catalyst analysis is case
 * insensitive.
 ","/**
 * List the tables in a namespace from the catalog.
 * <p>
 * If the catalog supports views, this must return identifiers for only tables and not views.
 *
 * @param namespace a multi-part namespace
 * @return an array of Identifiers for tables
 * @throws NoSuchNamespaceException If the namespace does not exist (optional).
 */
 
/**
 * Load table metadata by {@link Identifier identifier} from the catalog.
 * <p>
 * If the catalog supports views and contains a view for the identifier and not a table, this
 * must throw {@link NoSuchTableException}.
 *
 * @param ident a table identifier
 * @return the table's metadata
 * @throws NoSuchTableException If the table doesn't exist or is a view
 */
 
/**
 * Invalidate cached table metadata for an {@link Identifier identifier}.
 * <p>
 * If the table is already loaded or cached, drop cached data. If the table does not exist or is
 * not cached, do nothing. Calling this method should not query remote services.
 *
 * @param ident a table identifier
 */
 
/**
 * Test whether a table exists using an {@link Identifier identifier} from the catalog.
 * <p>
 * If the catalog supports views and contains a view for the identifier and not a table, this
 * must return false.
 *
 * @param ident a table identifier
 * @return true if the table exists, false otherwise
 */
 
/**
 * Create a table in the catalog.
 *
 * @param ident a table identifier
 * @param schema the schema of the new table, as a struct type
 * @param partitions transforms to use for partitioning data in the table
 * @param properties a string map of table properties
 * @return metadata for the new table
 * @throws TableAlreadyExistsException If a table or view already exists for the identifier
 * @throws UnsupportedOperationException If a requested partition transform is not supported
 * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)
 */
 
/**
 * Apply a set of {@link TableChange changes} to a table in the catalog.
 * <p>
 * Implementations may reject the requested changes. If any change is rejected, none of the
 * changes should be applied to the table.
 * <p>
 * If the catalog supports views and contains a view for the identifier and not a table, this
 * must throw {@link NoSuchTableException}.
 *
 * @param ident a table identifier
 * @param changes changes to apply to the table
 * @return updated metadata for the table
 * @throws NoSuchTableException If the table doesn't exist or is a view
 * @throws IllegalArgumentException If any change is rejected by the implementation.
 */
 
/**
 * Drop a table in the catalog.
 * <p>
 * If the catalog supports views and contains a view for the identifier and not a table, this
 * must not drop the view and must return false.
 *
 * @param ident a table identifier
 * @return true if a table was deleted, false if no table exists for the identifier
 */
 
/**
 * Renames a table in the catalog.
 * <p>
 * If the catalog supports views and contains a view for the old identifier and not a table, this
 * throws {@link NoSuchTableException}. Additionally, if the new identifier is a table or a view,
 * this throws {@link TableAlreadyExistsException}.
 * <p>
 * If the catalog does not support table renames between namespaces, it throws
 * {@link UnsupportedOperationException}.
 *
 * @param oldIdent the table identifier of the existing table to rename
 * @param newIdent the new table identifier of the table
 * @throws NoSuchTableException If the table to rename doesn't exist or is a view
 * @throws TableAlreadyExistsException If the new table name already exists or is a view
 * @throws UnsupportedOperationException If the namespaces of old and new identiers do not
 *                                       match (optional)
 */
 
","listTables 
loadTable 
{
} 
{
    try {
        return loadTable(ident) != null;
    } catch (NoSuchTableException e) {
        return false;
    }
} 
createTable 
alterTable 
dropTable 
renameTable 
",,,,,7,430
TableCapability.java,75,15,5.0,"
 * Capabilities that can be provided by a {@link Table} implementation.
 * <p>
 * Tables use {@link Table#capabilities()} to return a set of capabilities. Each capability signals
 * to Spark that the table supports a feature identified by the capability. For example, returning
 * {@link #BATCH_READ} allows Spark to read from the table using a batch scan.
 ",,,,,,,5,348
CatalogExtension.java,22,106,0.20754716981132076,"
 * An API to extend the Spark built-in session catalog. Implementation can get the built-in session
 * catalog from {@link #setDelegateCatalog(CatalogPlugin)}, implement catalog functions with
 * some custom logic and call the built-in session catalog at the end. For example, they can
 * implement {@code createTable}, do something else before calling {@code createTable} of the
 * built-in session catalog.
 ","/**
 * This will be called only once by Spark to pass in the Spark built-in session catalog, after
 * {@link #initialize(String, CaseInsensitiveStringMap)} is called.
 */
 
","setDelegateCatalog 
",,,,,5,400
SupportsNamespaces.java,106,29,3.6551724137931036,"
 * Catalog methods for working with namespaces.
 * <p>
 * If an object such as a table, view, or function exists, its parent namespaces must also exist
 * and must be returned by the discovery methods {@link #listNamespaces()} and
 * {@link #listNamespaces(String[])}.
 * <p>
 * Catalog implementations are not required to maintain the existence of namespaces independent of
 * objects in a namespace. For example, a function catalog that loads functions using reflection
 * and uses Java packages as namespaces is not required to support the methods to create, alter, or
 * drop a namespace. Implementations are allowed to discover the existence of objects or namespaces
 * without throwing {@link NoSuchNamespaceException} when no namespace is found.
 ","/**
 * Return a default namespace for the catalog.
 * <p>
 * When this catalog is set as the current catalog, the namespace returned by this method will be
 * set as the current namespace.
 * <p>
 * The namespace returned by this method is not required to exist.
 *
 * @return a multi-part namespace
 */
 
/**
 * List top-level namespaces from the catalog.
 * <p>
 * If an object such as a table, view, or function exists, its parent namespaces must also exist
 * and must be returned by this discovery method. For example, if table a.b.t exists, this method
 * must return [""a""] in the result array.
 *
 * @return an array of multi-part namespace names
 */
 
/**
 * List namespaces in a namespace.
 * <p>
 * If an object such as a table, view, or function exists, its parent namespaces must also exist
 * and must be returned by this discovery method. For example, if table a.b.t exists, this method
 * invoked as listNamespaces([""a""]) must return [""a"", ""b""] in the result array.
 *
 * @param namespace a multi-part namespace
 * @return an array of multi-part namespace names
 * @throws NoSuchNamespaceException If the namespace does not exist (optional)
 */
 
/**
 * Test whether a namespace exists.
 * <p>
 * If an object such as a table, view, or function exists, its parent namespaces must also exist.
 * For example, if table a.b.t exists, this method invoked as namespaceExists([""a""]) or
 * namespaceExists([""a"", ""b""]) must return true.
 *
 * @param namespace a multi-part namespace
 * @return true if the namespace exists, false otherwise
 */
 
/**
 * Load metadata properties for a namespace.
 *
 * @param namespace a multi-part namespace
 * @return a string map of properties for the given namespace
 * @throws NoSuchNamespaceException If the namespace does not exist (optional)
 * @throws UnsupportedOperationException If namespace properties are not supported
 */
 
/**
 * Create a namespace in the catalog.
 *
 * @param namespace a multi-part namespace
 * @param metadata a string map of properties for the given namespace
 * @throws NamespaceAlreadyExistsException If the namespace already exists
 * @throws UnsupportedOperationException If create is not a supported operation
 */
 
/**
 * Apply a set of metadata changes to a namespace in the catalog.
 *
 * @param namespace a multi-part namespace
 * @param changes a collection of changes to apply to the namespace
 * @throws NoSuchNamespaceException If the namespace does not exist (optional)
 * @throws UnsupportedOperationException If namespace properties are not supported
 */
 
/**
 * Drop a namespace from the catalog.
 * <p>
 * This operation may be rejected by the catalog implementation if the namespace is not empty by
 * throwing {@link IllegalStateException}. If the catalog implementation does not support this
 * operation, it may throw {@link UnsupportedOperationException}.
 *
 * @param namespace a multi-part namespace
 * @return true if the namespace was dropped
 * @throws NoSuchNamespaceException If the namespace does not exist (optional)
 * @throws IllegalStateException If the namespace is not empty
 * @throws UnsupportedOperationException If drop is not a supported operation
 */
 
","{
    return new String[0];
} 
listNamespaces 
listNamespaces 
{
    try {
        loadNamespaceMetadata(namespace);
        return true;
    } catch (NoSuchNamespaceException e) {
        return false;
    }
} 
loadNamespaceMetadata 
createNamespace 
alterNamespace 
dropNamespace 
",,,,,11,732
DelegatingCatalogExtension.java,22,106,0.20754716981132076,"
 * A simple implementation of {@link CatalogExtension}, which implements all the catalog functions
 * by calling the built-in session catalog directly. This is created for convenience, so that users
 * only need to override some methods where they want to apply custom logic. For example, they can
 * override {@code createTable}, do something else before calling {@code super.createTable}.
 ",,,,,,,4,384
StagingTableCatalog.java,106,29,3.6551724137931036,"
 * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of
 * the a table before committing the table's metadata along with its contents in CREATE TABLE AS
 * SELECT or REPLACE TABLE AS SELECT operations.
 * <p>
 * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS
 * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE
 * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first
 * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via
 * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform
 * the write via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)}. However, if the
 * write operation fails, the catalog will have already dropped the table, and the planner cannot
 * roll back the dropping of the table.
 * <p>
 * If the catalog implements this plugin, the catalog can implement the methods to ""stage"" the
 * creation and the replacement of a table. After the table's
 * {@link BatchWrite#commit(WriterCommitMessage[])} is called,
 * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can
 * complete both the data write and the metadata swap operation atomically.
 ","/**
 * Stage the creation of a table, preparing it to be committed into the metastore.
 * <p>
 * When the table is committed, the contents of any writes performed by the Spark planner are
 * committed along with the metadata about the table passed into this method's arguments. If the
 * table exists when this method is called, the method should throw an exception accordingly. If
 * another process concurrently creates the table before this table's staged changes are
 * committed, an exception should be thrown by {@link StagedTable#commitStagedChanges()}.
 *
 * @param ident a table identifier
 * @param schema the schema of the new table, as a struct type
 * @param partitions transforms to use for partitioning data in the table
 * @param properties a string map of table properties
 * @return metadata for the new table
 * @throws TableAlreadyExistsException If a table or view already exists for the identifier
 * @throws UnsupportedOperationException If a requested partition transform is not supported
 * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)
 */
 
/**
 * Stage the replacement of a table, preparing it to be committed into the metastore when the
 * returned table's {@link StagedTable#commitStagedChanges()} is called.
 * <p>
 * When the table is committed, the contents of any writes performed by the Spark planner are
 * committed along with the metadata about the table passed into this method's arguments. If the
 * table exists, the metadata and the contents of this table replace the metadata and contents of
 * the existing table. If a concurrent process commits changes to the table's data or metadata
 * while the write is being performed but before the staged changes are committed, the catalog
 * can decide whether to move forward with the table replacement anyways or abort the commit
 * operation.
 * <p>
 * If the table does not exist, committing the staged changes should fail with
 * {@link NoSuchTableException}. This differs from the semantics of
 * {@link #stageCreateOrReplace(Identifier, StructType, Transform[], Map)}, which should create
 * the table in the data source if the table does not exist at the time of committing the
 * operation.
 *
 * @param ident a table identifier
 * @param schema the schema of the new table, as a struct type
 * @param partitions transforms to use for partitioning data in the table
 * @param properties a string map of table properties
 * @return metadata for the new table
 * @throws UnsupportedOperationException If a requested partition transform is not supported
 * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)
 * @throws NoSuchTableException If the table does not exist
 */
 
/**
 * Stage the creation or replacement of a table, preparing it to be committed into the metastore
 * when the returned table's {@link StagedTable#commitStagedChanges()} is called.
 * <p>
 * When the table is committed, the contents of any writes performed by the Spark planner are
 * committed along with the metadata about the table passed into this method's arguments. If the
 * table exists, the metadata and the contents of this table replace the metadata and contents of
 * the existing table. If a concurrent process commits changes to the table's data or metadata
 * while the write is being performed but before the staged changes are committed, the catalog
 * can decide whether to move forward with the table replacement anyways or abort the commit
 * operation.
 * <p>
 * If the table does not exist when the changes are committed, the table should be created in the
 * backing data source. This differs from the expected semantics of
 * {@link #stageReplace(Identifier, StructType, Transform[], Map)}, which should fail when
 * the staged changes are committed but the table doesn't exist at commit time.
 *
 * @param ident a table identifier
 * @param schema the schema of the new table, as a struct type
 * @param partitions transforms to use for partitioning data in the table
 * @param properties a string map of table properties
 * @return metadata for the new table
 * @throws UnsupportedOperationException If a requested partition transform is not supported
 * @throws NoSuchNamespaceException If the identifier namespace does not exist (optional)
 */
 
","stageCreate 
stageReplace 
stageCreateOrReplace 
",,,,,18,1327
Table.java,16,26,0.6153846153846154,"
 * An interface representing a logical structured data set of a data source. For example, the
 * implementation can be a directory on the file system, a topic of Kafka, or a table in the
 * catalog, etc.
 * <p>
 * This interface can mixin {@code SupportsRead} and {@code SupportsWrite} to provide data reading
 * and writing ability.
 * <p>
 * The default implementation of {@link #partitioning()} returns an empty array of partitions, and
 * the default implementation of {@link #properties()} returns an empty map. These should be
 * overridden by implementations that support partitioning and table properties.
 ","/**
 * A name to identify this table. Implementations should provide a meaningful name, like the
 * database and table name from catalog, or the location of files for this table.
 */
 
/**
 * Returns the schema of this table. If the table is not readable and doesn't have a schema, an
 * empty schema can be returned here.
 */
 
/**
 * Returns the physical partitioning of this table.
 */
 
/**
 * Returns the string map of table properties.
 */
 
/**
 * Returns the set of capabilities for this table.
 */
 
","name 
schema 
{
    return new Transform[0];
} 
{
    return Collections.emptyMap();
} 
capabilities 
",,,,,10,595
Catalogs.java,35,72,0.4861111111111111,,"/**
 * Load and configure a catalog by name.
 * <p>
 * This loads, instantiates, and initializes the catalog plugin for each call; it does not cache
 * or reuse instances.
 *
 * @param name a String catalog name
 * @param conf a SQLConf
 * @return an initialized CatalogPlugin
 * @throws CatalogNotFoundException if the plugin class cannot be found
 * @throws SparkException if the plugin class cannot be instantiated
 */
 
/**
 * Extracts a named catalog's configuration from a SQLConf.
 *
 * @param name a catalog name
 * @param conf a SQLConf
 * @return a case insensitive string map of options starting with spark.sql.catalog.(name).
 */
 
","{
    String pluginClassName;
    try {
        pluginClassName = conf.getConfString(""spark.sql.catalog."" + name);
    } catch (NoSuchElementException e) {
        throw new CatalogNotFoundException(String.format(""Catalog '%s' plugin class not found: spark.sql.catalog.%s is not defined"", name, name));
    }
    ClassLoader loader = Utils.getContextOrSparkClassLoader();
    try {
        Class<?> pluginClass = loader.loadClass(pluginClassName);
        if (!CatalogPlugin.class.isAssignableFrom(pluginClass)) {
            throw new SparkException(String.format(""Plugin class for catalog '%s' does not implement CatalogPlugin: %s"", name, pluginClassName));
        }
        CatalogPlugin plugin = CatalogPlugin.class.cast(pluginClass.getDeclaredConstructor().newInstance());
        plugin.initialize(name, catalogOptions(name, conf));
        return plugin;
    } catch (ClassNotFoundException e) {
        throw new SparkException(String.format(""Cannot find catalog plugin class for catalog '%s': %s"", name, pluginClassName));
    } catch (NoSuchMethodException e) {
        throw new SparkException(String.format(""Failed to find public no-arg constructor for catalog '%s': %s"", name, pluginClassName), e);
    } catch (IllegalAccessException e) {
        throw new SparkException(String.format(""Failed to call public no-arg constructor for catalog '%s': %s"", name, pluginClassName), e);
    } catch (InstantiationException e) {
        throw new SparkException(String.format(""Cannot instantiate abstract catalog plugin class for catalog '%s': %s"", name, pluginClassName), e.getCause());
    } catch (InvocationTargetException e) {
        throw new SparkException(String.format(""Failed during instantiating constructor for catalog '%s': %s"", name, pluginClassName), e.getCause());
    }
} 
{
    Map<String, String> allConfs = mapAsJavaMapConverter(conf.getAllConfs()).asJava();
    Pattern prefix = Pattern.compile(""^spark\\.sql\\.catalog\\."" + name + ""\\.(.+)"");
    HashMap<String, String> options = new HashMap<>();
    for (Map.Entry<String, String> entry : allConfs.entrySet()) {
        Matcher matcher = prefix.matcher(entry.getKey());
        if (matcher.matches() && matcher.groupCount() > 0) {
            options.put(matcher.group(1), entry.getValue());
        }
    }
    return new CaseInsensitiveStringMap(options);
} 
",,,,,1,0
SupportsRead.java,28,9,3.111111111111111,"
 * A mix-in interface of {@link Table}, to indicate that it's readable. This adds
 * {@link #newScanBuilder(CaseInsensitiveStringMap)} that is used to create a scan for batch,
 * micro-batch, or continuous processing.
 ","/**
 * Returns a {@link ScanBuilder} which can be used to build a {@link Scan}. Spark will call this
 * method to configure each data source scan.
 *
 * @param options The options for reading, which is an immutable case-insensitive
 *                string-to-string map.
 */
 
","newScanBuilder 
",,,,,3,213
SupportsDelete.java,35,7,5.0,"
 * A mix-in interface for {@link Table} delete support. Data sources can implement this
 * interface to provide the ability to delete data from tables that matches filter expressions.
 ","/**
 * Delete data from a data source table that matches filter expressions.
 * <p>
 * Rows are deleted from the data source iff all of the filter expressions match. That is, the
 * expressions must be interpreted as a set of filters that are ANDed together.
 * <p>
 * Implementations may reject a delete operation if the delete isn't possible without significant
 * effort. For example, partitioned data sources may reject deletes that do not filter by
 * partition columns because the filter may require rewriting files without deleted records.
 * To reject a delete implementations should throw {@link IllegalArgumentException} with a clear
 * error message that identifies which expression was rejected.
 *
 * @param filters filter expressions, used to select rows to delete when all expressions match
 * @throws IllegalArgumentException If the delete is rejected due to required effort
 */
 
","deleteWhere 
",,,,,2,181
SessionConfigSupport.java,27,6,4.5,"
 * A mix-in interface for {@link TableProvider}. Data sources can implement this interface to
 * propagate session configs with the specified key-prefix to all data source operations in this
 * session.
 ","/**
 * Key prefix of the session configs to propagate, which is usually the data source name. Spark
 * will extract all session configs that starts with `spark.datasource.$keyPrefix`, turn
 * `spark.datasource.$keyPrefix.xxx -&gt; yyy` into `xxx -&gt; yyy`, and propagate them to all
 * data source operations in this session.
 */
 
","keyPrefix 
",,,,,3,198
StagedTable.java,36,11,3.272727272727273,"
 * Represents a table which is staged for being committed to the metastore.
 * <p>
 * This is used to implement atomic CREATE TABLE AS SELECT and REPLACE TABLE AS SELECT queries. The
 * planner will create one of these via
 * {@link StagingTableCatalog#stageCreate(Identifier, StructType, Transform[], Map)} or
 * {@link StagingTableCatalog#stageReplace(Identifier, StructType, Transform[], Map)} to prepare the
 * table for being written to. This table should usually implement {@link SupportsWrite}. A new
 * writer will be constructed via {@link SupportsWrite#newWriteBuilder(CaseInsensitiveStringMap)},
 * and the write will be committed. The job concludes with a call to {@link #commitStagedChanges()},
 * at which point implementations are expected to commit the table's metadata into the metastore
 * along with the data that was written by the writes from the write builder this table created.
 ","/**
 * Finalize the creation or replacement of this table.
 */
 
/**
 * Abort the changes that were staged, both in metadata and from temporary outputs of this
 * table's writers.
 */
 
","commitStagedChanges 
abortStagedChanges 
",,,,,11,881
IdentifierImpl.java,19,54,0.35185185185185186,"
 *  An {@link Identifier} implementation.
 ",,,,,,,1,41
CatalogPlugin.java,48,9,5.333333333333333,"
 * A marker interface to provide a catalog implementation for Spark.
 * <p>
 * Implementations can provide catalog functions by implementing additional interfaces for tables,
 * views, and functions.
 * <p>
 * Catalog implementations must implement this marker interface to be loaded by
 * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the
 * required public no-arg constructor. After creating an instance, it will be configured by calling
 * {@link #initialize(String, CaseInsensitiveStringMap)}.
 * <p>
 * Catalog implementations are registered to a name by adding a configuration option to Spark:
 * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties
 * in the Spark configuration that share the catalog name prefix,
 * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive
 * string map of options in initialization with the prefix removed.
 * {@code name}, is also passed and is the catalog's name; in this case, ""catalog-name"".
 ","/**
 * Called to initialize configuration.
 * <p>
 * This method is called once, just after the provider is instantiated.
 *
 * @param name the name used to identify and load this catalog
 * @param options a case-insensitive string map of configuration
 */
 
/**
 * Called to get this catalog's name.
 * <p>
 * This method is only called after {@link #initialize(String, CaseInsensitiveStringMap)} is
 * called to pass the catalog's name.
 */
 
","initialize 
name 
",,,,,16,1032
TableProvider.java,44,12,3.6666666666666665,"
 * The base interface for v2 data sources which don't have a real catalog. Implementations must
 * have a public, 0-arg constructor.
 * <p>
 * Note that, TableProvider can only apply data operations to existing tables, like read, append,
 * delete, and overwrite. It does not support the operations that require metadata changes, like
 * create/drop tables.
 * <p>
 * The major responsibility of this interface is to return a {@link Table} for read/write.
 * </p>
 ","/**
 * Return a {@link Table} instance to do read/write with user-specified options.
 *
 * @param options the user-specified options that can identify a table, e.g. file path, Kafka
 *                topic name, etc. It's an immutable case-insensitive string-to-string map.
 */
 
/**
 * Return a {@link Table} instance to do read/write with user-specified schema and options.
 * <p>
 * By default this method throws {@link UnsupportedOperationException}, implementations should
 * override this method to handle user-specified schema.
 * </p>
 * @param options the user-specified options that can identify a table, e.g. file path, Kafka
 *                topic name, etc. It's an immutable case-insensitive string-to-string map.
 * @param schema the user-specified schema.
 * @throws UnsupportedOperationException
 */
 
","getTable 
{
    throw new UnsupportedOperationException(this.getClass().getSimpleName() + "" source does not support user-specified schema"");
} 
",,,,,9,447
TableChange.java,179,249,0.7188755020080321,"
 * TableChange subclasses represent requested changes to a table. These are passed to
 * {@link TableCatalog#alterTable}. For example,
 * <pre>
 *   import TableChange._
 *   val catalog = Catalogs.load(name)
 *   catalog.asTableCatalog.alterTable(ident,
 *       addColumn(""x"", IntegerType),
 *       renameColumn(""a"", ""b""),
 *       deleteColumn(""c"")
 *     )
 * </pre>
 |
   * A TableChange to set a table property.
   * <p>
   * If the property already exists, it must be replaced with the new value.
   |
   * A TableChange to remove a table property.
   * <p>
   * If the property does not exist, the change should succeed.
   |
   * A TableChange to add a field.
   * <p>
   * If the field already exists, the change must result in an {@link IllegalArgumentException}.
   * If the new field is nested and its parent does not exist or is not a struct, the change must
   * result in an {@link IllegalArgumentException}.
   |
   * A TableChange to rename a field.
   * <p>
   * The name is used to find the field to rename. The new name will replace the leaf field name.
   * For example, renameColumn(""a.b.c"", ""x"") should produce column a.b.x.
   * <p>
   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.
   |
   * A TableChange to update the type of a field.
   * <p>
   * The field names are used to find the field to update.
   * <p>
   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.
   |
   * A TableChange to update the comment of a field.
   * <p>
   * The field names are used to find the field to update.
   * <p>
   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.
   |
   * A TableChange to delete a field.
   * <p>
   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.
   ","/**
 * Create a TableChange for setting a table property.
 * <p>
 * If the property already exists, it will be replaced with the new value.
 *
 * @param property the property name
 * @param value the new property value
 * @return a TableChange for the addition
 */
 
/**
 * Create a TableChange for removing a table property.
 * <p>
 * If the property does not exist, the change will succeed.
 *
 * @param property the property name
 * @return a TableChange for the addition
 */
 
/**
 * Create a TableChange for adding an optional column.
 * <p>
 * If the field already exists, the change will result in an {@link IllegalArgumentException}.
 * If the new field is nested and its parent does not exist or is not a struct, the change will
 * result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the new column
 * @param dataType the new column's data type
 * @return a TableChange for the addition
 */
 
/**
 * Create a TableChange for adding a column.
 * <p>
 * If the field already exists, the change will result in an {@link IllegalArgumentException}.
 * If the new field is nested and its parent does not exist or is not a struct, the change will
 * result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the new column
 * @param dataType the new column's data type
 * @param isNullable whether the new column can contain null
 * @return a TableChange for the addition
 */
 
/**
 * Create a TableChange for adding a column.
 * <p>
 * If the field already exists, the change will result in an {@link IllegalArgumentException}.
 * If the new field is nested and its parent does not exist or is not a struct, the change will
 * result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the new column
 * @param dataType the new column's data type
 * @param isNullable whether the new column can contain null
 * @param comment the new field's comment string
 * @return a TableChange for the addition
 */
 
/**
 * Create a TableChange for renaming a field.
 * <p>
 * The name is used to find the field to rename. The new name will replace the leaf field name.
 * For example, renameColumn([""a"", ""b"", ""c""], ""x"") should produce column a.b.x.
 * <p>
 * If the field does not exist, the change will result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames the current field names
 * @param newName the new name
 * @return a TableChange for the rename
 */
 
/**
 * Create a TableChange for updating the type of a field that is nullable.
 * <p>
 * The field names are used to find the field to update.
 * <p>
 * If the field does not exist, the change will result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the column to update
 * @param newDataType the new data type
 * @return a TableChange for the update
 */
 
/**
 * Create a TableChange for updating the type of a field.
 * <p>
 * The field names are used to find the field to update.
 * <p>
 * If the field does not exist, the change will result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the column to update
 * @param newDataType the new data type
 * @return a TableChange for the update
 */
 
/**
 * Create a TableChange for updating the comment of a field.
 * <p>
 * The name is used to find the field to update.
 * <p>
 * If the field does not exist, the change will result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the column to update
 * @param newComment the new comment
 * @return a TableChange for the update
 */
 
/**
 * Create a TableChange for deleting a field.
 * <p>
 * If the field does not exist, the change will result in an {@link IllegalArgumentException}.
 *
 * @param fieldNames field names of the column to delete
 * @return a TableChange for the delete
 */
 
","{
    return new SetProperty(property, value);
} 
{
    return new RemoveProperty(property);
} 
{
    return new AddColumn(fieldNames, dataType, true, null);
} 
{
    return new AddColumn(fieldNames, dataType, isNullable, null);
} 
{
    return new AddColumn(fieldNames, dataType, isNullable, comment);
} 
{
    return new RenameColumn(fieldNames, newName);
} 
{
    return new UpdateColumnType(fieldNames, newDataType, true);
} 
{
    return new UpdateColumnType(fieldNames, newDataType, isNullable);
} 
{
    return new UpdateColumnComment(fieldNames, newComment);
} 
{
    return new DeleteColumn(fieldNames);
} 
",,,,,48,1780
NamespaceChange.java,55,34,1.6176470588235294,"
 * NamespaceChange subclasses represent requested changes to a namespace. These are passed to
 * {@link SupportsNamespaces#alterNamespace}. For example,
 * <pre>
 *   import NamespaceChange._
 *   val catalog = Catalogs.load(name)
 *   catalog.alterNamespace(ident,
 *       setProperty(""prop"", ""value""),
 *       removeProperty(""other_prop"")
 *     )
 * </pre>
 |
   * A NamespaceChange to set a namespace property.
   * <p>
   * If the property already exists, it must be replaced with the new value.
   |
   * A NamespaceChange to remove a namespace property.
   * <p>
   * If the property does not exist, the change should succeed.
   ","/**
 * Create a NamespaceChange for setting a namespace property.
 * <p>
 * If the property already exists, it will be replaced with the new value.
 *
 * @param property the property name
 * @param value the new property value
 * @return a NamespaceChange for the addition
 */
 
/**
 * Create a NamespaceChange for removing a namespace property.
 * <p>
 * If the property does not exist, the change will succeed.
 *
 * @param property the property name
 * @return a NamespaceChange for the addition
 */
 
","{
    return new SetProperty(property, value);
} 
{
    return new RemoveProperty(property);
} 
",,,,,18,605
SupportsReportPartitioning.java,26,7,3.7142857142857144,"
 * A mix in interface for {@link Scan}. Data sources can implement this interface to
 * report data partitioning and try to avoid shuffle at Spark side.
 *
 * Note that, when a {@link Scan} implementation creates exactly one {@link InputPartition},
 * Spark may avoid adding a shuffle even if the reader does not implement this interface.
 ","/**
 * Returns the output data partitioning that this reader guarantees.
 */
 
","outputPartitioning 
",,,,,5,330
PartitionReader.java,34,9,3.7777777777777777,"
 * A partition reader returned by {@link PartitionReaderFactory#createReader(InputPartition)} or
 * {@link PartitionReaderFactory#createColumnarReader(InputPartition)}. It's responsible for
 * outputting data for a RDD partition.
 *
 * Note that, Currently the type `T` can only be {@link org.apache.spark.sql.catalyst.InternalRow}
 * for normal data sources, or {@link org.apache.spark.sql.vectorized.ColumnarBatch} for columnar
 * data sources(whose {@link PartitionReaderFactory#supportColumnarReads(InputPartition)}
 * returns true).
 ","/**
 * Proceed to next record, returns false if there is no more records.
 *
 * @throws IOException if failure happens during disk/network IO like reading files.
 */
 
/**
 * Return the current record. This method should return same value until `next` is called.
 */
 
","next 
get 
",,,,,8,523
Scan.java,72,23,3.130434782608696,"
 * A logical representation of a data source scan. This interface is used to provide logical
 * information, like what the actual read schema is.
 * <p>
 * This logical representation is shared between batch scan, micro-batch streaming scan and
 * continuous streaming scan. Data sources must implement the corresponding methods in this
 * interface, to match what the table promises to support. For example, {@link #toBatch()} must be
 * implemented, if the {@link Table} that creates this {@link Scan} returns
 * {@link TableCapability#BATCH_READ} support in its {@link Table#capabilities()}.
 * </p>
 ","/**
 * Returns the actual schema of this data source scan, which may be different from the physical
 * schema of the underlying storage, as column pruning or other optimizations may happen.
 */
 
/**
 * A description string of this scan, which may includes information like: what filters are
 * configured for this scan, what's the value of some important options like path, etc. The
 * description doesn't need to include {@link #readSchema()}, as Spark already knows it.
 * <p>
 * By default this returns the class name of the implementation. Please override it to provide a
 * meaningful description.
 * </p>
 */
 
/**
 * Returns the physical representation of this scan for batch query. By default this method throws
 * exception, data sources must overwrite this method to provide an implementation, if the
 * {@link Table} that creates this scan returns {@link TableCapability#BATCH_READ} support in its
 * {@link Table#capabilities()}.
 *
 * @throws UnsupportedOperationException
 */
 
/**
 * Returns the physical representation of this scan for streaming query with micro-batch mode. By
 * default this method throws exception, data sources must overwrite this method to provide an
 * implementation, if the {@link Table} that creates this scan returns
 * {@link TableCapability#MICRO_BATCH_READ} support in its {@link Table#capabilities()}.
 *
 * @param checkpointLocation a path to Hadoop FS scratch space that can be used for failure
 *                           recovery. Data streams for the same logical source in the same query
 *                           will be given the same checkpointLocation.
 *
 * @throws UnsupportedOperationException
 */
 
/**
 * Returns the physical representation of this scan for streaming query with continuous mode. By
 * default this method throws exception, data sources must overwrite this method to provide an
 * implementation, if the {@link Table} that creates this scan returns
 * {@link TableCapability#CONTINUOUS_READ} support in its {@link Table#capabilities()}.
 *
 * @param checkpointLocation a path to Hadoop FS scratch space that can be used for failure
 *                           recovery. Data streams for the same logical source in the same query
 *                           will be given the same checkpointLocation.
 *
 * @throws UnsupportedOperationException
 */
 
","readSchema 
{
    return this.getClass().toString();
} 
{
    throw new UnsupportedOperationException(description() + "": Batch scan are not supported"");
} 
{
    throw new UnsupportedOperationException(description() + "": Micro-batch scan are not supported"");
} 
{
    throw new UnsupportedOperationException(description() + "": Continuous scan are not supported"");
} 
",,,,,9,586
Batch.java,52,200,0.26,"
 * A physical representation of a data source scan for batch queries. This interface is used to
 * provide physical information, like how many partitions the scanned data has, and how to read
 * records from the partitions.
 ","/**
 * Returns a list of {@link InputPartition input partitions}. Each {@link InputPartition}
 * represents a data split that can be processed by one Spark task. The number of input
 * partitions returned here is the same as the number of RDD partitions this scan outputs.
 * <p>
 * If the {@link Scan} supports filter pushdown, this Batch is likely configured with a filter
 * and is responsible for creating splits for that filter, which is not a full scan.
 * </p>
 * <p>
 * This method will be called only once during a data source scan, to launch one Spark job.
 * </p>
 */
 
/**
 * Returns a factory to create a {@link PartitionReader} for each {@link InputPartition}.
 */
 
","planInputPartitions 
createReaderFactory 
",,,,,3,219
PartitionReaderFactory.java,20,16,1.25,"
 * A factory used to create {@link PartitionReader} instances.
 *
 * If Spark fails to execute any methods in the implementations of this interface or in the returned
 * {@link PartitionReader} (by throwing an exception), corresponding Spark task would fail and
 * get retried until hitting the maximum retry times.
 ","/**
 * Returns a row-based partition reader to read data from the given {@link InputPartition}.
 *
 * Implementations probably need to cast the input partition to the concrete
 * {@link InputPartition} class defined for the data source.
 */
 
/**
 * Returns a columnar partition reader to read data from the given {@link InputPartition}.
 *
 * Implementations probably need to cast the input partition to the concrete
 * {@link InputPartition} class defined for the data source.
 */
 
/**
 * Returns true if the given {@link InputPartition} should be read by Spark in a columnar way.
 * This means, implementations must also implement {@link #createColumnarReader(InputPartition)}
 * for the input partitions that this method returns true.
 *
 * As of Spark 2.4, Spark can only read all input partition in a columnar way, or none of them.
 * Data source can't mix columnar and row-based partitions. This may be relaxed in future
 * versions.
 */
 
","createReader 
{
    throw new UnsupportedOperationException(""Cannot create columnar reader."");
} 
{
    return false;
} 
",,,,,5,307
SupportsReportStatistics.java,27,6,4.5,"
 * A mix in interface for {@link Scan}. Data sources can implement this interface to
 * report statistics to Spark.
 *
 * As of Spark 2.4, statistics are reported to the optimizer before any operator is pushed to the
 * data source. Implementations that return more accurate statistics based on pushed operators will
 * not improve query performance until the planner can push operators before getting stats.
 ","/**
 * Returns the estimated statistics of this data source scan.
 */
 
","estimateStatistics 
",,,,,6,398
InputPartition.java,16,10,1.6,"
 * A serializable representation of an input partition returned by
 * {@link Batch#planInputPartitions()} and the corresponding ones in streaming .
 *
 * Note that {@link InputPartition} will be serialized and sent to executors, then
 * {@link PartitionReader} will be created by
 * {@link PartitionReaderFactory#createReader(InputPartition)} or
 * {@link PartitionReaderFactory#createColumnarReader(InputPartition)} on executors to do
 * the actual reading. So {@link InputPartition} must be serializable while {@link PartitionReader}
 * doesn't need to be.
 ","/**
 * The preferred locations where the input partition reader returned by this partition can run
 * faster, but Spark does not guarantee to run the input partition reader on these locations.
 * The implementations should make sure that it can be run on any location.
 * The location is a string representing the host name.
 *
 * Note that if a host name cannot be recognized by Spark, it will be ignored as it was not in
 * the returned locations. The default return value is empty string array, which means this
 * input partition's reader has no location preference.
 *
 * If this method fails (by throwing an exception), the action will fail and no Spark job will be
 * submitted.
 */
 
","{
    return new String[0];
} 
",,,,,9,542
SupportsPushDownFilters.java,39,8,4.875,"
 * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to
 * push down filters to the data source and reduce the size of the data to be read.
 ","/**
 * Pushes down filters, and returns filters that need to be evaluated after scanning.
 * <p>
 * Rows should be returned from the data source if and only if all of the filters match. That is,
 * filters must be interpreted as ANDed together.
 */
 
/**
 * Returns the filters that are pushed to the data source via {@link #pushFilters(Filter[])}.
 *
 * There are 3 kinds of filters:
 *  1. pushable filters which don't need to be evaluated again after scanning.
 *  2. pushable filters which still need to be evaluated after scanning, e.g. parquet
 *     row group filter.
 *  3. non-pushable filters.
 * Both case 1 and 2 should be considered as pushed filters and should be returned by this method.
 *
 * It's possible that there is no filters in the query and {@link #pushFilters(Filter[])}
 * is never called, empty array should be returned for this case.
 */
 
","pushFilters 
pushedFilters 
",,,,,2,173
PartitionOffset.java,22,6,3.6666666666666665,"
 * Used for per-partition offsets in continuous processing. ContinuousReader implementations will
 * provide a method to merge these into a global Offset.
 *
 * These offsets must be serializable.
 ",,,,,,,4,190
MicroBatchStream.java,39,12,3.25,"
 * A {@link SparkDataStream} for streaming queries with micro-batch mode.
 ","/**
 * Returns the most recent offset available.
 */
 
/**
 * Returns a list of {@link InputPartition input partitions} given the start and end offsets. Each
 * {@link InputPartition} represents a data split that can be processed by one Spark task. The
 * number of input partitions returned here is the same as the number of RDD partitions this scan
 * outputs.
 * <p>
 * If the {@link Scan} supports filter pushdown, this stream is likely configured with a filter
 * and is responsible for creating splits for that filter, which is not a full scan.
 * </p>
 * <p>
 * This method will be called multiple times, to launch one Spark job for each micro-batch in this
 * data stream.
 * </p>
 */
 
/**
 * Returns a factory to create a {@link PartitionReader} for each {@link InputPartition}.
 */
 
","latestOffset 
planInputPartitions 
createReaderFactory 
",,,,,1,73
SparkDataStream.java,41,9,4.555555555555555,"
 * The base interface representing a readable data stream in a Spark streaming query. It's
 * responsible to manage the offsets of the streaming source in the streaming query.
 *
 * Data sources should implement concrete data stream interfaces:
 * {@link MicroBatchStream} and {@link ContinuousStream}.
 ","/**
 * Returns the initial offset for a streaming query to start reading from. Note that the
 * streaming data source should not assume that it will start reading from its initial offset:
 * if Spark is restarting an existing query, it will restart from the check-pointed offset rather
 * than the initial one.
 */
 
/**
 * Deserialize a JSON string into an Offset of the implementation-defined offset type.
 *
 * @throws IllegalArgumentException if the JSON does not encode a valid offset for this reader
 */
 
/**
 * Informs the source that Spark has completed processing all data for offsets less than or
 * equal to `end` and will only request offsets greater than `end` in the future.
 */
 
/**
 * Stop this source and free any resources it has allocated.
 */
 
","initialOffset 
deserializeOffset 
commit 
stop 
",,,,,5,294
Offset.java,27,29,0.9310344827586207,"
 * An abstract representation of progress through a {@link MicroBatchStream} or
 * {@link ContinuousStream}.
 * During execution, offsets provided by the data source implementation will be logged and used as
 * restart checkpoints. Each source should provide an offset implementation which the source can use
 * to reconstruct a position in the stream up to which data has been seen/processed.
 ","/**
 * A JSON-serialized representation of an Offset that is
 * used for saving offsets to the offset log.
 * Note: We assume that equivalent/equal offsets serialize to
 * identical JSON strings.
 *
 * @return JSON string encoding
 */
 
/**
 * Equality based on JSON string representation. We leverage the
 * JSON representation for normalization between the Offset's
 * in deserialized and serialized representations.
 */
 
","json 
{
    if (obj instanceof Offset) {
        return this.json().equals(((Offset) obj).json());
    } else {
        return false;
    }
} 
",,,,,5,385
ContinuousStream.java,50,13,3.8461538461538463,"
 * A {@link SparkDataStream} for streaming queries with continuous mode.
 ","/**
 * Returns a list of {@link InputPartition input partitions} given the start offset. Each
 * {@link InputPartition} represents a data split that can be processed by one Spark task. The
 * number of input partitions returned here is the same as the number of RDD partitions this scan
 * outputs.
 * <p>
 * If the {@link Scan} supports filter pushdown, this stream is likely configured with a filter
 * and is responsible for creating splits for that filter, which is not a full scan.
 * </p>
 * <p>
 * This method will be called to launch one Spark job for reading the data stream. It will be
 * called more than once, if {@link #needsReconfiguration()} returns true and Spark needs to
 * launch a new job.
 * </p>
 */
 
/**
 * Returns a factory to create a {@link ContinuousPartitionReader} for each
 * {@link InputPartition}.
 */
 
/**
 * Merge partitioned offsets coming from {@link ContinuousPartitionReader} instances
 * for each partition to a single global offset.
 */
 
/**
 * The execution engine will call this method in every epoch to determine if new input
 * partitions need to be generated, which may be required if for example the underlying
 * source system has had partitions added or removed.
 *
 * If true, the Spark job to scan this continuous data stream will be interrupted and Spark will
 * launch it again with a new list of {@link InputPartition input partitions}.
 */
 
","planInputPartitions 
createContinuousReaderFactory 
mergeOffsets 
{
    return false;
} 
",,,,,1,72
ContinuousPartitionReader.java,26,7,3.7142857142857144,"
 * A variation on {@link PartitionReader} for use with continuous streaming processing.
 ","/**
 * Get the offset of the current record, or the start offset if no records have been read.
 *
 * The execution engine will call this method along with get() to keep track of the current
 * offset. When an epoch ends, the offset of the previous record in each partition will be saved
 * as a restart checkpoint.
 */
 
","getOffset 
",,,,,1,87
ContinuousPartitionReaderFactory.java,20,16,1.25,"
 * A variation on {@link PartitionReaderFactory} that returns {@link ContinuousPartitionReader}
 * instead of {@link PartitionReader}. It's used for continuous streaming processing.
 ",,,,,,,2,179
Statistics.java,20,8,2.5,"
 * An interface to represent statistics for a data source, which is returned by
 * {@link SupportsReportStatistics#estimateStatistics()}.
 ",,,,,,,2,135
ScanBuilder.java,16,24,0.6666666666666666,"
 * An interface for building the {@link Scan}. Implementations can mixin SupportsPushDownXYZ
 * interfaces to do operator pushdown, and keep the operator pushdown result in the returned
 * {@link Scan}.
 ",,,,,,,3,198
SupportsPushDownRequiredColumns.java,31,7,4.428571428571429,"
 * A mix-in interface for {@link ScanBuilder}. Data sources can implement this
 * interface to push down required columns to the data source and only read these columns during
 * scan to reduce the size of the data to be read.
 ","/**
 * Applies column pruning w.r.t. the given requiredSchema.
 *
 * Implementation should try its best to prune the unnecessary columns or nested fields, but it's
 * also OK to do the pruning partially, e.g., a data source may not be able to prune nested
 * fields, and only prune top-level columns.
 *
 * Note that, {@link Scan#readSchema()} implementation should take care of the column
 * pruning applied here.
 */
 
","pruneColumns 
",,,,,3,222
ClusteredDistribution.java,24,10,2.4,"
 * A concrete implementation of {@link Distribution}. Represents a distribution where records that
 * share the same values for the {@link #clusteredColumns} will be produced by the same
 * {@link PartitionReader}.
 ",,,,,"/**
 * The names of the clustered columns. Note that they are order insensitive.
 */
 
","Field clusteredColumns
",3,210
Partitioning.java,34,9,3.7777777777777777,"
 * An interface to represent the output data partitioning for a data source, which is returned by
 * {@link SupportsReportPartitioning#outputPartitioning()}. Note that this should work
 * like a snapshot. Once created, it should be deterministic and always report the same number of
 * partitions and the same ""satisfy"" result for a certain distribution.
 ","/**
 * Returns the number of partitions(i.e., {@link InputPartition}s) the data source outputs.
 */
 
/**
 * Returns true if this partitioning can satisfy the given distribution, which means Spark does
 * not need to shuffle the output data of this data source for some certain operations.
 *
 * Note that, Spark may add new concrete implementations of {@link Distribution} in new releases.
 * This method should be aware of it and always return false for unrecognized distributions. It's
 * recommended to check every Spark new release and support new distributions if possible, to
 * avoid shuffle at Spark side for more cases.
 */
 
","numPartitions 
satisfy 
",,,,,4,348
Distribution.java,24,10,2.4,"
 * An interface to represent data distribution requirement, which specifies how the records should
 * be distributed among the data partitions (one {@link PartitionReader} outputs data for one
 * partition).
 * Note that this interface has nothing to do with the data ordering inside one
 * partition(the output records of a single {@link PartitionReader}).
 *
 * The instance of this interface is created and provided by Spark, then consumed by
 * {@link Partitioning#satisfy(Distribution)}. This means data source developers don't need to
 * implement this interface, but need to catch as more concrete implementations of this interface
 * as possible in {@link Partitioning#satisfy(Distribution)}.
 *
 * Concrete implementations until now:
 * <ul>
 *   <li>{@link ClusteredDistribution}</li>
 * </ul>
 ",,,,,,,15,775
Transform.java,31,8,3.875,"
 * Represents a transform function in the public logical expression API.
 * <p>
 * For example, the transform date(ts) is used to derive a date value from a timestamp column. The
 * transform name is ""date"" and its argument is a reference to the ""ts"" column.
 ","/**
 * Returns the transform function name.
 */
 
/**
 * Returns all field references in the transform arguments.
 */
 
/**
 * Returns the arguments passed to the transform function.
 */
 
","name 
references 
arguments 
",,,,,4,252
Expression.java,22,6,3.6666666666666665,"
 * Base class of the public logical expression API.
 ","/**
 * Format the expression as a human readable SQL-like string.
 */
 
","describe 
",,,,,1,51
Literal.java,30,8,3.75,"
 * Represents a constant literal value in the public expression API.
 * <p>
 * The JVM type of the value held by a literal must be the type used by Spark's InternalRow API for
 * the literal's {@link DataType SQL data type}.
 *
 * @param <T> the JVM type of a value held by the literal
 ","/**
 * Returns the literal value.
 */
 
/**
 * Returns the SQL data type of the literal.
 */
 
","value 
dataType 
",,,,,6,275
NamedReference.java,24,6,4.0,"
 * Represents a field or column reference in the public logical expression API.
 ","/**
 * Returns the referenced field name as an array of String parts.
 * <p>
 * Each string in the returned array represents a field name.
 */
 
","fieldNames 
",,,,,1,79
Expressions.java,109,39,2.7948717948717947,"
 * Helper methods to create logical transforms to pass into Spark.
 ","/**
 * Create a logical transform for applying a named transform.
 * <p>
 * This transform can represent applying any named transform.
 *
 * @param name the transform name
 * @param args expression arguments to the transform
 * @return a logical transform
 */
 
/**
 * Create a named reference expression for a column.
 *
 * @param name a column name
 * @return a named reference for the column
 */
 
/**
 * Create a literal from a value.
 * <p>
 * The JVM type of the value held by a literal must be the type used by Spark's InternalRow API
 * for the literal's {@link DataType SQL data type}.
 *
 * @param value a value
 * @param <T> the JVM type of the value
 * @return a literal expression for the value
 */
 
/**
 * Create a bucket transform for one or more columns.
 * <p>
 * This transform represents a logical mapping from a value to a bucket id in [0, numBuckets)
 * based on a hash of the value.
 * <p>
 * The name reported by transforms created with this method is ""bucket"".
 *
 * @param numBuckets the number of output buckets
 * @param columns input columns for the bucket transform
 * @return a logical bucket transform with name ""bucket""
 */
 
/**
 * Create an identity transform for a column.
 * <p>
 * This transform represents a logical mapping from a value to itself.
 * <p>
 * The name reported by transforms created with this method is ""identity"".
 *
 * @param column an input column
 * @return a logical identity transform with name ""identity""
 */
 
/**
 * Create a yearly transform for a timestamp or date column.
 * <p>
 * This transform represents a logical mapping from a timestamp or date to a year, such as 2018.
 * <p>
 * The name reported by transforms created with this method is ""years"".
 *
 * @param column an input timestamp or date column
 * @return a logical yearly transform with name ""years""
 */
 
/**
 * Create a monthly transform for a timestamp or date column.
 * <p>
 * This transform represents a logical mapping from a timestamp or date to a month, such as
 * 2018-05.
 * <p>
 * The name reported by transforms created with this method is ""months"".
 *
 * @param column an input timestamp or date column
 * @return a logical monthly transform with name ""months""
 */
 
/**
 * Create a daily transform for a timestamp or date column.
 * <p>
 * This transform represents a logical mapping from a timestamp or date to a date, such as
 * 2018-05-13.
 * <p>
 * The name reported by transforms created with this method is ""days"".
 *
 * @param column an input timestamp or date column
 * @return a logical daily transform with name ""days""
 */
 
/**
 * Create an hourly transform for a timestamp column.
 * <p>
 * This transform represents a logical mapping from a timestamp to a date and hour, such as
 * 2018-05-13, hour 19.
 * <p>
 * The name reported by transforms created with this method is ""hours"".
 *
 * @param column an input timestamp column
 * @return a logical hourly transform with name ""hours""
 */
 
","{
    return LogicalExpressions.apply(name, JavaConverters.asScalaBuffer(Arrays.asList(args)).toSeq());
} 
{
    return LogicalExpressions.reference(name);
} 
{
    return LogicalExpressions.literal(value);
} 
{
    return LogicalExpressions.bucket(numBuckets, JavaConverters.asScalaBuffer(Arrays.asList(columns)).toSeq());
} 
{
    return LogicalExpressions.identity(column);
} 
{
    return LogicalExpressions.years(column);
} 
{
    return LogicalExpressions.months(column);
} 
{
    return LogicalExpressions.days(column);
} 
{
    return LogicalExpressions.hours(column);
} 
",,,,,1,66
ArrowColumnVector.java,30,348,0.08620689655172414,"
 * A column vector backed by Apache Arrow. Currently calendar interval type and map type are not
 * supported.
 |
   * Any call to ""get"" method will throw UnsupportedOperationException.
   *
   * Access struct values in a ArrowColumnVector doesn't use this accessor. Instead, it uses
   * getStruct() method defined in the parent class. Any call to ""get"" method in this class is a
   * bug in the code.
   *
   ",,,,,,,9,394
ColumnarRow.java,23,216,0.10648148148148148,"
 * Row abstraction in {@link ColumnVector}.
 ","/**
 * Revisit this. This is expensive. This is currently only used in test paths.
 */
 
","{
    GenericInternalRow row = new GenericInternalRow(numFields);
    for (int i = 0; i < numFields(); i++) {
        if (isNullAt(i)) {
            row.setNullAt(i);
        } else {
            DataType dt = data.getChild(i).dataType();
            if (dt instanceof BooleanType) {
                row.setBoolean(i, getBoolean(i));
            } else if (dt instanceof ByteType) {
                row.setByte(i, getByte(i));
            } else if (dt instanceof ShortType) {
                row.setShort(i, getShort(i));
            } else if (dt instanceof IntegerType) {
                row.setInt(i, getInt(i));
            } else if (dt instanceof LongType) {
                row.setLong(i, getLong(i));
            } else if (dt instanceof FloatType) {
                row.setFloat(i, getFloat(i));
            } else if (dt instanceof DoubleType) {
                row.setDouble(i, getDouble(i));
            } else if (dt instanceof StringType) {
                row.update(i, getUTF8String(i).copy());
            } else if (dt instanceof BinaryType) {
                row.update(i, getBinary(i));
            } else if (dt instanceof DecimalType) {
                DecimalType t = (DecimalType) dt;
                row.setDecimal(i, getDecimal(i, t.precision(), t.scale()), t.precision());
            } else if (dt instanceof DateType) {
                row.setInt(i, getInt(i));
            } else if (dt instanceof TimestampType) {
                row.setLong(i, getLong(i));
            } else {
                throw new RuntimeException(""Not implemented. "" + dt);
            }
        }
    }
    return row;
} 
",,,,,1,43
ColumnVector.java,223,423,0.5271867612293144,"
 * An interface representing in-memory columnar data in Spark. This interface defines the main APIs
 * to access the data, as well as their batched versions. The batched versions are considered to be
 * faster and preferable whenever possible.
 *
 * Most of the APIs take the rowId as a parameter. This is the batch local 0-based row id for values
 * in this ColumnVector.
 *
 * Spark only calls specific `get` method according to the data type of this {@link ColumnVector},
 * e.g. if it's int type, Spark is guaranteed to only call {@link #getInt(int)} or
 * {@link #getInts(int, int)}.
 *
 * ColumnVector supports all the data types including nested types. To handle nested types,
 * ColumnVector can have children and is a tree structure. Please refer to {@link #getStruct(int)},
 * {@link #getArray(int)} and {@link #getMap(int)} for the details about how to implement nested
 * types.
 *
 * ColumnVector is expected to be reused during the entire data loading process, to avoid allocating
 * memory again and again.
 *
 * ColumnVector is meant to maximize CPU efficiency but not to minimize storage footprint.
 * Implementations should prefer computing efficiency over storage efficiency when design the
 * format. Since it is expected to reuse the ColumnVector instance while loading data, the storage
 * footprint is negligible.
 ","/**
 * Returns the data type of this column vector.
 */
 
/**
 * Cleans up memory for this column vector. The column vector is not usable after this.
 *
 * This overwrites `AutoCloseable.close` to remove the `throws` clause, as column vector is
 * in-memory and we don't expect any exception to happen during closing.
 */
 
/**
 * Returns true if this column vector contains any null values.
 */
 
/**
 * Returns the number of nulls in this column vector.
 */
 
/**
 * Returns whether the value at rowId is NULL.
 */
 
/**
 * Returns the boolean type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets boolean type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the byte type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets byte type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the short type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets short type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the int type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets int type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the long type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets long type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the float type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets float type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the double type value for rowId. The return value is undefined and can be anything,
 * if the slot for rowId is null.
 */
 
/**
 * Gets double type values from [rowId, rowId + count). The return values for the null slots
 * are undefined and can be anything.
 */
 
/**
 * Returns the struct type value for rowId. If the slot for rowId is null, it should return null.
 *
 * To support struct type, implementations must implement {@link #getChild(int)} and make this
 * vector a tree structure. The number of child vectors must be same as the number of fields of
 * the struct type, and each child vector is responsible to store the data for its corresponding
 * struct field.
 */
 
/**
 * Returns the array type value for rowId. If the slot for rowId is null, it should return null.
 *
 * To support array type, implementations must construct an {@link ColumnarArray} and return it in
 * this method. {@link ColumnarArray} requires a {@link ColumnVector} that stores the data of all
 * the elements of all the arrays in this vector, and an offset and length which points to a range
 * in that {@link ColumnVector}, and the range represents the array for rowId. Implementations
 * are free to decide where to put the data vector and offsets and lengths. For example, we can
 * use the first child vector as the data vector, and store offsets and lengths in 2 int arrays in
 * this vector.
 */
 
/**
 * Returns the map type value for rowId. If the slot for rowId is null, it should return null.
 *
 * In Spark, map type value is basically a key data array and a value data array. A key from the
 * key array with a index and a value from the value array with the same index contribute to
 * an entry of this map type value.
 *
 * To support map type, implementations must construct a {@link ColumnarMap} and return it in
 * this method. {@link ColumnarMap} requires a {@link ColumnVector} that stores the data of all
 * the keys of all the maps in this vector, and another {@link ColumnVector} that stores the data
 * of all the values of all the maps in this vector, and a pair of offset and length which
 * specify the range of the key/value array that belongs to the map type value at rowId.
 */
 
/**
 * Returns the decimal type value for rowId. If the slot for rowId is null, it should return null.
 */
 
/**
 * Returns the string type value for rowId. If the slot for rowId is null, it should return null.
 * Note that the returned UTF8String may point to the data of this column vector, please copy it
 * if you want to keep it after this column vector is freed.
 */
 
/**
 * Returns the binary type value for rowId. If the slot for rowId is null, it should return null.
 */
 
/**
 * Returns the calendar interval type value for rowId. If the slot for rowId is null, it should
 * return null.
 *
 * In Spark, calendar interval type value is basically an integer value representing the number of
 * months in this interval, and a long value representing the number of microseconds in this
 * interval. An interval type vector is the same as a struct type vector with 2 fields: `months`
 * and `microseconds`.
 *
 * To support interval type, implementations must implement {@link #getChild(int)} and define 2
 * child vectors: the first child vector is an int type vector, containing all the month values of
 * all the interval values in this vector. The second child vector is a long type vector,
 * containing all the microsecond values of all the interval values in this vector.
 */
 
/**
 * @return child [[ColumnVector]] at the given ordinal.
 */
 
","{
    return type;
} 
close 
hasNull 
numNulls 
isNullAt 
getBoolean 
{
    boolean[] res = new boolean[count];
    for (int i = 0; i < count; i++) {
        res[i] = getBoolean(rowId + i);
    }
    return res;
} 
getByte 
{
    byte[] res = new byte[count];
    for (int i = 0; i < count; i++) {
        res[i] = getByte(rowId + i);
    }
    return res;
} 
getShort 
{
    short[] res = new short[count];
    for (int i = 0; i < count; i++) {
        res[i] = getShort(rowId + i);
    }
    return res;
} 
getInt 
{
    int[] res = new int[count];
    for (int i = 0; i < count; i++) {
        res[i] = getInt(rowId + i);
    }
    return res;
} 
getLong 
{
    long[] res = new long[count];
    for (int i = 0; i < count; i++) {
        res[i] = getLong(rowId + i);
    }
    return res;
} 
getFloat 
{
    float[] res = new float[count];
    for (int i = 0; i < count; i++) {
        res[i] = getFloat(rowId + i);
    }
    return res;
} 
getDouble 
{
    double[] res = new double[count];
    for (int i = 0; i < count; i++) {
        res[i] = getDouble(rowId + i);
    }
    return res;
} 
{
    if (isNullAt(rowId))
        return null;
    return new ColumnarRow(this, rowId);
} 
getArray 
getMap 
getDecimal 
getUTF8String 
getBinary 
{
    if (isNullAt(rowId))
        return null;
    final int months = getChild(0).getInt(rowId);
    final long microseconds = getChild(1).getLong(rowId);
    return new CalendarInterval(months, microseconds);
} 
getChild 
","/**
 * Sets up the data type of this column vector.
 */
 
","{
    this.type = type;
} 
","/**
 * Data type for this column.
 */
 
","Field type
",23,1292
ColumnarArray.java,22,132,0.16666666666666666,"
 * Array abstraction in {@link ColumnVector}.
 ",,,,,,,1,45
ColumnarMap.java,19,26,0.7307692307692307,"
 * Map abstraction in {@link ColumnVector}.
 ",,,,,,,1,43
ColumnarBatch.java,52,200,0.26,"
 * This class wraps multiple ColumnVectors as a row-wise table. It provides a row view of this
 * batch so that Spark can access the data row by row. Instance of it is meant to be reused during
 * the entire data loading process.
 |
 * An internal class, which wraps an array of {@link ColumnVector} and provides a row view.
 ","/**
 * Called to close all the columns in this batch. It is not valid to access the data after
 * calling this. This must be called at the end to clean up memory allocations.
 */
 
/**
 * Returns an iterator over the rows in this batch.
 */
 
/**
 * Sets the number of rows in this batch.
 */
 
/**
 * Returns the number of columns that make up this batch.
 */
 
/**
 * Returns the number of rows for read, including filtered rows.
 */
 
/**
 * Returns the column at `ordinal`.
 */
 
/**
 * Returns the row in this batch at `rowId`. Returned row is reused across calls.
 */
 
","{
    for (ColumnVector c : columns) {
        c.close();
    }
} 
{
    final int maxRows = numRows;
    final ColumnarBatchRow row = new ColumnarBatchRow(columns);
    return new Iterator<InternalRow>() {

        int rowId = 0;

        @Override
        public boolean hasNext() {
            return rowId < maxRows;
        }

        @Override
        public InternalRow next() {
            if (rowId >= maxRows) {
                throw new NoSuchElementException();
            }
            row.rowId = rowId++;
            return row;
        }

        @Override
        public void remove() {
            throw new UnsupportedOperationException();
        }
    };
} 
{
    this.numRows = numRows;
} 
{
    return columns.length;
} 
{
    return numRows;
} 
{
    return columns[ordinal];
} 
{
    assert (rowId >= 0 && rowId < numRows);
    row.rowId = rowId;
    return row;
} 
","/**
 * Create a new batch from existing column vectors.
 * @param columns The columns of this batch
 * @param numRows The number of rows in this batch
 */
 
","{
    this.columns = columns;
    this.numRows = numRows;
    this.row = new ColumnarBatchRow(columns);
} 
",,,5,317
RowFactory.java,27,9,3.0,"
 * A factory class used to construct {@link Row} objects.
 *
 * @since 1.3.0
 ","/**
 * Create a {@link Row} from the given arguments. Position i in the argument list becomes
 * position i in the created {@link Row} object.
 *
 * @since 1.3.0
 */
 
","{
    return new GenericRow(values);
} 
",,,,,3,72
Complex.java,45,1004,0.044820717131474105," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field aint is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field aString is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field lint is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field lString is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field lintString is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field mStringString is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // AINT
        1:
            return AINT;
        case // A_STRING
        2:
            return A_STRING;
        case // LINT
        3:
            return LINT;
        case // L_STRING
        4:
            return L_STRING;
        case // LINT_STRING
        5:
            return LINT_STRING;
        case // M_STRING_STRING
        6:
            return M_STRING_STRING;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __AINT_ISSET_ID);
} 
{
    return this.aString != null;
} 
{
    return this.lint != null;
} 
{
    return this.lString != null;
} 
{
    return this.lintString != null;
} 
{
    return this.mStringString != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case AINT:
            return isSetAint();
        case A_STRING:
            return isSetAString();
        case LINT:
            return isSetLint();
        case L_STRING:
            return isSetLString();
        case LINT_STRING:
            return isSetLintString();
        case M_STRING_STRING:
            return isSetMStringString();
        default:
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.aint = other.aint;
    if (other.isSetAString()) {
        this.aString = other.aString;
    }
    if (other.isSetLint()) {
        List<Integer> __this__lint = new ArrayList<>();
        for (Integer other_element : other.lint) {
            __this__lint.add(other_element);
        }
        this.lint = __this__lint;
    }
    if (other.isSetLString()) {
        List<String> __this__lString = new ArrayList<>();
        for (String other_element : other.lString) {
            __this__lString.add(other_element);
        }
        this.lString = __this__lString;
    }
    if (other.isSetLintString()) {
        List<IntString> __this__lintString = new ArrayList<>();
        for (IntString other_element : other.lintString) {
            __this__lintString.add(new IntString(other_element));
        }
        this.lintString = __this__lintString;
    }
    if (other.isSetMStringString()) {
        Map<String, String> __this__mStringString = new HashMap<>();
        for (Map.Entry<String, String> other_element : other.mStringString.entrySet()) {
            String other_element_key = other_element.getKey();
            String other_element_value = other_element.getValue();
            String __this__mStringString_copy_key = other_element_key;
            String __this__mStringString_copy_value = other_element_value;
            __this__mStringString.put(__this__mStringString_copy_key, __this__mStringString_copy_value);
        }
        this.mStringString = __this__mStringString;
    }
} 
",,,1,107
JavaDataFrameSuite.java,37,412,0.08980582524271845,,,,,,,,1,0
UDFToListInt.java,16,9,1.7777777777777777,,,,,,,,1,0
UDFToListMapStringListInt.java,20,11,1.8181818181818181,"
 * UDF that returns a nested list of maps that uses a string as its key and a list of ints as its
 * values.
 ",,,,,,,2,106
UDFRawMap.java,19,10,1.9,"
 * UDF that returns a raw (non-parameterized) java Map.
 ",,,,,,,1,55
UDFWildcardList.java,19,9,2.111111111111111,"
 * UDF that returns a raw (non-parameterized) java List.
 ",,,,,,,1,56
UDFToIntIntMap.java,16,15,1.0666666666666667,,,,,,,,1,0
UDFListString.java,16,14,1.1428571428571428,,,,,,,,1,0
UDFStringString.java,16,7,2.2857142857142856,,,,,,,,1,0
UDFIntegerToString.java,16,7,2.2857142857142856,,,,,,,,1,0
UDFTwoListList.java,16,8,2.0,,,,,,,,1,0
UDAFEmpty.java,19,11,1.7272727272727273,"
 * An empty UDAF that throws a semantic exception
 ",,,,,,,1,49
UDFToStringIntMap.java,16,14,1.1428571428571428,,,,,,,,1,0
UDFListListInt.java,21,22,0.9545454545454546,,"/**
 * @param obj
 *   SQL schema: array&lt;struct&lt;x: int, y: int, z: int&gt;&gt;
 *   Java Type: List&lt;List&lt;Integer&gt;&gt;
 */
 
","{
    if (obj == null) {
        return 0L;
    }
    List<List<?>> listList = (List<List<?>>) obj;
    long retVal = 0;
    for (List<?> aList : listList) {
        Number someInt = (Number) aList.get(1);
        try {
            retVal += someInt.longValue();
        } catch (NullPointerException e) {
            System.out.println(e);
        }
    }
    return retVal;
} 
",,,,,1,0
UDFToListString.java,16,9,1.7777777777777777,,,,,,,,1,0
UDFRawList.java,19,10,1.9,"
 * UDF that returns a raw (non-parameterized) java List.
 ",,,,,,,1,56
JavaMetastoreDataSourcesSuite.java,17,77,0.22077922077922077,,,,,,,,1,0
SparkOrcNewRecordReader.java,22,62,0.3548387096774194,"
 * This is based on hive-exec-1.2.1
 * {@link org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader}.
 * This class exposes getObjectInspector which can be used for reducing
 * NameNode calls in OrcRelation.
 ",,,,,,,4,214
TOperationState.java,13,46,0.2826086956521739,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return INITIALIZED_STATE;
        case 1:
            return RUNNING_STATE;
        case 2:
            return FINISHED_STATE;
        case 3:
            return CANCELED_STATE;
        case 4:
            return CLOSED_STATE;
        case 5:
            return ERROR_STATE;
        case 6:
            return UKNOWN_STATE;
        case 7:
            return PENDING_STATE;
        case 8:
            return TIMEDOUT_STATE;
        default:
            return null;
    }
} 
",,,,,1,0
TGetDelegationTokenResp.java,26,405,0.06419753086419754," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TGetCatalogsReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TExecuteStatementReq.java,30,726,0.04132231404958678," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field statement is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field confOverlay is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field runAsync is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field queryTimeout is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // STATEMENT
        2:
            return STATEMENT;
        case // CONF_OVERLAY
        3:
            return CONF_OVERLAY;
        case // RUN_ASYNC
        4:
            return RUN_ASYNC;
        case // QUERY_TIMEOUT
        5:
            return QUERY_TIMEOUT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.statement != null;
} 
{
    return this.confOverlay != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __RUNASYNC_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __QUERYTIMEOUT_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case STATEMENT:
            return isSetStatement();
        case CONF_OVERLAY:
            return isSetConfOverlay();
        case RUN_ASYNC:
            return isSetRunAsync();
        case QUERY_TIMEOUT:
            return isSetQueryTimeout();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetStatement()) {
        this.statement = other.statement;
    }
    if (other.isSetConfOverlay()) {
        Map<String, String> __this__confOverlay = new HashMap<String, String>(other.confOverlay);
        this.confOverlay = __this__confOverlay;
    }
    this.runAsync = other.runAsync;
    this.queryTimeout = other.queryTimeout;
} 
",,,1,107
TCloseSessionResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TI32Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TGetColumnsReq.java,29,693,0.04184704184704185," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field columnName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // TABLE_NAME
        4:
            return TABLE_NAME;
        case // COLUMN_NAME
        5:
            return COLUMN_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.tableName != null;
} 
{
    return this.columnName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case TABLE_NAME:
            return isSetTableName();
        case COLUMN_NAME:
            return isSetColumnName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetTableName()) {
        this.tableName = other.tableName;
    }
    if (other.isSetColumnName()) {
        this.columnName = other.columnName;
    }
} 
",,,1,107
TGetFunctionsReq.java,28,591,0.047377326565143825," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field functionName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // FUNCTION_NAME
        4:
            return FUNCTION_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.functionName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case FUNCTION_NAME:
            return isSetFunctionName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetFunctionName()) {
        this.functionName = other.functionName;
    }
} 
",,,1,107
TGetFunctionsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TI64Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TGetCrossReferenceReq.java,31,885,0.03502824858757062," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field parentCatalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field parentSchemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field parentTableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field foreignCatalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field foreignSchemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field foreignTableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // PARENT_CATALOG_NAME
        2:
            return PARENT_CATALOG_NAME;
        case // PARENT_SCHEMA_NAME
        3:
            return PARENT_SCHEMA_NAME;
        case // PARENT_TABLE_NAME
        4:
            return PARENT_TABLE_NAME;
        case // FOREIGN_CATALOG_NAME
        5:
            return FOREIGN_CATALOG_NAME;
        case // FOREIGN_SCHEMA_NAME
        6:
            return FOREIGN_SCHEMA_NAME;
        case // FOREIGN_TABLE_NAME
        7:
            return FOREIGN_TABLE_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.parentCatalogName != null;
} 
{
    return this.parentSchemaName != null;
} 
{
    return this.parentTableName != null;
} 
{
    return this.foreignCatalogName != null;
} 
{
    return this.foreignSchemaName != null;
} 
{
    return this.foreignTableName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case PARENT_CATALOG_NAME:
            return isSetParentCatalogName();
        case PARENT_SCHEMA_NAME:
            return isSetParentSchemaName();
        case PARENT_TABLE_NAME:
            return isSetParentTableName();
        case FOREIGN_CATALOG_NAME:
            return isSetForeignCatalogName();
        case FOREIGN_SCHEMA_NAME:
            return isSetForeignSchemaName();
        case FOREIGN_TABLE_NAME:
            return isSetForeignTableName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetParentCatalogName()) {
        this.parentCatalogName = other.parentCatalogName;
    }
    if (other.isSetParentSchemaName()) {
        this.parentSchemaName = other.parentSchemaName;
    }
    if (other.isSetParentTableName()) {
        this.parentTableName = other.parentTableName;
    }
    if (other.isSetForeignCatalogName()) {
        this.foreignCatalogName = other.foreignCatalogName;
    }
    if (other.isSetForeignSchemaName()) {
        this.foreignSchemaName = other.foreignSchemaName;
    }
    if (other.isSetForeignTableName()) {
        this.foreignTableName = other.foreignTableName;
    }
} 
",,,1,107
TSessionHandle.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionId is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_ID
        1:
            return SESSION_ID;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionId != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_ID:
            return isSetSessionId();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionId()) {
        this.sessionId = new THandleIdentifier(other.sessionId);
    }
} 
",,,1,107
TGetInfoResp.java,26,397,0.0654911838790932," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field infoValue is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // INFO_VALUE
        2:
            return INFO_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.infoValue != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case INFO_VALUE:
            return isSetInfoValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetInfoValue()) {
        this.infoValue = new TGetInfoValue(other.infoValue);
    }
} 
",,,1,107
TOperationHandle.java,41,575,0.07130434782608695," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationId is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TOperationType
 */
 
/**
 * @see TOperationType
 */
 
/**
 * Returns true if field operationType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field hasResultSet is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field modifiedRowCount is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_ID
        1:
            return OPERATION_ID;
        case // OPERATION_TYPE
        2:
            return OPERATION_TYPE;
        case // HAS_RESULT_SET
        3:
            return HAS_RESULT_SET;
        case // MODIFIED_ROW_COUNT
        4:
            return MODIFIED_ROW_COUNT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationId != null;
} 
{
    return this.operationType;
} 
{
    this.operationType = operationType;
} 
{
    return this.operationType != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __HASRESULTSET_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __MODIFIEDROWCOUNT_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_ID:
            return isSetOperationId();
        case OPERATION_TYPE:
            return isSetOperationType();
        case HAS_RESULT_SET:
            return isSetHasResultSet();
        case MODIFIED_ROW_COUNT:
            return isSetModifiedRowCount();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetOperationId()) {
        this.operationId = new THandleIdentifier(other.operationId);
    }
    if (other.isSetOperationType()) {
        this.operationType = other.operationType;
    }
    this.hasResultSet = other.hasResultSet;
    this.modifiedRowCount = other.modifiedRowCount;
} 
",,,1,107
TGetSchemasReq.java,27,501,0.05389221556886228," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
} 
",,,1,107
TFetchResultsReq.java,41,576,0.07118055555555555," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TFetchOrientation
 */
 
/**
 * @see TFetchOrientation
 */
 
/**
 * Returns true if field orientation is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field maxRows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field fetchType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        case // ORIENTATION
        2:
            return ORIENTATION;
        case // MAX_ROWS
        3:
            return MAX_ROWS;
        case // FETCH_TYPE
        4:
            return FETCH_TYPE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    return this.orientation;
} 
{
    this.orientation = orientation;
} 
{
    return this.orientation != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __MAXROWS_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __FETCHTYPE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
        case ORIENTATION:
            return isSetOrientation();
        case MAX_ROWS:
            return isSetMaxRows();
        case FETCH_TYPE:
            return isSetFetchType();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
    if (other.isSetOrientation()) {
        this.orientation = other.orientation;
    }
    this.maxRows = other.maxRows;
    this.fetchType = other.fetchType;
} 
",,,1,107
TStatusCode.java,13,34,0.38235294117647056,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return SUCCESS_STATUS;
        case 1:
            return SUCCESS_WITH_INFO_STATUS;
        case 2:
            return STILL_EXECUTING_STATUS;
        case 3:
            return ERROR_STATUS;
        case 4:
            return INVALID_HANDLE_STATUS;
        default:
            return null;
    }
} 
",,,,,1,0
TGetTableTypesResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TGetCrossReferenceResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TGetCatalogsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TTypeId.java,13,85,0.15294117647058825,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return BOOLEAN_TYPE;
        case 1:
            return TINYINT_TYPE;
        case 2:
            return SMALLINT_TYPE;
        case 3:
            return INT_TYPE;
        case 4:
            return BIGINT_TYPE;
        case 5:
            return FLOAT_TYPE;
        case 6:
            return DOUBLE_TYPE;
        case 7:
            return STRING_TYPE;
        case 8:
            return TIMESTAMP_TYPE;
        case 9:
            return BINARY_TYPE;
        case 10:
            return ARRAY_TYPE;
        case 11:
            return MAP_TYPE;
        case 12:
            return STRUCT_TYPE;
        case 13:
            return UNION_TYPE;
        case 14:
            return USER_DEFINED_TYPE;
        case 15:
            return DECIMAL_TYPE;
        case 16:
            return NULL_TYPE;
        case 17:
            return DATE_TYPE;
        case 18:
            return VARCHAR_TYPE;
        case 19:
            return CHAR_TYPE;
        case 20:
            return INTERVAL_YEAR_MONTH_TYPE;
        case 21:
            return INTERVAL_DAY_TIME_TYPE;
        default:
            return null;
    }
} 
",,,,,1,0
TGetDelegationTokenReq.java,27,485,0.05567010309278351," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field owner is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field renewer is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // OWNER
        2:
            return OWNER;
        case // RENEWER
        3:
            return RENEWER;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.owner != null;
} 
{
    return this.renewer != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case OWNER:
            return isSetOwner();
        case RENEWER:
            return isSetRenewer();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetOwner()) {
        this.owner = other.owner;
    }
    if (other.isSetRenewer()) {
        this.renewer = other.renewer;
    }
} 
",,,1,107
TI64Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Long> __this__values = new ArrayList<Long>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TCloseSessionReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TCloseOperationReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TProgressUpdateResp.java,43,870,0.04942528735632184," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field headerNames is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field rows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field progressedPercentage is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TJobExecutionStatus
 */
 
/**
 * @see TJobExecutionStatus
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field footerSummary is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field startTime is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // HEADER_NAMES
        1:
            return HEADER_NAMES;
        case // ROWS
        2:
            return ROWS;
        case // PROGRESSED_PERCENTAGE
        3:
            return PROGRESSED_PERCENTAGE;
        case // STATUS
        4:
            return STATUS;
        case // FOOTER_SUMMARY
        5:
            return FOOTER_SUMMARY;
        case // START_TIME
        6:
            return START_TIME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.headerNames != null;
} 
{
    return this.rows != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __PROGRESSEDPERCENTAGE_ISSET_ID);
} 
{
    return this.status;
} 
{
    this.status = status;
} 
{
    return this.status != null;
} 
{
    return this.footerSummary != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __STARTTIME_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case HEADER_NAMES:
            return isSetHeaderNames();
        case ROWS:
            return isSetRows();
        case PROGRESSED_PERCENTAGE:
            return isSetProgressedPercentage();
        case STATUS:
            return isSetStatus();
        case FOOTER_SUMMARY:
            return isSetFooterSummary();
        case START_TIME:
            return isSetStartTime();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetHeaderNames()) {
        List<String> __this__headerNames = new ArrayList<String>(other.headerNames);
        this.headerNames = __this__headerNames;
    }
    if (other.isSetRows()) {
        List<List<String>> __this__rows = new ArrayList<List<String>>(other.rows.size());
        for (List<String> other_element : other.rows) {
            List<String> __this__rows_copy = new ArrayList<String>(other_element);
            __this__rows.add(__this__rows_copy);
        }
        this.rows = __this__rows;
    }
    this.progressedPercentage = other.progressedPercentage;
    if (other.isSetStatus()) {
        this.status = other.status;
    }
    if (other.isSetFooterSummary()) {
        this.footerSummary = other.footerSummary;
    }
    this.startTime = other.startTime;
} 
",,,1,107
TColumn.java,17,642,0.0264797507788162," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // BOOL_VAL
        1:
            return BOOL_VAL;
        case // BYTE_VAL
        2:
            return BYTE_VAL;
        case // I16_VAL
        3:
            return I16_VAL;
        case // I32_VAL
        4:
            return I32_VAL;
        case // I64_VAL
        5:
            return I64_VAL;
        case // DOUBLE_VAL
        6:
            return DOUBLE_VAL;
        case // STRING_VAL
        7:
            return STRING_VAL;
        case // BINARY_VAL
        8:
            return BINARY_VAL;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TGetPrimaryKeysReq.java,28,597,0.04690117252931323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // TABLE_NAME
        4:
            return TABLE_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.tableName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case TABLE_NAME:
            return isSetTableName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetTableName()) {
        this.tableName = other.tableName;
    }
} 
",,,1,107
TCancelDelegationTokenResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TOperationType.java,13,46,0.2826086956521739,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return EXECUTE_STATEMENT;
        case 1:
            return GET_TYPE_INFO;
        case 2:
            return GET_CATALOGS;
        case 3:
            return GET_SCHEMAS;
        case 4:
            return GET_TABLES;
        case 5:
            return GET_TABLE_TYPES;
        case 6:
            return GET_COLUMNS;
        case 7:
            return GET_FUNCTIONS;
        case 8:
            return UNKNOWN;
        default:
            return null;
    }
} 
",,,,,1,0
TGetOperationStatusResp.java,47,1150,0.0408695652173913," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TOperationState
 */
 
/**
 * @see TOperationState
 */
 
/**
 * Returns true if field operationState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sqlState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorMessage is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field taskStatus is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationStarted is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationCompleted is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field hasResultSet is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field progressUpdateResponse is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_STATE
        2:
            return OPERATION_STATE;
        case // SQL_STATE
        3:
            return SQL_STATE;
        case // ERROR_CODE
        4:
            return ERROR_CODE;
        case // ERROR_MESSAGE
        5:
            return ERROR_MESSAGE;
        case // TASK_STATUS
        6:
            return TASK_STATUS;
        case // OPERATION_STARTED
        7:
            return OPERATION_STARTED;
        case // OPERATION_COMPLETED
        8:
            return OPERATION_COMPLETED;
        case // HAS_RESULT_SET
        9:
            return HAS_RESULT_SET;
        case // PROGRESS_UPDATE_RESPONSE
        10:
            return PROGRESS_UPDATE_RESPONSE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationState;
} 
{
    this.operationState = operationState;
} 
{
    return this.operationState != null;
} 
{
    return this.sqlState != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __ERRORCODE_ISSET_ID);
} 
{
    return this.errorMessage != null;
} 
{
    return this.taskStatus != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __OPERATIONSTARTED_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __OPERATIONCOMPLETED_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __HASRESULTSET_ISSET_ID);
} 
{
    return this.progressUpdateResponse != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_STATE:
            return isSetOperationState();
        case SQL_STATE:
            return isSetSqlState();
        case ERROR_CODE:
            return isSetErrorCode();
        case ERROR_MESSAGE:
            return isSetErrorMessage();
        case TASK_STATUS:
            return isSetTaskStatus();
        case OPERATION_STARTED:
            return isSetOperationStarted();
        case OPERATION_COMPLETED:
            return isSetOperationCompleted();
        case HAS_RESULT_SET:
            return isSetHasResultSet();
        case PROGRESS_UPDATE_RESPONSE:
            return isSetProgressUpdateResponse();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationState()) {
        this.operationState = other.operationState;
    }
    if (other.isSetSqlState()) {
        this.sqlState = other.sqlState;
    }
    this.errorCode = other.errorCode;
    if (other.isSetErrorMessage()) {
        this.errorMessage = other.errorMessage;
    }
    if (other.isSetTaskStatus()) {
        this.taskStatus = other.taskStatus;
    }
    this.operationStarted = other.operationStarted;
    this.operationCompleted = other.operationCompleted;
    this.hasResultSet = other.hasResultSet;
    if (other.isSetProgressUpdateResponse()) {
        this.progressUpdateResponse = other.progressUpdateResponse;
    }
} 
",,,1,107
TDoubleColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Double> __this__values = new ArrayList<Double>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TStructTypeEntry.java,25,357,0.0700280112044818," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field nameToTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // NAME_TO_TYPE_PTR
        1:
            return NAME_TO_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.nameToTypePtr != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case NAME_TO_TYPE_PTR:
            return isSetNameToTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetNameToTypePtr()) {
        Map<String, Integer> __this__nameToTypePtr = new HashMap<String, Integer>(other.nameToTypePtr.size());
        for (Map.Entry<String, Integer> other_element : other.nameToTypePtr.entrySet()) {
            String other_element_key = other_element.getKey();
            Integer other_element_value = other_element.getValue();
            String __this__nameToTypePtr_copy_key = other_element_key;
            Integer __this__nameToTypePtr_copy_value = other_element_value;
            __this__nameToTypePtr.put(__this__nameToTypePtr_copy_key, __this__nameToTypePtr_copy_value);
        }
        this.nameToTypePtr = __this__nameToTypePtr;
    }
} 
",,,1,107
TBoolValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TByteColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Byte> __this__values = new ArrayList<Byte>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TFetchResultsResp.java,28,502,0.055776892430278883," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field hasMoreRows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field results is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // HAS_MORE_ROWS
        2:
            return HAS_MORE_ROWS;
        case // RESULTS
        3:
            return RESULTS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __HASMOREROWS_ISSET_ID);
} 
{
    return this.results != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case HAS_MORE_ROWS:
            return isSetHasMoreRows();
        case RESULTS:
            return isSetResults();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    this.hasMoreRows = other.hasMoreRows;
    if (other.isSetResults()) {
        this.results = new TRowSet(other.results);
    }
} 
",,,1,107
TTypeQualifiers.java,25,359,0.06963788300835655," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field qualifiers is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // QUALIFIERS
        1:
            return QUALIFIERS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.qualifiers != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case QUALIFIERS:
            return isSetQualifiers();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetQualifiers()) {
        Map<String, TTypeQualifierValue> __this__qualifiers = new HashMap<String, TTypeQualifierValue>(other.qualifiers.size());
        for (Map.Entry<String, TTypeQualifierValue> other_element : other.qualifiers.entrySet()) {
            String other_element_key = other_element.getKey();
            TTypeQualifierValue other_element_value = other_element.getValue();
            String __this__qualifiers_copy_key = other_element_key;
            TTypeQualifierValue __this__qualifiers_copy_value = new TTypeQualifierValue(other_element_value);
            __this__qualifiers.put(__this__qualifiers_copy_key, __this__qualifiers_copy_value);
        }
        this.qualifiers = __this__qualifiers;
    }
} 
",,,1,107
TRenewDelegationTokenReq.java,26,395,0.06582278481012659," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TUnionTypeEntry.java,25,357,0.0700280112044818," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field nameToTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // NAME_TO_TYPE_PTR
        1:
            return NAME_TO_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.nameToTypePtr != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case NAME_TO_TYPE_PTR:
            return isSetNameToTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetNameToTypePtr()) {
        Map<String, Integer> __this__nameToTypePtr = new HashMap<String, Integer>(other.nameToTypePtr.size());
        for (Map.Entry<String, Integer> other_element : other.nameToTypePtr.entrySet()) {
            String other_element_key = other_element.getKey();
            Integer other_element_value = other_element.getValue();
            String __this__nameToTypePtr_copy_key = other_element_key;
            Integer __this__nameToTypePtr_copy_value = other_element_value;
            __this__nameToTypePtr.put(__this__nameToTypePtr_copy_key, __this__nameToTypePtr_copy_value);
        }
        this.nameToTypePtr = __this__nameToTypePtr;
    }
} 
",,,1,107
TRowSet.java,30,781,0.03841229193341869," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field startRowOffset is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field rows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field columns is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field binaryColumns is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field columnCount is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // START_ROW_OFFSET
        1:
            return START_ROW_OFFSET;
        case // ROWS
        2:
            return ROWS;
        case // COLUMNS
        3:
            return COLUMNS;
        case // BINARY_COLUMNS
        4:
            return BINARY_COLUMNS;
        case // COLUMN_COUNT
        5:
            return COLUMN_COUNT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __STARTROWOFFSET_ISSET_ID);
} 
{
    return this.rows != null;
} 
{
    return this.columns != null;
} 
{
    return this.binaryColumns != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __COLUMNCOUNT_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case START_ROW_OFFSET:
            return isSetStartRowOffset();
        case ROWS:
            return isSetRows();
        case COLUMNS:
            return isSetColumns();
        case BINARY_COLUMNS:
            return isSetBinaryColumns();
        case COLUMN_COUNT:
            return isSetColumnCount();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.startRowOffset = other.startRowOffset;
    if (other.isSetRows()) {
        List<TRow> __this__rows = new ArrayList<TRow>(other.rows.size());
        for (TRow other_element : other.rows) {
            __this__rows.add(new TRow(other_element));
        }
        this.rows = __this__rows;
    }
    if (other.isSetColumns()) {
        List<TColumn> __this__columns = new ArrayList<TColumn>(other.columns.size());
        for (TColumn other_element : other.columns) {
            __this__columns.add(new TColumn(other_element));
        }
        this.columns = __this__columns;
    }
    if (other.isSetBinaryColumns()) {
        this.binaryColumns = org.apache.thrift.TBaseHelper.copyBinary(other.binaryColumns);
    }
    this.columnCount = other.columnCount;
} 
",,,1,107
TColumnValue.java,17,586,0.02901023890784983," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // BOOL_VAL
        1:
            return BOOL_VAL;
        case // BYTE_VAL
        2:
            return BYTE_VAL;
        case // I16_VAL
        3:
            return I16_VAL;
        case // I32_VAL
        4:
            return I32_VAL;
        case // I64_VAL
        5:
            return I64_VAL;
        case // DOUBLE_VAL
        6:
            return DOUBLE_VAL;
        case // STRING_VAL
        7:
            return STRING_VAL;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TBinaryColumn.java,26,445,0.058426966292134834," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<ByteBuffer> __this__values = new ArrayList<ByteBuffer>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TFetchOrientation.java,13,37,0.35135135135135137,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return FETCH_NEXT;
        case 1:
            return FETCH_PRIOR;
        case 2:
            return FETCH_RELATIVE;
        case 3:
            return FETCH_ABSOLUTE;
        case 4:
            return FETCH_FIRST;
        case 5:
            return FETCH_LAST;
        default:
            return null;
    }
} 
",,,,,1,0
TCancelOperationReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TGetColumnsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TCancelDelegationTokenReq.java,26,395,0.06582278481012659," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TGetSchemasResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TOpenSessionResp.java,40,650,0.06153846153846154," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * Returns true if field serverProtocolVersion is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field configuration is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // SERVER_PROTOCOL_VERSION
        2:
            return SERVER_PROTOCOL_VERSION;
        case // SESSION_HANDLE
        3:
            return SESSION_HANDLE;
        case // CONFIGURATION
        4:
            return CONFIGURATION;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.serverProtocolVersion;
} 
{
    this.serverProtocolVersion = serverProtocolVersion;
} 
{
    return this.serverProtocolVersion != null;
} 
{
    return this.sessionHandle != null;
} 
{
    return this.configuration != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case SERVER_PROTOCOL_VERSION:
            return isSetServerProtocolVersion();
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CONFIGURATION:
            return isSetConfiguration();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetServerProtocolVersion()) {
        this.serverProtocolVersion = other.serverProtocolVersion;
    }
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetConfiguration()) {
        Map<String, String> __this__configuration = new HashMap<String, String>(other.configuration);
        this.configuration = __this__configuration;
    }
} 
",,,1,107
TGetInfoReq.java,38,395,0.09620253164556962," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TGetInfoType
 */
 
/**
 * @see TGetInfoType
 */
 
/**
 * Returns true if field infoType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // INFO_TYPE
        2:
            return INFO_TYPE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.infoType;
} 
{
    this.infoType = infoType;
} 
{
    return this.infoType != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case INFO_TYPE:
            return isSetInfoType();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetInfoType()) {
        this.infoType = other.infoType;
    }
} 
",,,1,107
TOpenSessionReq.java,40,646,0.06191950464396285," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * Returns true if field client_protocol is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field username is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field password is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field configuration is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // CLIENT_PROTOCOL
        1:
            return CLIENT_PROTOCOL;
        case // USERNAME
        2:
            return USERNAME;
        case // PASSWORD
        3:
            return PASSWORD;
        case // CONFIGURATION
        4:
            return CONFIGURATION;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.client_protocol;
} 
{
    this.client_protocol = client_protocol;
} 
{
    return this.client_protocol != null;
} 
{
    return this.username != null;
} 
{
    return this.password != null;
} 
{
    return this.configuration != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case CLIENT_PROTOCOL:
            return isSetClient_protocol();
        case USERNAME:
            return isSetUsername();
        case PASSWORD:
            return isSetPassword();
        case CONFIGURATION:
            return isSetConfiguration();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetClient_protocol()) {
        this.client_protocol = other.client_protocol;
    }
    if (other.isSetUsername()) {
        this.username = other.username;
    }
    if (other.isSetPassword()) {
        this.password = other.password;
    }
    if (other.isSetConfiguration()) {
        Map<String, String> __this__configuration = new HashMap<String, String>(other.configuration);
        this.configuration = __this__configuration;
    }
} 
",,,1,107
TGetInfoValue.java,17,513,0.03313840155945419," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // STRING_VALUE
        1:
            return STRING_VALUE;
        case // SMALL_INT_VALUE
        2:
            return SMALL_INT_VALUE;
        case // INTEGER_BITMASK
        3:
            return INTEGER_BITMASK;
        case // INTEGER_FLAG
        4:
            return INTEGER_FLAG;
        case // BINARY_VALUE
        5:
            return BINARY_VALUE;
        case // LEN_VALUE
        6:
            return LEN_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TDoubleValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TI16Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Short> __this__values = new ArrayList<Short>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TStringColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<String> __this__values = new ArrayList<String>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TCancelOperationResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TPrimitiveTypeEntry.java,38,405,0.09382716049382717," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TTypeId
 */
 
/**
 * @see TTypeId
 */
 
/**
 * Returns true if field type is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field typeQualifiers is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPE
        1:
            return TYPE;
        case // TYPE_QUALIFIERS
        2:
            return TYPE_QUALIFIERS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.type;
} 
{
    this.type = type;
} 
{
    return this.type != null;
} 
{
    return this.typeQualifiers != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPE:
            return isSetType();
        case TYPE_QUALIFIERS:
            return isSetTypeQualifiers();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetType()) {
        this.type = other.type;
    }
    if (other.isSetTypeQualifiers()) {
        this.typeQualifiers = new TTypeQualifiers(other.typeQualifiers);
    }
} 
",,,1,107
TGetInfoType.java,13,160,0.08125,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return CLI_MAX_DRIVER_CONNECTIONS;
        case 1:
            return CLI_MAX_CONCURRENT_ACTIVITIES;
        case 2:
            return CLI_DATA_SOURCE_NAME;
        case 8:
            return CLI_FETCH_DIRECTION;
        case 13:
            return CLI_SERVER_NAME;
        case 14:
            return CLI_SEARCH_PATTERN_ESCAPE;
        case 17:
            return CLI_DBMS_NAME;
        case 18:
            return CLI_DBMS_VER;
        case 19:
            return CLI_ACCESSIBLE_TABLES;
        case 20:
            return CLI_ACCESSIBLE_PROCEDURES;
        case 23:
            return CLI_CURSOR_COMMIT_BEHAVIOR;
        case 25:
            return CLI_DATA_SOURCE_READ_ONLY;
        case 26:
            return CLI_DEFAULT_TXN_ISOLATION;
        case 28:
            return CLI_IDENTIFIER_CASE;
        case 29:
            return CLI_IDENTIFIER_QUOTE_CHAR;
        case 30:
            return CLI_MAX_COLUMN_NAME_LEN;
        case 31:
            return CLI_MAX_CURSOR_NAME_LEN;
        case 32:
            return CLI_MAX_SCHEMA_NAME_LEN;
        case 34:
            return CLI_MAX_CATALOG_NAME_LEN;
        case 35:
            return CLI_MAX_TABLE_NAME_LEN;
        case 43:
            return CLI_SCROLL_CONCURRENCY;
        case 46:
            return CLI_TXN_CAPABLE;
        case 47:
            return CLI_USER_NAME;
        case 72:
            return CLI_TXN_ISOLATION_OPTION;
        case 73:
            return CLI_INTEGRITY;
        case 81:
            return CLI_GETDATA_EXTENSIONS;
        case 85:
            return CLI_NULL_COLLATION;
        case 86:
            return CLI_ALTER_TABLE;
        case 90:
            return CLI_ORDER_BY_COLUMNS_IN_SELECT;
        case 94:
            return CLI_SPECIAL_CHARACTERS;
        case 97:
            return CLI_MAX_COLUMNS_IN_GROUP_BY;
        case 98:
            return CLI_MAX_COLUMNS_IN_INDEX;
        case 99:
            return CLI_MAX_COLUMNS_IN_ORDER_BY;
        case 100:
            return CLI_MAX_COLUMNS_IN_SELECT;
        case 101:
            return CLI_MAX_COLUMNS_IN_TABLE;
        case 102:
            return CLI_MAX_INDEX_SIZE;
        case 104:
            return CLI_MAX_ROW_SIZE;
        case 105:
            return CLI_MAX_STATEMENT_LEN;
        case 106:
            return CLI_MAX_TABLES_IN_SELECT;
        case 107:
            return CLI_MAX_USER_NAME_LEN;
        case 115:
            return CLI_OJ_CAPABILITIES;
        case 10000:
            return CLI_XOPEN_CLI_YEAR;
        case 10001:
            return CLI_CURSOR_SENSITIVITY;
        case 10002:
            return CLI_DESCRIBE_PARAMETER;
        case 10003:
            return CLI_CATALOG_NAME;
        case 10004:
            return CLI_COLLATION_SEQ;
        case 10005:
            return CLI_MAX_IDENTIFIER_LEN;
        default:
            return null;
    }
} 
",,,,,1,0
TGetTablesResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TTableSchema.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field columns is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COLUMNS
        1:
            return COLUMNS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.columns != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COLUMNS:
            return isSetColumns();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetColumns()) {
        List<TColumnDesc> __this__columns = new ArrayList<TColumnDesc>(other.columns.size());
        for (TColumnDesc other_element : other.columns) {
            __this__columns.add(new TColumnDesc(other_element));
        }
        this.columns = __this__columns;
    }
} 
",,,1,107
TProtocolVersion.java,13,49,0.2653061224489796,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return HIVE_CLI_SERVICE_PROTOCOL_V1;
        case 1:
            return HIVE_CLI_SERVICE_PROTOCOL_V2;
        case 2:
            return HIVE_CLI_SERVICE_PROTOCOL_V3;
        case 3:
            return HIVE_CLI_SERVICE_PROTOCOL_V4;
        case 4:
            return HIVE_CLI_SERVICE_PROTOCOL_V5;
        case 5:
            return HIVE_CLI_SERVICE_PROTOCOL_V6;
        case 6:
            return HIVE_CLI_SERVICE_PROTOCOL_V7;
        case 7:
            return HIVE_CLI_SERVICE_PROTOCOL_V8;
        case 8:
            return HIVE_CLI_SERVICE_PROTOCOL_V9;
        case 9:
            return HIVE_CLI_SERVICE_PROTOCOL_V10;
        default:
            return null;
    }
} 
",,,,,1,0
TTypeQualifierValue.java,17,301,0.05647840531561462," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // I32_VALUE
        1:
            return I32_VALUE;
        case // STRING_VALUE
        2:
            return STRING_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TTypeDesc.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field types is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPES
        1:
            return TYPES;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.types != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPES:
            return isSetTypes();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetTypes()) {
        List<TTypeEntry> __this__types = new ArrayList<TTypeEntry>(other.types.size());
        for (TTypeEntry other_element : other.types) {
            __this__types.add(new TTypeEntry(other_element));
        }
        this.types = __this__types;
    }
} 
",,,1,107
TGetResultSetMetadataResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schema is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // SCHEMA
        2:
            return SCHEMA;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.schema != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case SCHEMA:
            return isSetSchema();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetSchema()) {
        this.schema = new TTableSchema(other.schema);
    }
} 
",,,1,107
TArrayTypeEntry.java,26,297,0.08754208754208755," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field objectTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OBJECT_TYPE_PTR
        1:
            return OBJECT_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __OBJECTTYPEPTR_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OBJECT_TYPE_PTR:
            return isSetObjectTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.objectTypePtr = other.objectTypePtr;
} 
",,,1,107
TStatus.java,42,730,0.057534246575342465," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TStatusCode
 */
 
/**
 * @see TStatusCode
 */
 
/**
 * Returns true if field statusCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field infoMessages is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sqlState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorMessage is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS_CODE
        1:
            return STATUS_CODE;
        case // INFO_MESSAGES
        2:
            return INFO_MESSAGES;
        case // SQL_STATE
        3:
            return SQL_STATE;
        case // ERROR_CODE
        4:
            return ERROR_CODE;
        case // ERROR_MESSAGE
        5:
            return ERROR_MESSAGE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.statusCode;
} 
{
    this.statusCode = statusCode;
} 
{
    return this.statusCode != null;
} 
{
    return this.infoMessages != null;
} 
{
    return this.sqlState != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __ERRORCODE_ISSET_ID);
} 
{
    return this.errorMessage != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS_CODE:
            return isSetStatusCode();
        case INFO_MESSAGES:
            return isSetInfoMessages();
        case SQL_STATE:
            return isSetSqlState();
        case ERROR_CODE:
            return isSetErrorCode();
        case ERROR_MESSAGE:
            return isSetErrorMessage();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatusCode()) {
        this.statusCode = other.statusCode;
    }
    if (other.isSetInfoMessages()) {
        List<String> __this__infoMessages = new ArrayList<String>(other.infoMessages);
        this.infoMessages = __this__infoMessages;
    }
    if (other.isSetSqlState()) {
        this.sqlState = other.sqlState;
    }
    this.errorCode = other.errorCode;
    if (other.isSetErrorMessage()) {
        this.errorMessage = other.errorMessage;
    }
} 
",,,1,107
TRow.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field colVals is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COL_VALS
        1:
            return COL_VALS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.colVals != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COL_VALS:
            return isSetColVals();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetColVals()) {
        List<TColumnValue> __this__colVals = new ArrayList<TColumnValue>(other.colVals.size());
        for (TColumnValue other_element : other.colVals) {
            __this__colVals.add(new TColumnValue(other_element));
        }
        this.colVals = __this__colVals;
    }
} 
",,,1,107
TGetTablesReq.java,29,739,0.03924221921515562," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableTypes is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // TABLE_NAME
        4:
            return TABLE_NAME;
        case // TABLE_TYPES
        5:
            return TABLE_TYPES;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.tableName != null;
} 
{
    return this.tableTypes != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case TABLE_NAME:
            return isSetTableName();
        case TABLE_TYPES:
            return isSetTableTypes();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetTableName()) {
        this.tableName = other.tableName;
    }
    if (other.isSetTableTypes()) {
        List<String> __this__tableTypes = new ArrayList<String>(other.tableTypes);
        this.tableTypes = __this__tableTypes;
    }
} 
",,,1,107
TCLIServiceConstants.java,6,89,0.06741573033707865,,,,,,,,1,0
TGetPrimaryKeysResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TExecuteStatementResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TCloseOperationResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TStringValue.java,25,306,0.08169934640522876," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.value != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValue()) {
        this.value = other.value;
    }
} 
",,,1,107
TTypeEntry.java,17,530,0.03207547169811321," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // PRIMITIVE_ENTRY
        1:
            return PRIMITIVE_ENTRY;
        case // ARRAY_ENTRY
        2:
            return ARRAY_ENTRY;
        case // MAP_ENTRY
        3:
            return MAP_ENTRY;
        case // STRUCT_ENTRY
        4:
            return STRUCT_ENTRY;
        case // UNION_ENTRY
        5:
            return UNION_ENTRY;
        case // USER_DEFINED_TYPE_ENTRY
        6:
            return USER_DEFINED_TYPE_ENTRY;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TGetTypeInfoReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TRenewDelegationTokenResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TColumnDesc.java,29,582,0.04982817869415808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field columnName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field typeDesc is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field position is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field comment is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COLUMN_NAME
        1:
            return COLUMN_NAME;
        case // TYPE_DESC
        2:
            return TYPE_DESC;
        case // POSITION
        3:
            return POSITION;
        case // COMMENT
        4:
            return COMMENT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.columnName != null;
} 
{
    return this.typeDesc != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __POSITION_ISSET_ID);
} 
{
    return this.comment != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COLUMN_NAME:
            return isSetColumnName();
        case TYPE_DESC:
            return isSetTypeDesc();
        case POSITION:
            return isSetPosition();
        case COMMENT:
            return isSetComment();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetColumnName()) {
        this.columnName = other.columnName;
    }
    if (other.isSetTypeDesc()) {
        this.typeDesc = new TTypeDesc(other.typeDesc);
    }
    this.position = other.position;
    if (other.isSetComment()) {
        this.comment = other.comment;
    }
} 
",,,1,107
TByteValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TI32Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Integer> __this__values = new ArrayList<Integer>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TGetTypeInfoResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
THandleIdentifier.java,26,404,0.06435643564356436," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field guid is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field secret is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // GUID
        1:
            return GUID;
        case // SECRET
        2:
            return SECRET;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.guid != null;
} 
{
    return this.secret != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case GUID:
            return isSetGuid();
        case SECRET:
            return isSetSecret();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetGuid()) {
        this.guid = org.apache.thrift.TBaseHelper.copyBinary(other.guid);
    }
    if (other.isSetSecret()) {
        this.secret = org.apache.thrift.TBaseHelper.copyBinary(other.secret);
    }
} 
",,,1,107
TGetOperationStatusReq.java,27,401,0.06733167082294264," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field getProgressUpdate is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        case // GET_PROGRESS_UPDATE
        2:
            return GET_PROGRESS_UPDATE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __GETPROGRESSUPDATE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
        case GET_PROGRESS_UPDATE:
            return isSetGetProgressUpdate();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
    this.getProgressUpdate = other.getProgressUpdate;
} 
",,,1,107
TCLIService.java,804,14414,0.055779103649229916," The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetReq()) {
        this.req = new TOpenSessionReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TOpenSessionResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCloseSessionReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCloseSessionResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetInfoReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetInfoResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TExecuteStatementReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TExecuteStatementResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTypeInfoReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTypeInfoResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetCatalogsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetCatalogsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetSchemasReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetSchemasResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTablesReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTablesResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTableTypesReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTableTypesResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetColumnsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetColumnsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetFunctionsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetFunctionsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetPrimaryKeysReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetPrimaryKeysResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetCrossReferenceReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetCrossReferenceResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetOperationStatusReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetOperationStatusResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCancelOperationReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCancelOperationResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCloseOperationReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCloseOperationResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetResultSetMetadataReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetResultSetMetadataResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TFetchResultsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TFetchResultsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetDelegationTokenResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCancelDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCancelDelegationTokenResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TRenewDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TRenewDelegationTokenResp(other.success);
    }
} 
",,,1,4535
TI16Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TGetResultSetMetadataReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TMapTypeEntry.java,27,381,0.07086614173228346," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field keyTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field valueTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // KEY_TYPE_PTR
        1:
            return KEY_TYPE_PTR;
        case // VALUE_TYPE_PTR
        2:
            return VALUE_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __KEYTYPEPTR_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUETYPEPTR_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case KEY_TYPE_PTR:
            return isSetKeyTypePtr();
        case VALUE_TYPE_PTR:
            return isSetValueTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.keyTypePtr = other.keyTypePtr;
    this.valueTypePtr = other.valueTypePtr;
} 
",,,1,107
TUserDefinedTypeEntry.java,25,300,0.08333333333333333," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field typeClassName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPE_CLASS_NAME
        1:
            return TYPE_CLASS_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.typeClassName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPE_CLASS_NAME:
            return isSetTypeClassName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetTypeClassName()) {
        this.typeClassName = other.typeClassName;
    }
} 
",,,1,107
TBoolColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Boolean> __this__values = new ArrayList<Boolean>(other.values);
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
    }
} 
",,,1,107
TGetTableTypesReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TJobExecutionStatus.java,13,28,0.4642857142857143,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return IN_PROGRESS;
        case 1:
            return COMPLETE;
        case 2:
            return NOT_AVAILABLE;
        default:
            return null;
    }
} 
",,,,,1,0
ServiceUtils.java,29,39,0.7435897435897436,,"/**
 * Close the Closeable objects and <b>ignore</b> any {@link IOException} or
 * null pointers. Must only be used for cleanup in exception handlers.
 *
 * @param log the log to record problems to at debug level. Can be null.
 * @param closeables the objects to close
 */
 
","{
    for (java.io.Closeable c : closeables) {
        if (c != null) {
            try {
                c.close();
            } catch (IOException e) {
                if (log != null && log.isDebugEnabled()) {
                    log.debug(""Exception in closing "" + c, e);
                }
            }
        }
    }
} 
",,,,,1,0
HttpAuthUtils.java,56,117,0.47863247863247865,"
 * Utility functions for HTTP mode authentication.
 |
   * We'll create an instance of this class within a doAs block so that the client's TGT credentials
   * can be read from the Subject
   ","/**
 * @return Stringified Base64 encoded kerberosAuthHeader on success
 * @throws Exception
 */
 
/**
 * Creates and returns a HS2 cookie token.
 * @param clientUserName Client User name.
 * @return An unsigned cookie token generated from input parameters.
 * The final cookie generated is of the following format :
 * {@code cu=<username>&rn=<randomNumber>&s=<cookieSignature>}
 */
 
/**
 * Parses a cookie token to retrieve client user name.
 * @param tokenStr Token String.
 * @return A valid user name if input is of valid format, else returns null.
 */
 
/**
 * Splits the cookie token into attributes pairs.
 * @param str input token.
 * @return a map with the attribute pairs of the token if the input is valid.
 * Else, returns null.
 */
 
","{
    String serverPrincipal = ShimLoader.getHadoopThriftAuthBridge().getServerPrincipal(principal, host);
    if (assumeSubject) {
        // With this option, we're assuming that the external application,
        // using the JDBC driver has done a JAAS kerberos login already
        AccessControlContext context = AccessController.getContext();
        Subject subject = Subject.getSubject(context);
        if (subject == null) {
            throw new Exception(""The Subject is not set"");
        }
        return Subject.doAs(subject, new HttpKerberosClientAction(serverPrincipal, serverHttpUrl));
    } else {
        // JAAS login from ticket cache to setup the client UserGroupInformation
        UserGroupInformation clientUGI = ShimLoader.getHadoopThriftAuthBridge().getCurrentUGIWithConf(""kerberos"");
        return clientUGI.doAs(new HttpKerberosClientAction(serverPrincipal, serverHttpUrl));
    }
} 
{
    StringBuffer sb = new StringBuffer();
    sb.append(COOKIE_CLIENT_USER_NAME).append(COOKIE_KEY_VALUE_SEPARATOR).append(clientUserName).append(COOKIE_ATTR_SEPARATOR);
    sb.append(COOKIE_CLIENT_RAND_NUMBER).append(COOKIE_KEY_VALUE_SEPARATOR).append((new Random(System.currentTimeMillis())).nextLong());
    return sb.toString();
} 
{
    Map<String, String> map = splitCookieToken(tokenStr);
    if (!map.keySet().equals(COOKIE_ATTRIBUTES)) {
        LOG.error(""Invalid token with missing attributes "" + tokenStr);
        return null;
    }
    return map.get(COOKIE_CLIENT_USER_NAME);
} 
{
    Map<String, String> map = new HashMap<String, String>();
    StringTokenizer st = new StringTokenizer(tokenStr, COOKIE_ATTR_SEPARATOR);
    while (st.hasMoreTokens()) {
        String part = st.nextToken();
        int separator = part.indexOf(COOKIE_KEY_VALUE_SEPARATOR);
        if (separator == -1) {
            LOG.error(""Invalid token string "" + tokenStr);
            return null;
        }
        String key = part.substring(0, separator);
        String value = part.substring(separator + 1);
        map.put(key, value);
    }
    return map;
} 
",,,,,4,185
HiveAuthFactory.java,37,364,0.10164835164835165,"
 * This class helps in some aspects of authentication. It creates the proper Thrift classes for the
 * given configuration as well as helps with authenticating requests.
 ","/**
 * Returns the thrift processor factory for HiveServer2 running in binary mode
 * @param service
 * @return
 * @throws LoginException
 */
 
","{
    if (authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.getAuthName())) {
        return KerberosSaslHelper.getKerberosProcessorFactory(saslServer, service);
    } else {
        return PlainSaslHelper.getPlainProcessorFactory(service);
    }
} 
",,,,,2,167
KerberosSaslHelper.java,17,80,0.2125,,,,,,,,1,0
TSetIpAddressProcessor.java,25,76,0.32894736842105265,"
 * This class is responsible for setting the ipAddress for operations executed via HiveServer2.
 *
 * - IP address is only set for operations that calls listeners with hookContext
 * - IP address is only set if the underlying transport mechanism is socket
 *
 * @see org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext
 ",,,,,,,6,311
PlainSaslHelper.java,18,114,0.15789473684210525,,,,,,,,1,0
HiveServer2.java,57,208,0.27403846153846156,"
 * HiveServer2.
 *
 |
   * ServerOptionsProcessor.
   * Process arguments given to HiveServer2 (-hiveconf property=value)
   * Set properties in System properties
   * Create an appropriate response object,
   * which has executor to execute the appropriate command based on the parsed options.
   |
   * The response sent back from {@link ServerOptionsProcessor#parse(String[])}
   |
   * The executor interface for running the appropriate HiveServer2 command based on parsed options
   |
   * HelpOptionExecutor: executes the --help option by printing out the usage
   |
   * StartOptionExecutor: starts HiveServer2.
   * This is the default executor, when no option is specified.
   ",,,,,,,17,657
ThreadWithGarbageCleanup.java,29,38,0.7631578947368421,"
 * A HiveServer2 thread used to construct new server threads.
 * In particular, this thread ensures an orderly cleanup,
 * when killed by its corresponding ExecutorService.
 ","/**
 * Add any Thread specific garbage cleanup code here.
 * Currently, it shuts down the RawStore object for this thread if it is not null.
 */
 
/**
 * Cache the ThreadLocal RawStore object. Called from the corresponding thread.
 */
 
","{
    cleanRawStore();
    super.finalize();
} 
{
    Long threadId = this.getId();
    RawStore threadLocalRawStore = HiveMetaStore.HMSHandler.getRawStore();
    if (threadLocalRawStore != null && !threadRawStoreMap.containsKey(threadId)) {
        LOG.debug(""Adding RawStore: "" + threadLocalRawStore + "", for the thread: "" + this.getName() + "" to threadRawStoreMap for future cleanup."");
        threadRawStoreMap.put(threadId, threadLocalRawStore);
    }
} 
",,,,,3,168
MetadataOperation.java,44,75,0.5866666666666667,"
 * MetadataOperation.
 *
 ","/**
 * Convert wildchars and escape sequence from JDBC format to datanucleous/regex
 */
 
/**
 * Convert wildchars and escape sequence of schema pattern from JDBC format to datanucleous/regex
 * The schema pattern treats empty string also as wildchar
 */
 
/**
 * Convert a pattern containing JDBC catalog search wildcards into
 * Java regex patterns.
 *
 * @param pattern input which may contain '%' or '_' wildcard characters, or
 * these characters escaped using {@link #getSearchStringEscape()}.
 * @return replace %/_ with regex search characters, also handle escaped
 * characters.
 *
 * The datanucleus module expects the wildchar as '*'. The columns search on the
 * other hand is done locally inside the hive code and that requires the regex wildchar
 * format '.*'  This is driven by the datanucleusFormat flag.
 */
 
","{
    if (pattern == null) {
        return convertPattern(""%"", true);
    } else {
        return convertPattern(pattern, datanucleusFormat);
    }
} 
{
    if ((pattern == null) || pattern.isEmpty()) {
        return convertPattern(""%"", true);
    } else {
        return convertPattern(pattern, true);
    }
} 
{
    String wStr;
    if (datanucleusFormat) {
        wStr = ""*"";
    } else {
        wStr = "".*"";
    }
    return pattern.replaceAll(""([^\\\\])%"", ""$1"" + wStr).replaceAll(""\\\\%"", ""%"").replaceAll(""^%"", wStr).replaceAll(""([^\\\\])_"", ""$1."").replaceAll(""\\\\_"", ""_"").replaceAll(""^_"", ""."");
} 
",,,,,2,22
GetFunctionsOperation.java,29,106,0.27358490566037735,"
 * GetFunctionsOperation.
 *
 ",,,,,,,2,26
OperationManager.java,30,238,0.12605042016806722,"
 * OperationManager.
 *
 ",,,,,,,2,21
ClassicTableTypeMapping.java,24,61,0.39344262295081966,"
 * ClassicTableTypeMapping.
 * Classic table type mapping :
 *  Managed Table to Table
 *  External Table to Table
 *  Virtual View to View
 ",,,,,,,5,131
HiveTableTypeMapping.java,22,22,1.0,"
 * HiveTableTypeMapping.
 * Default table type mapping
 *
 ",,,,,,,3,53
Operation.java,86,333,0.25825825825825827,,"/**
 * Invoked before runInternal().
 * Set up some preconditions, or configurations.
 */
 
/**
 * Invoked after runInternal(), even if an exception is thrown in runInternal().
 * Clean up resources, which was set up in beforeRun().
 */
 
/**
 * Implemented by subclass of Operation class to execute specific behaviors.
 * @throws HiveSQLException
 */
 
/**
 * Verify if the given fetch orientation is part of the default orientation types.
 * @param orientation
 * @throws HiveSQLException
 */
 
/**
 * Verify if the given fetch orientation is part of the supported orientation types.
 * @param orientation
 * @param supportedOrientations
 * @throws HiveSQLException
 */
 
","{
    createOperationLog();
} 
{
    unregisterOperationLog();
} 
runInternal 
{
    validateFetchOrientation(orientation, DEFAULT_FETCH_ORIENTATION_SET);
} 
{
    if (!supportedOrientations.contains(orientation)) {
        throw new HiveSQLException(""The fetch type "" + orientation.toString() + "" is not supported for this resultset"", ""HY106"");
    }
} 
",,,,,1,0
GetSchemasOperation.java,27,59,0.4576271186440678,"
 * GetSchemasOperation.
 *
 ",,,,,,,2,24
ExecuteStatementOperation.java,17,57,0.2982456140350877,,,,,,,,1,0
GetPrimaryKeysOperation.java,35,69,0.5072463768115942,"
 * GetPrimaryKeysOperation.
 *
 ",,,,,"/**
 *  TABLE_CAT String => table catalog (may be null)
 *  TABLE_SCHEM String => table schema (may be null)
 *  TABLE_NAME String => table name
 *  COLUMN_NAME String => column name
 *  KEY_SEQ short => sequence number within primary key( a value of 1 represents the first column of the primary key, a value of 2 would represent the second column within the primary key).
 *  PK_NAME String => primary key name (may be null)
 */
 
","Field RESULT_SET_SCHEMA
",2,28
SQLOperation.java,86,333,0.25825825825825827,"
 * SQLOperation.
 *
 ","/**
 * Compile the query and extract metadata
 * @param queryState
 * @throws HiveSQLException
 */
 
/**
 * Returns the current UGI on the stack
 * @param opConfig
 * @return UserGroupInformation
 * @throws HiveSQLException
 */
 
/**
 * Returns the ThreadLocal Hive for the current thread
 * @return Hive
 * @throws HiveSQLException
 */
 
/**
 * If there are query specific settings to overlay, then create a copy of config
 * There are two cases we need to clone the session config that's being passed to hive driver
 * 1. Async query -
 *    If the client changes a config setting, that shouldn't reflect in the execution already underway
 * 2. confOverlay -
 *    The query specific settings should only be applied to the query config and not session
 * @return new configuration
 * @throws HiveSQLException
 */
 
","{
    setState(OperationState.RUNNING);
    try {
        driver = new Driver(queryState, getParentSession().getUserName());
        // set the operation handle information in Driver, so that thrift API users
        // can use the operation handle they receive, to lookup query information in
        // Yarn ATS
        String guid64 = Base64.encodeBase64URLSafeString(getHandle().getHandleIdentifier().toTHandleIdentifier().getGuid()).trim();
        driver.setOperationId(guid64);
        // In Hive server mode, we are not able to retry in the FetchTask
        // case, when calling fetch queries since execute() has returned.
        // For now, we disable the test attempts.
        driver.setTryCount(Integer.MAX_VALUE);
        response = driver.compileAndRespond(statement);
        if (0 != response.getResponseCode()) {
            throw toSQLException(""Error while compiling statement"", response);
        }
        mResultSchema = driver.getSchema();
        // hasResultSet should be true only if the query has a FetchTask
        // ""explain"" is an exception for now
        if (driver.getPlan().getFetchTask() != null) {
            // Schema has to be set
            if (mResultSchema == null || !mResultSchema.isSetFieldSchemas()) {
                throw new HiveSQLException(""Error compiling query: Schema and FieldSchema "" + ""should be set when query plan has a FetchTask"");
            }
            resultSchema = new TableSchema(mResultSchema);
            setHasResultSet(true);
        } else {
            setHasResultSet(false);
        }
        // Set hasResultSet true if the plan has ExplainTask
        // TODO explain should use a FetchTask for reading
        for (Task<? extends Serializable> task : driver.getPlan().getRootTasks()) {
            if (task.getClass() == ExplainTask.class) {
                resultSchema = new TableSchema(mResultSchema);
                setHasResultSet(true);
                break;
            }
        }
    } catch (HiveSQLException e) {
        setState(OperationState.ERROR);
        throw e;
    } catch (Exception e) {
        setState(OperationState.ERROR);
        throw new HiveSQLException(""Error running query: "" + e.toString(), e);
    }
} 
{
    try {
        return Utils.getUGI();
    } catch (Exception e) {
        throw new HiveSQLException(""Unable to get current user"", e);
    }
} 
{
    try {
        return Hive.get();
    } catch (HiveException e) {
        throw new HiveSQLException(""Failed to get ThreadLocal Hive object"", e);
    }
} 
{
    HiveConf sqlOperationConf = getParentSession().getHiveConf();
    if (!getConfOverlay().isEmpty() || shouldRunAsync()) {
        // clone the parent session config for this query
        sqlOperationConf = new HiveConf(sqlOperationConf);
        // apply overlay query specific settings, if any
        for (Map.Entry<String, String> confEntry : getConfOverlay().entrySet()) {
            try {
                sqlOperationConf.verifyAndSet(confEntry.getKey(), confEntry.getValue());
            } catch (IllegalArgumentException e) {
                throw new HiveSQLException(""Error applying statement specific settings"", e);
            }
        }
    }
    return sqlOperationConf;
} 
",,,,,2,17
HiveCommandOperation.java,42,152,0.27631578947368424,"
 * Executes a HiveCommand
 ","/**
 * Reads the temporary results for non-Hive (non-Driver) commands to the
 * resulting List of strings.
 * @param nLines number of lines read at once. If it is <= 0, then read all lines.
 */
 
","{
    if (resultReader == null) {
        SessionState sessionState = getParentSession().getSessionState();
        File tmp = sessionState.getTmpOutputFile();
        try {
            resultReader = new BufferedReader(new FileReader(tmp));
        } catch (FileNotFoundException e) {
            LOG.error(""File "" + tmp + "" not found. "", e);
            throw new HiveSQLException(e);
        }
    }
    List<String> results = new ArrayList<String>();
    for (int i = 0; i < nLines || nLines <= 0; ++i) {
        try {
            String line = resultReader.readLine();
            if (line == null) {
                // reached the end of the result file
                break;
            } else {
                results.add(line);
            }
        } catch (IOException e) {
            LOG.error(""Reading temp results encountered an exception: "", e);
            throw new HiveSQLException(e);
        }
    }
    return results;
} 
",,,"/**
 * For processors other than Hive queries (Driver), they output to session.out (a temp file)
 * first and the fetchOne/fetchN/fetchAll functions get the output from pipeIn.
 */
 
","Field resultReader
",1,25
GetTypeInfoOperation.java,27,105,0.2571428571428571,"
 * GetTypeInfoOperation.
 *
 ",,,,,,,2,25
GetCrossReferenceOperation.java,56,96,0.5833333333333334,"
 * GetCrossReferenceOperation.
 *
 ",,,,,"/**
 *  PKTABLE_CAT String => parent key table catalog (may be null)
 *  PKTABLE_SCHEM String => parent key table schema (may be null)
 *  PKTABLE_NAME String => parent key table name
 *  PKCOLUMN_NAME String => parent key column name
 *  FKTABLE_CAT String => foreign key table catalog (may be null) being exported (may be null)
 *  FKTABLE_SCHEM String => foreign key table schema (may be null) being exported (may be null)
 *  FKTABLE_NAME String => foreign key table name being exported
 *  FKCOLUMN_NAME String => foreign key column name being exported
 *  KEY_SEQ short => sequence number within foreign key( a value of 1 represents the first column of the foreign key, a value of 2 would represent the second column within the foreign key).
 *  UPDATE_RULE short => What happens to foreign key when parent key is updated:
 *  importedNoAction - do not allow update of parent key if it has been imported
 *  importedKeyCascade - change imported key to agree with parent key update
 *  importedKeySetNull - change imported key to NULL if its parent key has been updated
 *  importedKeySetDefault - change imported key to default values if its parent key has been updated
 *  importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility)
 *  DELETE_RULE short => What happens to the foreign key when parent key is deleted.
 *  importedKeyNoAction - do not allow delete of parent key if it has been imported
 *  importedKeyCascade - delete rows that import a deleted key
 *  importedKeySetNull - change imported key to NULL if its primary key has been deleted
 *  importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility)
 *  importedKeySetDefault - change imported key to default if its parent key has been deleted
 *  FK_NAME String => foreign key name (may be null)
 *  PK_NAME String => parent key name (may be null)
 *  DEFERRABILITY short => can the evaluation of foreign key constraints be deferred until commit
 *  importedKeyInitiallyDeferred - see SQL92 for definition
 *  importedKeyInitiallyImmediate - see SQL92 for definition
 *  importedKeyNotDeferrable - see SQL92 for definition
 */
 
","Field RESULT_SET_SCHEMA
",2,31
GetColumnsOperation.java,27,202,0.13366336633663367,"
 * GetColumnsOperation.
 *
 ",,,,,,,2,24
TableTypeMapping.java,24,61,0.39344262295081966,,"/**
 * Map client's table type name to hive's table type
 * @param clientTypeName
 * @return
 */
 
/**
 * Map hive's table type name to client's table type
 * @param hiveTypeName
 * @return
 */
 
/**
 * Get all the table types of this mapping
 * @return
 */
 
","mapToHiveType 
mapToClientType 
getTableTypeNames 
",,,,,1,0
GetCatalogsOperation.java,27,45,0.6,"
 * GetCatalogsOperation.
 *
 ",,,,,,,2,25
GetTablesOperation.java,27,104,0.25961538461538464,"
 * GetTablesOperation.
 *
 ",,,,,,,2,23
GetTableTypesOperation.java,27,56,0.48214285714285715,"
 * GetTableTypesOperation.
 *
 ",,,,,,,2,27
OperationType.java,13,46,0.2826086956521739,"
 * OperationType.
 *
 ",,,,,,,2,18
ICLIService.java,17,65,0.26153846153846155,,,,,,,,1,0
FetchOrientation.java,13,37,0.35135135135135137,"
 * FetchOrientation.
 *
 ",,,,,,,2,21
SessionHandle.java,25,305,0.08196721311475409,"
 * SessionHandle.
 *
 ",,,,,,,2,18
ColumnValue.java,17,586,0.02901023890784983,"
 * Protocols before HIVE_CLI_SERVICE_PROTOCOL_V6 (used by RowBasedSet)
 *
 ",,,,,,,2,71
CLIService.java,804,14414,0.055779103649229916,"
 * CLIService.
 *
 ","/**
 * @deprecated  Use {@link #openSession(TProtocolVersion, String, String, String, Map)}
 */
 
/**
 * @deprecated  Use {@link #openSessionWithImpersonation(TProtocolVersion, String, String, String, Map, String)}
 */
 
/**
 * Execute statement on the server with a timeout. This is a blocking call.
 */
 
/**
 * Execute statement asynchronously on the server. This is a non-blocking call
 */
 
/**
 * Execute statement asynchronously on the server with a timeout. This is a non-blocking call
 */
 
","{
    SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, false, null);
    LOG.debug(sessionHandle + "": openSession()"");
    return sessionHandle;
} 
{
    SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, true, delegationToken);
    LOG.debug(sessionHandle + "": openSessionWithImpersonation()"");
    return sessionHandle;
} 
{
    HiveSession session = sessionManager.getSession(sessionHandle);
    // need to reset the monitor, as operation handle is not available down stream, Ideally the
    // monitor should be associated with the operation handle.
    session.getSessionState().updateProgressMonitor(null);
    OperationHandle opHandle = session.executeStatement(statement, confOverlay, queryTimeout);
    LOG.debug(sessionHandle + "": executeStatement()"");
    return opHandle;
} 
{
    HiveSession session = sessionManager.getSession(sessionHandle);
    // need to reset the monitor, as operation handle is not available down stream, Ideally the
    // monitor should be associated with the operation handle.
    session.getSessionState().updateProgressMonitor(null);
    OperationHandle opHandle = session.executeStatementAsync(statement, confOverlay);
    LOG.debug(sessionHandle + "": executeStatementAsync()"");
    return opHandle;
} 
{
    HiveSession session = sessionManager.getSession(sessionHandle);
    // need to reset the monitor, as operation handle is not available down stream, Ideally the
    // monitor should be associated with the operation handle.
    session.getSessionState().updateProgressMonitor(null);
    OperationHandle opHandle = session.executeStatementAsync(statement, confOverlay, queryTimeout);
    LOG.debug(sessionHandle + "": executeStatementAsync()"");
    return opHandle;
} 
",,,,,2,15
ThriftCLIServiceClient.java,78,382,0.20418848167539266,"
 * ThriftCLIServiceClient.
 *
 ",,,,,,,2,27
ThriftBinaryCLIService.java,21,77,0.2727272727272727,,,,,,,,1,0
ThriftHttpServlet.java,126,393,0.32061068702290074,"
 *
 * ThriftHttpServlet
 *
 ","/**
 * Retrieves the client name from cookieString. If the cookie does not
 * correspond to a valid client, the function returns null.
 * @param cookies HTTP Request cookies.
 * @return Client Username if cookieString has a HS2 Generated cookie that is currently valid.
 * Else, returns null.
 */
 
/**
 * Convert cookie array to human readable cookie string
 * @param cookies Cookie Array
 * @return String containing all the cookies separated by a newline character.
 * Each cookie is of the format [key]=[value]
 */
 
/**
 * Validate the request cookie. This function iterates over the request cookie headers
 * and finds a cookie that represents a valid client/server session. If it finds one, it
 * returns the client name associated with the session. Else, it returns null.
 * @param request The HTTP Servlet Request send by the client
 * @return Client Username if the request has valid HS2 cookie, else returns null
 * @throws UnsupportedEncodingException
 */
 
/**
 * Generate a server side cookie given the cookie value as the input.
 * @param str Input string token.
 * @return The generated cookie.
 * @throws UnsupportedEncodingException
 */
 
/**
 * Generate httponly cookie from HS2 cookie
 * @param cookie HS2 generated cookie
 * @return The httponly cookie
 */
 
/**
 * Do the LDAP/PAM authentication
 * @param request
 * @param authType
 * @throws HttpAuthenticationException
 */
 
/**
 * Do the GSS-API kerberos authentication.
 * We already have a logged in subject in the form of serviceUGI,
 * which GSS-API will extract information from.
 * In case of a SPNego request we use the httpUGI,
 * for the authenticating service tickets.
 * @param request
 * @return
 * @throws HttpAuthenticationException
 */
 
/**
 * Returns the base64 encoded auth header payload
 * @param request
 * @param authType
 * @return
 * @throws HttpAuthenticationException
 */
 
","{
    // Current Cookie Name, Current Cookie Value
    String currName, currValue;
    // Following is the main loop which iterates through all the cookies send by the client.
    // The HS2 generated cookies are of the format hive.server2.auth=<value>
    // A cookie which is identified as a hiveserver2 generated cookie is validated
    // by calling signer.verifyAndExtract(). If the validation passes, send the
    // username for which the cookie is validated to the caller. If no client side
    // cookie passes the validation, return null to the caller.
    for (Cookie currCookie : cookies) {
        // Get the cookie name
        currName = currCookie.getName();
        if (!currName.equals(AUTH_COOKIE)) {
            // Not a HS2 generated cookie, continue.
            continue;
        }
        // If we reached here, we have match for HS2 generated cookie
        currValue = currCookie.getValue();
        // Validate the value.
        currValue = signer.verifyAndExtract(currValue);
        // Retrieve the user name, do the final validation step.
        if (currValue != null) {
            String userName = HttpAuthUtils.getUserNameFromCookieToken(currValue);
            if (userName == null) {
                LOG.warn(""Invalid cookie token "" + currValue);
                continue;
            }
            // We have found a valid cookie in the client request.
            if (LOG.isDebugEnabled()) {
                LOG.debug(""Validated the cookie for user "" + userName);
            }
            return userName;
        }
    }
    // No valid HS2 generated cookies found, return null
    return null;
} 
{
    String cookieStr = """";
    for (Cookie c : cookies) {
        cookieStr += c.getName() + ""="" + c.getValue() + "" ;\n"";
    }
    return cookieStr;
} 
{
    // Find all the valid cookies associated with the request.
    Cookie[] cookies = request.getCookies();
    if (cookies == null) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(""No valid cookies associated with the request "" + request);
        }
        return null;
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Received cookies: "" + toCookieStr(cookies));
    }
    return getClientNameFromCookie(cookies);
} 
{
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Cookie name = "" + AUTH_COOKIE + "" value = "" + str);
    }
    Cookie cookie = new Cookie(AUTH_COOKIE, str);
    cookie.setMaxAge(cookieMaxAge);
    if (cookieDomain != null) {
        cookie.setDomain(cookieDomain);
    }
    if (cookiePath != null) {
        cookie.setPath(cookiePath);
    }
    cookie.setSecure(isCookieSecure);
    return cookie;
} 
{
    NewCookie newCookie = new NewCookie(cookie.getName(), cookie.getValue(), cookie.getPath(), cookie.getDomain(), cookie.getVersion(), cookie.getComment(), cookie.getMaxAge(), cookie.getSecure());
    return newCookie + ""; HttpOnly"";
} 
{
    String userName = getUsername(request, authType);
    // No-op when authType is NOSASL
    if (!authType.equalsIgnoreCase(HiveAuthFactory.AuthTypes.NOSASL.toString())) {
        try {
            AuthMethods authMethod = AuthMethods.getValidAuthMethod(authType);
            PasswdAuthenticationProvider provider = AuthenticationProviderFactory.getAuthenticationProvider(authMethod);
            provider.Authenticate(userName, getPassword(request, authType));
        } catch (Exception e) {
            throw new HttpAuthenticationException(e);
        }
    }
    return userName;
} 
{
    // Try authenticating with the http/_HOST principal
    if (httpUGI != null) {
        try {
            return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));
        } catch (Exception e) {
            LOG.info(""Failed to authenticate with http/_HOST kerberos principal, "" + ""trying with hive/_HOST kerberos principal"");
        }
    }
    // Now try with hive/_HOST principal
    try {
        return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));
    } catch (Exception e) {
        LOG.error(""Failed to authenticate with hive/_HOST kerberos principal"");
        throw new HttpAuthenticationException(e);
    }
} 
{
    String authHeader = request.getHeader(HttpAuthUtils.AUTHORIZATION);
    // Each http request must have an Authorization header
    if (authHeader == null || authHeader.isEmpty()) {
        throw new HttpAuthenticationException(""Authorization header received "" + ""from the client is empty."");
    }
    String authHeaderBase64String;
    int beginIndex;
    if (isKerberosAuthMode(authType)) {
        beginIndex = (HttpAuthUtils.NEGOTIATE + "" "").length();
    } else {
        beginIndex = (HttpAuthUtils.BASIC + "" "").length();
    }
    authHeaderBase64String = authHeader.substring(beginIndex);
    // Authorization header must have a payload
    if (authHeaderBase64String == null || authHeaderBase64String.isEmpty()) {
        throw new HttpAuthenticationException(""Authorization header received "" + ""from the client does not contain any data."");
    }
    return authHeaderBase64String;
} 
",,,,,3,22
ThriftCLIService.java,66,598,0.11036789297658862,"
 * ThriftCLIService.
 *
 ","/**
 * Returns the effective username.
 * 1. If hive.server2.allow.user.substitution = false: the username of the connecting user
 * 2. If hive.server2.allow.user.substitution = true: the username of the end user,
 * that the connecting user is trying to proxy for.
 * This includes a check whether the connecting user is allowed to proxy for the end user.
 * @param req
 * @return
 * @throws HiveSQLException
 */
 
/**
 * Create a session handle
 * @param req
 * @param res
 * @return
 * @throws HiveSQLException
 * @throws LoginException
 * @throws IOException
 */
 
/**
 * If the proxy user name is provided then check privileges to substitute the user.
 * @param realUser
 * @param sessionConf
 * @param ipAddress
 * @return
 * @throws HiveSQLException
 */
 
","{
    String userName = null;
    // Kerberos
    if (isKerberosAuthMode()) {
        userName = hiveAuthFactory.getRemoteUser();
    }
    // Except kerberos, NOSASL
    if (userName == null) {
        userName = TSetIpAddressProcessor.getUserName();
    }
    // Http transport mode.
    // We set the thread local username, in ThriftHttpServlet.
    if (cliService.getHiveConf().getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase(""http"")) {
        userName = SessionManager.getUserName();
    }
    if (userName == null) {
        userName = req.getUsername();
    }
    userName = getShortName(userName);
    String effectiveClientUser = getProxyUser(userName, req.getConfiguration(), getIpAddress());
    LOG.debug(""Client's username: "" + effectiveClientUser);
    return effectiveClientUser;
} 
{
    String userName = getUserName(req);
    String ipAddress = getIpAddress();
    TProtocolVersion protocol = getMinVersion(CLIService.SERVER_VERSION, req.getClient_protocol());
    res.setServerProtocolVersion(protocol);
    SessionHandle sessionHandle;
    if (cliService.getHiveConf().getBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS) && (userName != null)) {
        String delegationTokenStr = getDelegationToken(userName);
        sessionHandle = cliService.openSessionWithImpersonation(protocol, userName, req.getPassword(), ipAddress, req.getConfiguration(), delegationTokenStr);
    } else {
        sessionHandle = cliService.openSession(protocol, userName, req.getPassword(), ipAddress, req.getConfiguration());
    }
    return sessionHandle;
} 
{
    String proxyUser = null;
    // Http transport mode.
    // We set the thread local proxy username, in ThriftHttpServlet.
    if (cliService.getHiveConf().getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase(""http"")) {
        proxyUser = SessionManager.getProxyUserName();
        LOG.debug(""Proxy user from query string: "" + proxyUser);
    }
    if (proxyUser == null && sessionConf != null && sessionConf.containsKey(HiveAuthFactory.HS2_PROXY_USER)) {
        String proxyUserFromThriftBody = sessionConf.get(HiveAuthFactory.HS2_PROXY_USER);
        LOG.debug(""Proxy user from thrift body: "" + proxyUserFromThriftBody);
        proxyUser = proxyUserFromThriftBody;
    }
    if (proxyUser == null) {
        return realUser;
    }
    // check whether substitution is allowed
    if (!hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ALLOW_USER_SUBSTITUTION)) {
        throw new HiveSQLException(""Proxy user substitution is not allowed"");
    }
    // If there's no authentication, then directly substitute the user
    if (HiveAuthFactory.AuthTypes.NONE.toString().equalsIgnoreCase(hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION))) {
        return proxyUser;
    }
    // Verify proxy user privilege of the realUser for the proxyUser
    HiveAuthFactory.verifyProxyAccess(realUser, proxyUser, ipAddress, hiveConf);
    LOG.debug(""Verified proxy user: "" + proxyUser);
    return proxyUser;
} 
",,,,,2,21
ThriftHttpCLIService.java,42,125,0.336,,"/**
 * Configure Jetty to serve http requests. Example of a client connection URL:
 * http://localhost:10000/servlets/thrifths2/ A gateway may cause actual target URL to differ,
 * e.g. http://gateway:port/hive2/servlets/thrifths2/
 */
 
/**
 * The config parameter can be like ""path"", ""/path"", ""/path/"", ""path/*"", ""/path1/path2/*"" and so on.
 * httpPath should end up as ""/*"", ""/path/*"" or ""/path1/../pathN/*""
 * @param httpPath
 * @return
 */
 
","{
    try {
        // Server thread pool
        // Start with minWorkerThreads, expand till maxWorkerThreads and reject subsequent requests
        String threadPoolName = ""HiveServer2-HttpHandler-Pool"";
        ThreadPoolExecutor executorService = new ThreadPoolExecutor(minWorkerThreads, maxWorkerThreads, workerKeepAliveTime, TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), new ThreadFactoryWithGarbageCleanup(threadPoolName));
        ExecutorThreadPool threadPool = new ExecutorThreadPool(executorService);
        // HTTP Server
        httpServer = new org.eclipse.jetty.server.Server(threadPool);
        // Connector configs
        ConnectionFactory[] connectionFactories;
        boolean useSsl = hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_USE_SSL);
        String schemeName = useSsl ? ""https"" : ""http"";
        // Change connector if SSL is used
        if (useSsl) {
            String keyStorePath = hiveConf.getVar(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH).trim();
            String keyStorePassword = ShimLoader.getHadoopShims().getPassword(hiveConf, HiveConf.ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PASSWORD.varname);
            if (keyStorePath.isEmpty()) {
                throw new IllegalArgumentException(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH.varname + "" Not configured for SSL connection"");
            }
            SslContextFactory sslContextFactory = new SslContextFactory.Server();
            String[] excludedProtocols = hiveConf.getVar(ConfVars.HIVE_SSL_PROTOCOL_BLACKLIST).split("","");
            LOG.info(""HTTP Server SSL: adding excluded protocols: "" + Arrays.toString(excludedProtocols));
            sslContextFactory.addExcludeProtocols(excludedProtocols);
            LOG.info(""HTTP Server SSL: SslContextFactory.getExcludeProtocols = "" + Arrays.toString(sslContextFactory.getExcludeProtocols()));
            sslContextFactory.setKeyStorePath(keyStorePath);
            sslContextFactory.setKeyStorePassword(keyStorePassword);
            connectionFactories = AbstractConnectionFactory.getFactories(sslContextFactory, new HttpConnectionFactory());
        } else {
            connectionFactories = new ConnectionFactory[] { new HttpConnectionFactory() };
        }
        ServerConnector connector = new ServerConnector(httpServer, null, // Call this full constructor to set this, which forces daemon threads:
        new ScheduledExecutorScheduler(""HiveServer2-HttpHandler-JettyScheduler"", true), null, -1, -1, connectionFactories);
        connector.setPort(portNum);
        // Linux:yes, Windows:no
        connector.setReuseAddress(!Shell.WINDOWS);
        int maxIdleTime = (int) hiveConf.getTimeVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_MAX_IDLE_TIME, TimeUnit.MILLISECONDS);
        connector.setIdleTimeout(maxIdleTime);
        httpServer.addConnector(connector);
        // Thrift configs
        hiveAuthFactory = new HiveAuthFactory(hiveConf);
        TProcessor processor = new TCLIService.Processor<Iface>(this);
        TProtocolFactory protocolFactory = new TBinaryProtocol.Factory();
        // Set during the init phase of HiveServer2 if auth mode is kerberos
        // UGI for the hive/_HOST (kerberos) principal
        UserGroupInformation serviceUGI = cliService.getServiceUGI();
        // UGI for the http/_HOST (SPNego) principal
        UserGroupInformation httpUGI = cliService.getHttpUGI();
        String authType = hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION);
        TServlet thriftHttpServlet = new ThriftHttpServlet(processor, protocolFactory, authType, serviceUGI, httpUGI, hiveAuthFactory);
        // Context handler
        final ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);
        context.setContextPath(""/"");
        String httpPath = getHttpPath(hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_PATH));
        httpServer.setHandler(context);
        context.addServlet(new ServletHolder(thriftHttpServlet), httpPath);
        // TODO: check defaults: maxTimeout, keepalive, maxBodySize, bodyRecieveDuration, etc.
        // Finally, start the server
        httpServer.start();
        String msg = ""Started "" + ThriftHttpCLIService.class.getSimpleName() + "" in "" + schemeName + "" mode on port "" + portNum + "" path="" + httpPath + "" with "" + minWorkerThreads + ""..."" + maxWorkerThreads + "" worker threads"";
        LOG.info(msg);
        httpServer.join();
    } catch (Throwable t) {
        LOG.error(""Error starting HiveServer2: could not start "" + ThriftHttpCLIService.class.getSimpleName(), t);
        System.exit(-1);
    }
} 
{
    if (httpPath == null || httpPath.equals("""")) {
        httpPath = ""/*"";
    } else {
        if (!httpPath.startsWith(""/"")) {
            httpPath = ""/"" + httpPath;
        }
        if (httpPath.endsWith(""/"")) {
            httpPath = httpPath + ""*"";
        }
        if (!httpPath.endsWith(""/*"")) {
            httpPath = httpPath + ""/*"";
        }
    }
    return httpPath;
} 
",,,,,1,0
HandleIdentifier.java,26,404,0.06435643564356436,"
 * HandleIdentifier.
 *
 ",,,,,,,2,21
GetInfoValue.java,17,513,0.03313840155945419,"
 * GetInfoValue.
 *
 ",,,,,,,2,17
TableSchema.java,25,351,0.07122507122507123,"
 * TableSchema.
 *
 ",,,,,,,2,16
GetInfoType.java,13,160,0.08125,"
 * GetInfoType.
 *
 ",,,,,,,2,16
TypeDescriptor.java,39,104,0.375,"
 * TypeDescriptor.
 *
 ","/**
 * The column size for this type.
 * For numeric data this is the maximum precision.
 * For character data this is the length in characters.
 * For datetime types this is the length in characters of the String representation
 * (assuming the maximum allowed precision of the fractional seconds component).
 * For binary data this is the length in bytes.
 * Null is returned for data types where the column size is not applicable.
 */
 
/**
 * Maximum precision for numeric types.
 * Returns null for non-numeric types.
 * @return
 */
 
/**
 * The number of fractional digits for this type.
 * Null is returned for data types where this is not applicable.
 */
 
","{
    if (type.isNumericType()) {
        return getPrecision();
    }
    switch(type) {
        case STRING_TYPE:
        case BINARY_TYPE:
            return Integer.MAX_VALUE;
        case CHAR_TYPE:
        case VARCHAR_TYPE:
            return typeQualifiers.getCharacterMaximumLength();
        case DATE_TYPE:
            return 10;
        case TIMESTAMP_TYPE:
            return 29;
        default:
            return null;
    }
} 
{
    if (this.type == Type.DECIMAL_TYPE) {
        return typeQualifiers.getPrecision();
    }
    return this.type.getMaxPrecision();
} 
{
    switch(this.type) {
        case BOOLEAN_TYPE:
        case TINYINT_TYPE:
        case SMALLINT_TYPE:
        case INT_TYPE:
        case BIGINT_TYPE:
            return 0;
        case FLOAT_TYPE:
            return 7;
        case DOUBLE_TYPE:
            return 15;
        case DECIMAL_TYPE:
            return typeQualifiers.getScale();
        case TIMESTAMP_TYPE:
            return 9;
        default:
            return null;
    }
} 
",,,,,2,19
RowBasedSet.java,20,97,0.20618556701030927,"
 * RowBasedSet
 ",,,,,,,1,14
OperationState.java,23,80,0.2875,"
 * OperationState.
 *
 ",,,,,,,2,19
ColumnDescriptor.java,24,59,0.4067796610169492,"
 * ColumnDescriptor.
 *
 ",,,,,,,2,21
RowSet.java,30,781,0.03841229193341869,,,,,,,,1,0
OperationHandle.java,41,575,0.07130434782608695,,,,,,,,1,0
TypeQualifiers.java,25,359,0.06963788300835655,"
 * This class holds type qualifier information for a primitive type,
 * such as char/varchar length or decimal precision/scale.
 ",,,,,,,2,125
Handle.java,41,575,0.07130434782608695,,,,,,,,1,0
HiveSQLException.java,82,143,0.5734265734265734,"
 * HiveSQLException.
 *
 ","/**
 * Converts current object to a {@link TStatus} object
 * @return a {@link TStatus} object
 */
 
/**
 * Converts the specified {@link Exception} object into a {@link TStatus} object
 * @param e a {@link Exception} object
 * @return a {@link TStatus} object
 */
 
/**
 * Converts a {@link Throwable} object into a flattened list of texts including its stack trace
 * and the stack traces of the nested causes.
 * @param ex  a {@link Throwable} object
 * @return    a flattened list of texts including the {@link Throwable} object's stack trace
 *            and the stack traces of the nested causes.
 */
 
/**
 * Converts a flattened list of texts including the stack trace and the stack
 * traces of the nested causes into a {@link Throwable} object.
 * @param details a flattened list of texts including the stack trace and the stack
 *                traces of the nested causes
 * @return        a {@link Throwable} object
 */
 
","{
    // TODO: convert sqlState, etc.
    TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
    tStatus.setSqlState(getSQLState());
    tStatus.setErrorCode(getErrorCode());
    tStatus.setErrorMessage(getMessage());
    tStatus.setInfoMessages(toString(this));
    return tStatus;
} 
{
    if (e instanceof HiveSQLException) {
        return ((HiveSQLException) e).toTStatus();
    }
    TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
    tStatus.setErrorMessage(e.getMessage());
    tStatus.setInfoMessages(toString(e));
    return tStatus;
} 
{
    return toString(ex, null);
} 
{
    return toStackTrace(details, null, 0);
} 
","/**
 */
 
/**
 * @param reason
 */
 
/**
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 */
 
/**
 * @param reason
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 * @param vendorCode
 */
 
/**
 * @param reason
 * @param sqlState
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 * @param vendorCode
 * @param cause
 */
 
","{
    super();
} 
{
    super(reason);
} 
{
    super(cause);
} 
{
    super(reason, sqlState);
} 
{
    super(reason, cause);
} 
{
    super(reason, sqlState, vendorCode);
} 
{
    super(reason, sqlState, cause);
} 
{
    super(reason, sqlState, vendorCode, cause);
} 
","/**
 */
 
","Field serialVersionUID
",2,21
RowSetFactory.java,17,19,0.8947368421052632,,,,,,,,1,0
ColumnBasedSet.java,22,150,0.14666666666666667,"
 * ColumnBasedSet.
 ",,,,,,,1,18
HiveSessionImpl.java,57,759,0.07509881422924901,"
 * HiveSession
 *
 | Copy from org.apache.hadoop.hive.ql.processors.SetProcessor, only change:|
   * It is used for processing hiverc file from HiveServer2 side.
   ","/**
 * 1. We'll remove the ThreadLocal SessionState as this thread might now serve
 * other requests.
 * 2. We'll cache the ThreadLocal RawStore object for this background thread for an orderly cleanup
 * when this thread is garbage collected later.
 * @see org.apache.hive.service.server.ThreadWithGarbageCleanup#finalize()
 */
 
","{
    SessionState.detachSession();
    if (ThreadWithGarbageCleanup.currentThread() instanceof ThreadWithGarbageCleanup) {
        ThreadWithGarbageCleanup currentThread = (ThreadWithGarbageCleanup) ThreadWithGarbageCleanup.currentThread();
        currentThread.cacheThreadLocalRawStore();
    }
    if (userAccess) {
        lastAccessTime = System.currentTimeMillis();
    }
    if (opHandleSet.isEmpty()) {
        lastIdleTime = System.currentTimeMillis();
    } else {
        lastIdleTime = 0;
    }
} 
",,,,,4,158
HiveSessionHookContextImpl.java,23,20,1.15,"
 *
 * HiveSessionHookContextImpl.
 * Session hook context implementation which is created by session  manager
 * and passed to hook invocation.
 ",,,,,,,4,137
HiveSessionImplwithUGI.java,38,123,0.3089430894308943,"
 *
 * HiveSessionImplwithUGI.
 * HiveSession with connecting user's UGI and delegation token if required
 ","/**
 * Close the file systems for the session and remove it from the FileSystem cache.
 * Cancel the session's delegation token and close the metastore connection
 */
 
/**
 * Enable delegation token for the session
 * save the token string and set the token.signature in hive conf. The metastore client uses
 * this token.signature to determine where to use kerberos or delegation token
 * @throws HiveException
 * @throws IOException
 */
 
","{
    try {
        acquire(true);
        cancelDelegationToken();
    } finally {
        try {
            super.close();
        } finally {
            try {
                FileSystem.closeAllForUGI(sessionUgi);
            } catch (IOException ioe) {
                throw new HiveSQLException(""Could not clean up file-system handles for UGI: "" + sessionUgi, ioe);
            }
        }
    }
} 
{
    this.delegationTokenStr = delegationTokenStr;
    if (delegationTokenStr != null) {
        getHiveConf().set(""hive.metastore.token.signature"", HS2TOKEN);
        try {
            Utils.setTokenStr(sessionUgi, delegationTokenStr, HS2TOKEN);
        } catch (IOException e) {
            throw new HiveSQLException(""Couldn't setup delegation token in the ugi"", e);
        }
    }
} 
",,,,,3,100
HiveSessionHookContext.java,34,7,4.857142857142857,"
 * HiveSessionHookContext.
 * Interface passed to the HiveServer2 session hook execution. This enables
 * the hook implementation to access session config, user and session handle
 ","/**
 * Retrieve session conf
 * @return
 */
 
/**
 * The get the username starting the session
 * @return
 */
 
/**
 * Retrieve handle for the session
 * @return
 */
 
","getSessionConf 
getSessionUser 
getSessionHandle 
",,,,,3,175
HiveSessionBase.java,43,26,1.6538461538461537,"
 * Methods that don't need to be executed under a doAs
 * context are here. Rest of them in HiveSession interface
 ","/**
 * Set the session manager for the session
 * @param sessionManager
 */
 
/**
 * Get the session manager for the session
 */
 
/**
 * Set operation manager for the session
 * @param operationManager
 */
 
/**
 * Check whether operation logging is enabled and session dir is created successfully
 */
 
/**
 * Get the session dir, which is the parent dir of operation logs
 * @return a file representing the parent directory of operation logs
 */
 
/**
 * Set the session dir, which is the parent dir of operation logs
 * @param operationLogRootDir the parent dir of the session dir
 */
 
","setSessionManager 
getSessionManager 
setOperationManager 
isOperationLogEnabled 
getOperationLogSessionDir 
setOperationLogSessionDir 
",,,,,2,111
SessionManager.java,48,272,0.17647058823529413,"
 * SessionManager.
 *
 ","/**
 * Opens a new session and creates a session handle.
 * The username passed to this method is the effective username.
 * If withImpersonation is true (==doAs true) we wrap all the calls in HiveSession
 * within a UGI.doAs, where UGI corresponds to the effective user.
 *
 * Please see {@code org.apache.hive.service.cli.thrift.ThriftCLIService.getUserName()} for
 * more details.
 *
 * @param protocol
 * @param username
 * @param password
 * @param ipAddress
 * @param sessionConf
 * @param withImpersonation
 * @param delegationToken
 * @return
 * @throws HiveSQLException
 */
 
","{
    HiveSession session;
    // If doAs is set to true for HiveServer2, we will create a proxy object for the session impl.
    // Within the proxy object, we wrap the method call in a UserGroupInformation#doAs
    if (withImpersonation) {
        HiveSessionImplwithUGI sessionWithUGI = new HiveSessionImplwithUGI(protocol, username, password, hiveConf, ipAddress, delegationToken);
        session = HiveSessionProxy.getProxy(sessionWithUGI, sessionWithUGI.getSessionUgi());
        sessionWithUGI.setProxySession(session);
    } else {
        session = new HiveSessionImpl(protocol, username, password, hiveConf, ipAddress);
    }
    session.setSessionManager(this);
    session.setOperationManager(operationManager);
    try {
        session.open(sessionConf);
    } catch (Exception e) {
        try {
            session.close();
        } catch (Throwable t) {
            LOG.warn(""Error closing session"", t);
        }
        session = null;
        throw new HiveSQLException(""Failed to open new session: "" + e, e);
    }
    if (isOperationLogEnabled) {
        session.setOperationLogSessionDir(operationLogRootDir);
    }
    handleToSession.put(session.getSessionHandle(), session);
    return session.getSessionHandle();
} 
",,,,,2,19
HiveSession.java,124,49,2.5306122448979593,,"/**
 * getInfo operation handler
 * @param getInfoType
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @param queryTimeout
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @param queryTimeout
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTypeInfo operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getCatalogs operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getSchemas operation handler
 * @param catalogName
 * @param schemaName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTables operation handler
 * @param catalogName
 * @param schemaName
 * @param tableName
 * @param tableTypes
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTableTypes operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getColumns operation handler
 * @param catalogName
 * @param schemaName
 * @param tableName
 * @param columnName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getFunctions operation handler
 * @param catalogName
 * @param schemaName
 * @param functionName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getPrimaryKeys operation handler
 * @param catalog
 * @param schema
 * @param table
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getCrossReference operation handler
 * @param primaryCatalog
 * @param primarySchema
 * @param primaryTable
 * @param foreignCatalog
 * @param foreignSchema
 * @param foreignTable
 * @return
 * @throws HiveSQLException
 */
 
/**
 * close the session
 * @throws HiveSQLException
 */
 
","getInfo 
executeStatement 
executeStatement 
executeStatementAsync 
executeStatementAsync 
getTypeInfo 
getCatalogs 
getSchemas 
getTables 
getTableTypes 
getColumns 
getFunctions 
getPrimaryKeys 
getCrossReference 
close 
",,,,,1,0
ServiceOperations.java,77,51,1.5098039215686274,"
 * ServiceOperations.
 *
 ","/**
 * Verify that a service is in a given state.
 * @param state the actual state a service is in
 * @param expectedState the desired state
 * @throws IllegalStateException if the service state is different from
 * the desired state
 */
 
/**
 * Initialize then start a service.
 *
 * The service state is checked <i>before</i> the operation begins.
 * This process is <i>not</i> thread safe.
 * @param service a service that must be in the state
 *   {@link Service.STATE#NOTINITED}
 * @param configuration the configuration to initialize the service with
 * @throws RuntimeException on a state change failure
 * @throws IllegalStateException if the service is in the wrong state
 */
 
/**
 * Stop a service.
 *
 * Do nothing if the service is null or not in a state in which it can be/needs to be stopped.
 *
 * The service state is checked <i>before</i> the operation begins.
 * This process is <i>not</i> thread safe.
 * @param service a service or null
 */
 
/**
 * Stop a service; if it is null do nothing. Exceptions are caught and
 * logged at warn level. (but not Throwables). This operation is intended to
 * be used in cleanup operations
 *
 * @param service a service; may be null
 * @return any exception that was caught; null if none was.
 */
 
","{
    if (state != expectedState) {
        throw new IllegalStateException(""For this operation, the "" + ""current service state must be "" + expectedState + "" instead of "" + state);
    }
} 
{
    init(service, configuration);
    start(service);
} 
{
    if (service != null) {
        Service.STATE state = service.getServiceState();
        if (state == Service.STATE.STARTED) {
            service.stop();
        }
    }
} 
{
    try {
        stop(service);
    } catch (Exception e) {
        LOG.warn(""When stopping the service "" + service.getName() + "" : "" + e, e);
        return e;
    }
    return null;
} 
",,,,,2,22
CompositeService.java,32,80,0.4,"
 * CompositeService.
 *
 |
   * JVM Shutdown hook for CompositeService which will stop the given
   * CompositeService gracefully in case of JVM shutdown.
   ",,,,,,,5,149
AbstractService.java,86,76,1.131578947368421,"
 * AbstractService.
 *
 ","/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * Verify that a service is in a given state.
 *
 * @param currentState
 *          the desired state
 * @throws IllegalStateException
 *           if the service state is different from
 *           the desired state
 */
 
/**
 * Change to a new state and notify all listeners.
 * This is a private method that is only invoked from synchronized methods,
 * which avoid having to clone the listener list. It does imply that
 * the state change listener methods should be short lived, as they
 * will delay the state transition.
 *
 * @param newState
 *          new service state
 */
 
","{
    ensureCurrentState(STATE.NOTINITED);
    this.hiveConf = hiveConf;
    changeState(STATE.INITED);
    LOG.info(""Service:"" + getName() + "" is inited."");
} 
{
    startTime = System.currentTimeMillis();
    ensureCurrentState(STATE.INITED);
    changeState(STATE.STARTED);
    LOG.info(""Service:"" + getName() + "" is started."");
} 
{
    if (state == STATE.STOPPED || state == STATE.INITED || state == STATE.NOTINITED) {
        // already stopped, or else it was never
        // started (eg another service failing canceled startup)
        return;
    }
    ensureCurrentState(STATE.STARTED);
    changeState(STATE.STOPPED);
    LOG.info(""Service:"" + getName() + "" is stopped."");
} 
{
    ServiceOperations.ensureCurrentState(state, currentState);
} 
{
    state = newState;
    // notify listeners
    for (ServiceStateChangeListener l : listeners) {
        l.stateChanged(this);
    }
} 
","/**
 * Construct the service.
 *
 * @param name
 *          service name
 */
 
","{
    this.name = name;
} 
","/**
 * Service state: initially {@link STATE#NOTINITED}.
 */
 
/**
 * Service name.
 */
 
/**
 * Service start time. Will be zero until the service is started.
 */
 
/**
 * The configuration. Will be null until the service is initialized.
 */
 
/**
 * List of state change listeners; it is final to ensure
 * that it will never be null.
 */
 
","Field state
Field name
Field startTime
Field hiveConf
Field listeners
",2,20
CookieSigner.java,41,57,0.7192982456140351,"
 * The cookie signer generates a signature based on SHA digest
 * and appends it to the cookie value generated at the
 * server side. It uses SHA digest algorithm to sign and verify signatures.
 ","/**
 * Sign the cookie given the string token as input.
 * @param str Input token
 * @return Signed token that can be used to create a cookie
 */
 
/**
 * Verify a signed string and extracts the original string.
 * @param signedStr The already signed string
 * @return Raw Value of the string without the signature
 */
 
/**
 * Get the signature of the input string based on SHA digest algorithm.
 * @param str Input token
 * @return Signed String
 */
 
","{
    if (str == null || str.isEmpty()) {
        throw new IllegalArgumentException(""NULL or empty string to sign"");
    }
    String signature = getSignature(str);
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Signature generated for "" + str + "" is "" + signature);
    }
    return str + SIGNATURE + signature;
} 
{
    int index = signedStr.lastIndexOf(SIGNATURE);
    if (index == -1) {
        throw new IllegalArgumentException(""Invalid input sign: "" + signedStr);
    }
    String originalSignature = signedStr.substring(index + SIGNATURE.length());
    String rawValue = signedStr.substring(0, index);
    String currentSignature = getSignature(rawValue);
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Signature generated for "" + rawValue + "" inside verify is "" + currentSignature);
    }
    if (!originalSignature.equals(currentSignature)) {
        throw new IllegalArgumentException(""Invalid sign, original = "" + originalSignature + "" current = "" + currentSignature);
    }
    return rawValue;
} 
{
    try {
        MessageDigest md = MessageDigest.getInstance(SHA_STRING);
        md.update(str.getBytes());
        md.update(secretBytes);
        byte[] digest = md.digest();
        return new Base64(0).encodeToString(digest);
    } catch (NoSuchAlgorithmException ex) {
        throw new RuntimeException(""Invalid SHA digest String: "" + SHA_STRING + "" "" + ex.getMessage(), ex);
    }
} 
","/**
 * Constructor
 * @param secret Secret Bytes
 */
 
","{
    if (secret == null) {
        throw new IllegalArgumentException("" NULL Secret Bytes"");
    }
    this.secretBytes = secret.clone();
} 
",,,3,189
TOperationState.java,13,46,0.2826086956521739,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return INITIALIZED_STATE;
        case 1:
            return RUNNING_STATE;
        case 2:
            return FINISHED_STATE;
        case 3:
            return CANCELED_STATE;
        case 4:
            return CLOSED_STATE;
        case 5:
            return ERROR_STATE;
        case 6:
            return UKNOWN_STATE;
        case 7:
            return PENDING_STATE;
        default:
            return null;
    }
} 
",,,,,1,0
TGetDelegationTokenResp.java,26,405,0.06419753086419754," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TGetCatalogsReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TExecuteStatementReq.java,30,726,0.04132231404958678," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field statement is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field confOverlay is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field runAsync is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // STATEMENT
        2:
            return STATEMENT;
        case // CONF_OVERLAY
        3:
            return CONF_OVERLAY;
        case // RUN_ASYNC
        4:
            return RUN_ASYNC;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.statement != null;
} 
{
    return this.confOverlay != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __RUNASYNC_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case STATEMENT:
            return isSetStatement();
        case CONF_OVERLAY:
            return isSetConfOverlay();
        case RUN_ASYNC:
            return isSetRunAsync();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetStatement()) {
        this.statement = other.statement;
    }
    if (other.isSetConfOverlay()) {
        Map<String, String> __this__confOverlay = new HashMap<String, String>();
        for (Map.Entry<String, String> other_element : other.confOverlay.entrySet()) {
            String other_element_key = other_element.getKey();
            String other_element_value = other_element.getValue();
            String __this__confOverlay_copy_key = other_element_key;
            String __this__confOverlay_copy_value = other_element_value;
            __this__confOverlay.put(__this__confOverlay_copy_key, __this__confOverlay_copy_value);
        }
        this.confOverlay = __this__confOverlay;
    }
    this.runAsync = other.runAsync;
} 
",,,1,107
TCloseSessionResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TI32Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TGetColumnsReq.java,29,693,0.04184704184704185," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field columnName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // TABLE_NAME
        4:
            return TABLE_NAME;
        case // COLUMN_NAME
        5:
            return COLUMN_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.tableName != null;
} 
{
    return this.columnName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case TABLE_NAME:
            return isSetTableName();
        case COLUMN_NAME:
            return isSetColumnName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetTableName()) {
        this.tableName = other.tableName;
    }
    if (other.isSetColumnName()) {
        this.columnName = other.columnName;
    }
} 
",,,1,107
TGetFunctionsReq.java,28,591,0.047377326565143825," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field functionName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // FUNCTION_NAME
        4:
            return FUNCTION_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.functionName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case FUNCTION_NAME:
            return isSetFunctionName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetFunctionName()) {
        this.functionName = other.functionName;
    }
} 
",,,1,107
TGetFunctionsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TI64Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TSessionHandle.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionId is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_ID
        1:
            return SESSION_ID;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionId != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_ID:
            return isSetSessionId();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionId()) {
        this.sessionId = new THandleIdentifier(other.sessionId);
    }
} 
",,,1,107
TGetInfoResp.java,26,397,0.0654911838790932," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field infoValue is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // INFO_VALUE
        2:
            return INFO_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.infoValue != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case INFO_VALUE:
            return isSetInfoValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetInfoValue()) {
        this.infoValue = new TGetInfoValue(other.infoValue);
    }
} 
",,,1,107
TOperationHandle.java,41,575,0.07130434782608695," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationId is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TOperationType
 */
 
/**
 * @see TOperationType
 */
 
/**
 * Returns true if field operationType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field hasResultSet is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field modifiedRowCount is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_ID
        1:
            return OPERATION_ID;
        case // OPERATION_TYPE
        2:
            return OPERATION_TYPE;
        case // HAS_RESULT_SET
        3:
            return HAS_RESULT_SET;
        case // MODIFIED_ROW_COUNT
        4:
            return MODIFIED_ROW_COUNT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationId != null;
} 
{
    return this.operationType;
} 
{
    this.operationType = operationType;
} 
{
    return this.operationType != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __HASRESULTSET_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __MODIFIEDROWCOUNT_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_ID:
            return isSetOperationId();
        case OPERATION_TYPE:
            return isSetOperationType();
        case HAS_RESULT_SET:
            return isSetHasResultSet();
        case MODIFIED_ROW_COUNT:
            return isSetModifiedRowCount();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetOperationId()) {
        this.operationId = new THandleIdentifier(other.operationId);
    }
    if (other.isSetOperationType()) {
        this.operationType = other.operationType;
    }
    this.hasResultSet = other.hasResultSet;
    this.modifiedRowCount = other.modifiedRowCount;
} 
",,,1,107
TGetSchemasReq.java,27,501,0.05389221556886228," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
} 
",,,1,107
TFetchResultsReq.java,41,576,0.07118055555555555," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TFetchOrientation
 */
 
/**
 * @see TFetchOrientation
 */
 
/**
 * Returns true if field orientation is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field maxRows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field fetchType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        case // ORIENTATION
        2:
            return ORIENTATION;
        case // MAX_ROWS
        3:
            return MAX_ROWS;
        case // FETCH_TYPE
        4:
            return FETCH_TYPE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    return this.orientation;
} 
{
    this.orientation = orientation;
} 
{
    return this.orientation != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __MAXROWS_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __FETCHTYPE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
        case ORIENTATION:
            return isSetOrientation();
        case MAX_ROWS:
            return isSetMaxRows();
        case FETCH_TYPE:
            return isSetFetchType();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
    if (other.isSetOrientation()) {
        this.orientation = other.orientation;
    }
    this.maxRows = other.maxRows;
    this.fetchType = other.fetchType;
} 
",,,1,107
TStatusCode.java,13,34,0.38235294117647056,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return SUCCESS_STATUS;
        case 1:
            return SUCCESS_WITH_INFO_STATUS;
        case 2:
            return STILL_EXECUTING_STATUS;
        case 3:
            return ERROR_STATUS;
        case 4:
            return INVALID_HANDLE_STATUS;
        default:
            return null;
    }
} 
",,,,,1,0
TGetTableTypesResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TGetCatalogsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TTypeId.java,13,85,0.15294117647058825,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return BOOLEAN_TYPE;
        case 1:
            return TINYINT_TYPE;
        case 2:
            return SMALLINT_TYPE;
        case 3:
            return INT_TYPE;
        case 4:
            return BIGINT_TYPE;
        case 5:
            return FLOAT_TYPE;
        case 6:
            return DOUBLE_TYPE;
        case 7:
            return STRING_TYPE;
        case 8:
            return TIMESTAMP_TYPE;
        case 9:
            return BINARY_TYPE;
        case 10:
            return ARRAY_TYPE;
        case 11:
            return MAP_TYPE;
        case 12:
            return STRUCT_TYPE;
        case 13:
            return UNION_TYPE;
        case 14:
            return USER_DEFINED_TYPE;
        case 15:
            return DECIMAL_TYPE;
        case 16:
            return NULL_TYPE;
        case 17:
            return DATE_TYPE;
        case 18:
            return VARCHAR_TYPE;
        case 19:
            return CHAR_TYPE;
        case 20:
            return INTERVAL_YEAR_MONTH_TYPE;
        case 21:
            return INTERVAL_DAY_TIME_TYPE;
        default:
            return null;
    }
} 
",,,,,1,0
TGetDelegationTokenReq.java,27,485,0.05567010309278351," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field owner is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field renewer is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // OWNER
        2:
            return OWNER;
        case // RENEWER
        3:
            return RENEWER;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.owner != null;
} 
{
    return this.renewer != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case OWNER:
            return isSetOwner();
        case RENEWER:
            return isSetRenewer();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetOwner()) {
        this.owner = other.owner;
    }
    if (other.isSetRenewer()) {
        this.renewer = other.renewer;
    }
} 
",,,1,107
TI64Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Long> __this__values = new ArrayList<Long>();
        for (Long other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TCloseSessionReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TCloseOperationReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TColumn.java,17,642,0.0264797507788162," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // BOOL_VAL
        1:
            return BOOL_VAL;
        case // BYTE_VAL
        2:
            return BYTE_VAL;
        case // I16_VAL
        3:
            return I16_VAL;
        case // I32_VAL
        4:
            return I32_VAL;
        case // I64_VAL
        5:
            return I64_VAL;
        case // DOUBLE_VAL
        6:
            return DOUBLE_VAL;
        case // STRING_VAL
        7:
            return STRING_VAL;
        case // BINARY_VAL
        8:
            return BINARY_VAL;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TCancelDelegationTokenResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TOperationType.java,13,46,0.2826086956521739,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return EXECUTE_STATEMENT;
        case 1:
            return GET_TYPE_INFO;
        case 2:
            return GET_CATALOGS;
        case 3:
            return GET_SCHEMAS;
        case 4:
            return GET_TABLES;
        case 5:
            return GET_TABLE_TYPES;
        case 6:
            return GET_COLUMNS;
        case 7:
            return GET_FUNCTIONS;
        case 8:
            return UNKNOWN;
        default:
            return null;
    }
} 
",,,,,1,0
TGetOperationStatusResp.java,47,1150,0.0408695652173913," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TOperationState
 */
 
/**
 * @see TOperationState
 */
 
/**
 * Returns true if field operationState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sqlState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorMessage is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_STATE
        2:
            return OPERATION_STATE;
        case // SQL_STATE
        3:
            return SQL_STATE;
        case // ERROR_CODE
        4:
            return ERROR_CODE;
        case // ERROR_MESSAGE
        5:
            return ERROR_MESSAGE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationState;
} 
{
    this.operationState = operationState;
} 
{
    return this.operationState != null;
} 
{
    return this.sqlState != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __ERRORCODE_ISSET_ID);
} 
{
    return this.errorMessage != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_STATE:
            return isSetOperationState();
        case SQL_STATE:
            return isSetSqlState();
        case ERROR_CODE:
            return isSetErrorCode();
        case ERROR_MESSAGE:
            return isSetErrorMessage();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationState()) {
        this.operationState = other.operationState;
    }
    if (other.isSetSqlState()) {
        this.sqlState = other.sqlState;
    }
    this.errorCode = other.errorCode;
    if (other.isSetErrorMessage()) {
        this.errorMessage = other.errorMessage;
    }
} 
",,,1,107
TDoubleColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Double> __this__values = new ArrayList<Double>();
        for (Double other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TStructTypeEntry.java,25,357,0.0700280112044818," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field nameToTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // NAME_TO_TYPE_PTR
        1:
            return NAME_TO_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.nameToTypePtr != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case NAME_TO_TYPE_PTR:
            return isSetNameToTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetNameToTypePtr()) {
        Map<String, Integer> __this__nameToTypePtr = new HashMap<String, Integer>();
        for (Map.Entry<String, Integer> other_element : other.nameToTypePtr.entrySet()) {
            String other_element_key = other_element.getKey();
            Integer other_element_value = other_element.getValue();
            String __this__nameToTypePtr_copy_key = other_element_key;
            Integer __this__nameToTypePtr_copy_value = other_element_value;
            __this__nameToTypePtr.put(__this__nameToTypePtr_copy_key, __this__nameToTypePtr_copy_value);
        }
        this.nameToTypePtr = __this__nameToTypePtr;
    }
} 
",,,1,107
TBoolValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TByteColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Byte> __this__values = new ArrayList<Byte>();
        for (Byte other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TFetchResultsResp.java,28,502,0.055776892430278883," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field hasMoreRows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field results is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // HAS_MORE_ROWS
        2:
            return HAS_MORE_ROWS;
        case // RESULTS
        3:
            return RESULTS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __HASMOREROWS_ISSET_ID);
} 
{
    return this.results != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case HAS_MORE_ROWS:
            return isSetHasMoreRows();
        case RESULTS:
            return isSetResults();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    this.hasMoreRows = other.hasMoreRows;
    if (other.isSetResults()) {
        this.results = new TRowSet(other.results);
    }
} 
",,,1,107
TTypeQualifiers.java,25,359,0.06963788300835655," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field qualifiers is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // QUALIFIERS
        1:
            return QUALIFIERS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.qualifiers != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case QUALIFIERS:
            return isSetQualifiers();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetQualifiers()) {
        Map<String, TTypeQualifierValue> __this__qualifiers = new HashMap<String, TTypeQualifierValue>();
        for (Map.Entry<String, TTypeQualifierValue> other_element : other.qualifiers.entrySet()) {
            String other_element_key = other_element.getKey();
            TTypeQualifierValue other_element_value = other_element.getValue();
            String __this__qualifiers_copy_key = other_element_key;
            TTypeQualifierValue __this__qualifiers_copy_value = new TTypeQualifierValue(other_element_value);
            __this__qualifiers.put(__this__qualifiers_copy_key, __this__qualifiers_copy_value);
        }
        this.qualifiers = __this__qualifiers;
    }
} 
",,,1,107
TRenewDelegationTokenReq.java,26,395,0.06582278481012659," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TUnionTypeEntry.java,25,357,0.0700280112044818," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field nameToTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // NAME_TO_TYPE_PTR
        1:
            return NAME_TO_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.nameToTypePtr != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case NAME_TO_TYPE_PTR:
            return isSetNameToTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetNameToTypePtr()) {
        Map<String, Integer> __this__nameToTypePtr = new HashMap<String, Integer>();
        for (Map.Entry<String, Integer> other_element : other.nameToTypePtr.entrySet()) {
            String other_element_key = other_element.getKey();
            Integer other_element_value = other_element.getValue();
            String __this__nameToTypePtr_copy_key = other_element_key;
            Integer __this__nameToTypePtr_copy_value = other_element_value;
            __this__nameToTypePtr.put(__this__nameToTypePtr_copy_key, __this__nameToTypePtr_copy_value);
        }
        this.nameToTypePtr = __this__nameToTypePtr;
    }
} 
",,,1,107
TRowSet.java,30,781,0.03841229193341869," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field startRowOffset is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field rows is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field columns is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // START_ROW_OFFSET
        1:
            return START_ROW_OFFSET;
        case // ROWS
        2:
            return ROWS;
        case // COLUMNS
        3:
            return COLUMNS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __STARTROWOFFSET_ISSET_ID);
} 
{
    return this.rows != null;
} 
{
    return this.columns != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case START_ROW_OFFSET:
            return isSetStartRowOffset();
        case ROWS:
            return isSetRows();
        case COLUMNS:
            return isSetColumns();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.startRowOffset = other.startRowOffset;
    if (other.isSetRows()) {
        List<TRow> __this__rows = new ArrayList<TRow>();
        for (TRow other_element : other.rows) {
            __this__rows.add(new TRow(other_element));
        }
        this.rows = __this__rows;
    }
    if (other.isSetColumns()) {
        List<TColumn> __this__columns = new ArrayList<TColumn>();
        for (TColumn other_element : other.columns) {
            __this__columns.add(new TColumn(other_element));
        }
        this.columns = __this__columns;
    }
} 
",,,1,107
TColumnValue.java,17,586,0.02901023890784983," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // BOOL_VAL
        1:
            return BOOL_VAL;
        case // BYTE_VAL
        2:
            return BYTE_VAL;
        case // I16_VAL
        3:
            return I16_VAL;
        case // I32_VAL
        4:
            return I32_VAL;
        case // I64_VAL
        5:
            return I64_VAL;
        case // DOUBLE_VAL
        6:
            return DOUBLE_VAL;
        case // STRING_VAL
        7:
            return STRING_VAL;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TBinaryColumn.java,26,445,0.058426966292134834," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<ByteBuffer> __this__values = new ArrayList<ByteBuffer>();
        for (ByteBuffer other_element : other.values) {
            ByteBuffer temp_binary_element = org.apache.thrift.TBaseHelper.copyBinary(other_element);
            ;
            __this__values.add(temp_binary_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TFetchOrientation.java,13,37,0.35135135135135137,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return FETCH_NEXT;
        case 1:
            return FETCH_PRIOR;
        case 2:
            return FETCH_RELATIVE;
        case 3:
            return FETCH_ABSOLUTE;
        case 4:
            return FETCH_FIRST;
        case 5:
            return FETCH_LAST;
        default:
            return null;
    }
} 
",,,,,1,0
TCancelOperationReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TGetColumnsResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TCancelDelegationTokenReq.java,26,395,0.06582278481012659," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field delegationToken is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // DELEGATION_TOKEN
        2:
            return DELEGATION_TOKEN;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.delegationToken != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case DELEGATION_TOKEN:
            return isSetDelegationToken();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetDelegationToken()) {
        this.delegationToken = other.delegationToken;
    }
} 
",,,1,107
TGetSchemasResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TOpenSessionResp.java,40,650,0.06153846153846154," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * Returns true if field serverProtocolVersion is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field configuration is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // SERVER_PROTOCOL_VERSION
        2:
            return SERVER_PROTOCOL_VERSION;
        case // SESSION_HANDLE
        3:
            return SESSION_HANDLE;
        case // CONFIGURATION
        4:
            return CONFIGURATION;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.serverProtocolVersion;
} 
{
    this.serverProtocolVersion = serverProtocolVersion;
} 
{
    return this.serverProtocolVersion != null;
} 
{
    return this.sessionHandle != null;
} 
{
    return this.configuration != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case SERVER_PROTOCOL_VERSION:
            return isSetServerProtocolVersion();
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CONFIGURATION:
            return isSetConfiguration();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetServerProtocolVersion()) {
        this.serverProtocolVersion = other.serverProtocolVersion;
    }
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetConfiguration()) {
        Map<String, String> __this__configuration = new HashMap<String, String>();
        for (Map.Entry<String, String> other_element : other.configuration.entrySet()) {
            String other_element_key = other_element.getKey();
            String other_element_value = other_element.getValue();
            String __this__configuration_copy_key = other_element_key;
            String __this__configuration_copy_value = other_element_value;
            __this__configuration.put(__this__configuration_copy_key, __this__configuration_copy_value);
        }
        this.configuration = __this__configuration;
    }
} 
",,,1,107
TGetInfoReq.java,38,395,0.09620253164556962," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * @see TGetInfoType
 */
 
/**
 * @see TGetInfoType
 */
 
/**
 * Returns true if field infoType is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // INFO_TYPE
        2:
            return INFO_TYPE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.infoType;
} 
{
    this.infoType = infoType;
} 
{
    return this.infoType != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case INFO_TYPE:
            return isSetInfoType();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetInfoType()) {
        this.infoType = other.infoType;
    }
} 
",,,1,107
TOpenSessionReq.java,40,646,0.06191950464396285," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * @see TProtocolVersion
 */
 
/**
 * Returns true if field client_protocol is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field username is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field password is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field configuration is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // CLIENT_PROTOCOL
        1:
            return CLIENT_PROTOCOL;
        case // USERNAME
        2:
            return USERNAME;
        case // PASSWORD
        3:
            return PASSWORD;
        case // CONFIGURATION
        4:
            return CONFIGURATION;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.client_protocol;
} 
{
    this.client_protocol = client_protocol;
} 
{
    return this.client_protocol != null;
} 
{
    return this.username != null;
} 
{
    return this.password != null;
} 
{
    return this.configuration != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case CLIENT_PROTOCOL:
            return isSetClient_protocol();
        case USERNAME:
            return isSetUsername();
        case PASSWORD:
            return isSetPassword();
        case CONFIGURATION:
            return isSetConfiguration();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetClient_protocol()) {
        this.client_protocol = other.client_protocol;
    }
    if (other.isSetUsername()) {
        this.username = other.username;
    }
    if (other.isSetPassword()) {
        this.password = other.password;
    }
    if (other.isSetConfiguration()) {
        Map<String, String> __this__configuration = new HashMap<String, String>();
        for (Map.Entry<String, String> other_element : other.configuration.entrySet()) {
            String other_element_key = other_element.getKey();
            String other_element_value = other_element.getValue();
            String __this__configuration_copy_key = other_element_key;
            String __this__configuration_copy_value = other_element_value;
            __this__configuration.put(__this__configuration_copy_key, __this__configuration_copy_value);
        }
        this.configuration = __this__configuration;
    }
} 
",,,1,107
TGetInfoValue.java,17,513,0.03313840155945419," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // STRING_VALUE
        1:
            return STRING_VALUE;
        case // SMALL_INT_VALUE
        2:
            return SMALL_INT_VALUE;
        case // INTEGER_BITMASK
        3:
            return INTEGER_BITMASK;
        case // INTEGER_FLAG
        4:
            return INTEGER_FLAG;
        case // BINARY_VALUE
        5:
            return BINARY_VALUE;
        case // LEN_VALUE
        6:
            return LEN_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TDoubleValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TI16Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Short> __this__values = new ArrayList<Short>();
        for (Short other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TStringColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<String> __this__values = new ArrayList<String>();
        for (String other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TCancelOperationResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TPrimitiveTypeEntry.java,38,405,0.09382716049382717," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TTypeId
 */
 
/**
 * @see TTypeId
 */
 
/**
 * Returns true if field type is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field typeQualifiers is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPE
        1:
            return TYPE;
        case // TYPE_QUALIFIERS
        2:
            return TYPE_QUALIFIERS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.type;
} 
{
    this.type = type;
} 
{
    return this.type != null;
} 
{
    return this.typeQualifiers != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPE:
            return isSetType();
        case TYPE_QUALIFIERS:
            return isSetTypeQualifiers();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetType()) {
        this.type = other.type;
    }
    if (other.isSetTypeQualifiers()) {
        this.typeQualifiers = new TTypeQualifiers(other.typeQualifiers);
    }
} 
",,,1,107
TGetInfoType.java,13,160,0.08125,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return CLI_MAX_DRIVER_CONNECTIONS;
        case 1:
            return CLI_MAX_CONCURRENT_ACTIVITIES;
        case 2:
            return CLI_DATA_SOURCE_NAME;
        case 8:
            return CLI_FETCH_DIRECTION;
        case 13:
            return CLI_SERVER_NAME;
        case 14:
            return CLI_SEARCH_PATTERN_ESCAPE;
        case 17:
            return CLI_DBMS_NAME;
        case 18:
            return CLI_DBMS_VER;
        case 19:
            return CLI_ACCESSIBLE_TABLES;
        case 20:
            return CLI_ACCESSIBLE_PROCEDURES;
        case 23:
            return CLI_CURSOR_COMMIT_BEHAVIOR;
        case 25:
            return CLI_DATA_SOURCE_READ_ONLY;
        case 26:
            return CLI_DEFAULT_TXN_ISOLATION;
        case 28:
            return CLI_IDENTIFIER_CASE;
        case 29:
            return CLI_IDENTIFIER_QUOTE_CHAR;
        case 30:
            return CLI_MAX_COLUMN_NAME_LEN;
        case 31:
            return CLI_MAX_CURSOR_NAME_LEN;
        case 32:
            return CLI_MAX_SCHEMA_NAME_LEN;
        case 34:
            return CLI_MAX_CATALOG_NAME_LEN;
        case 35:
            return CLI_MAX_TABLE_NAME_LEN;
        case 43:
            return CLI_SCROLL_CONCURRENCY;
        case 46:
            return CLI_TXN_CAPABLE;
        case 47:
            return CLI_USER_NAME;
        case 72:
            return CLI_TXN_ISOLATION_OPTION;
        case 73:
            return CLI_INTEGRITY;
        case 81:
            return CLI_GETDATA_EXTENSIONS;
        case 85:
            return CLI_NULL_COLLATION;
        case 86:
            return CLI_ALTER_TABLE;
        case 90:
            return CLI_ORDER_BY_COLUMNS_IN_SELECT;
        case 94:
            return CLI_SPECIAL_CHARACTERS;
        case 97:
            return CLI_MAX_COLUMNS_IN_GROUP_BY;
        case 98:
            return CLI_MAX_COLUMNS_IN_INDEX;
        case 99:
            return CLI_MAX_COLUMNS_IN_ORDER_BY;
        case 100:
            return CLI_MAX_COLUMNS_IN_SELECT;
        case 101:
            return CLI_MAX_COLUMNS_IN_TABLE;
        case 102:
            return CLI_MAX_INDEX_SIZE;
        case 104:
            return CLI_MAX_ROW_SIZE;
        case 105:
            return CLI_MAX_STATEMENT_LEN;
        case 106:
            return CLI_MAX_TABLES_IN_SELECT;
        case 107:
            return CLI_MAX_USER_NAME_LEN;
        case 115:
            return CLI_OJ_CAPABILITIES;
        case 10000:
            return CLI_XOPEN_CLI_YEAR;
        case 10001:
            return CLI_CURSOR_SENSITIVITY;
        case 10002:
            return CLI_DESCRIBE_PARAMETER;
        case 10003:
            return CLI_CATALOG_NAME;
        case 10004:
            return CLI_COLLATION_SEQ;
        case 10005:
            return CLI_MAX_IDENTIFIER_LEN;
        default:
            return null;
    }
} 
",,,,,1,0
TGetTablesResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TTableSchema.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field columns is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COLUMNS
        1:
            return COLUMNS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.columns != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COLUMNS:
            return isSetColumns();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetColumns()) {
        List<TColumnDesc> __this__columns = new ArrayList<TColumnDesc>();
        for (TColumnDesc other_element : other.columns) {
            __this__columns.add(new TColumnDesc(other_element));
        }
        this.columns = __this__columns;
    }
} 
",,,1,107
TProtocolVersion.java,13,49,0.2653061224489796,,"/**
 * Get the integer value of this enum value, as defined in the Thrift IDL.
 */
 
/**
 * Find a the enum type by its integer value, as defined in the Thrift IDL.
 * @return null if the value is not found.
 */
 
","{
    return value;
} 
{
    switch(value) {
        case 0:
            return HIVE_CLI_SERVICE_PROTOCOL_V1;
        case 1:
            return HIVE_CLI_SERVICE_PROTOCOL_V2;
        case 2:
            return HIVE_CLI_SERVICE_PROTOCOL_V3;
        case 3:
            return HIVE_CLI_SERVICE_PROTOCOL_V4;
        case 4:
            return HIVE_CLI_SERVICE_PROTOCOL_V5;
        case 5:
            return HIVE_CLI_SERVICE_PROTOCOL_V6;
        case 6:
            return HIVE_CLI_SERVICE_PROTOCOL_V7;
        case 7:
            return HIVE_CLI_SERVICE_PROTOCOL_V8;
        default:
            return null;
    }
} 
",,,,,1,0
TTypeQualifierValue.java,17,301,0.05647840531561462," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // I32_VALUE
        1:
            return I32_VALUE;
        case // STRING_VALUE
        2:
            return STRING_VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TTypeDesc.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field types is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPES
        1:
            return TYPES;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.types != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPES:
            return isSetTypes();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetTypes()) {
        List<TTypeEntry> __this__types = new ArrayList<TTypeEntry>();
        for (TTypeEntry other_element : other.types) {
            __this__types.add(new TTypeEntry(other_element));
        }
        this.types = __this__types;
    }
} 
",,,1,107
TGetResultSetMetadataResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schema is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // SCHEMA
        2:
            return SCHEMA;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.schema != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case SCHEMA:
            return isSetSchema();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetSchema()) {
        this.schema = new TTableSchema(other.schema);
    }
} 
",,,1,107
TArrayTypeEntry.java,26,297,0.08754208754208755," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field objectTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OBJECT_TYPE_PTR
        1:
            return OBJECT_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __OBJECTTYPEPTR_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OBJECT_TYPE_PTR:
            return isSetObjectTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.objectTypePtr = other.objectTypePtr;
} 
",,,1,107
TStatus.java,42,730,0.057534246575342465," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * @see TStatusCode
 */
 
/**
 * @see TStatusCode
 */
 
/**
 * Returns true if field statusCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field infoMessages is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field sqlState is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorCode is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field errorMessage is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS_CODE
        1:
            return STATUS_CODE;
        case // INFO_MESSAGES
        2:
            return INFO_MESSAGES;
        case // SQL_STATE
        3:
            return SQL_STATE;
        case // ERROR_CODE
        4:
            return ERROR_CODE;
        case // ERROR_MESSAGE
        5:
            return ERROR_MESSAGE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.statusCode;
} 
{
    this.statusCode = statusCode;
} 
{
    return this.statusCode != null;
} 
{
    return this.infoMessages != null;
} 
{
    return this.sqlState != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __ERRORCODE_ISSET_ID);
} 
{
    return this.errorMessage != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS_CODE:
            return isSetStatusCode();
        case INFO_MESSAGES:
            return isSetInfoMessages();
        case SQL_STATE:
            return isSetSqlState();
        case ERROR_CODE:
            return isSetErrorCode();
        case ERROR_MESSAGE:
            return isSetErrorMessage();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetStatusCode()) {
        this.statusCode = other.statusCode;
    }
    if (other.isSetInfoMessages()) {
        List<String> __this__infoMessages = new ArrayList<String>();
        for (String other_element : other.infoMessages) {
            __this__infoMessages.add(other_element);
        }
        this.infoMessages = __this__infoMessages;
    }
    if (other.isSetSqlState()) {
        this.sqlState = other.sqlState;
    }
    this.errorCode = other.errorCode;
    if (other.isSetErrorMessage()) {
        this.errorMessage = other.errorMessage;
    }
} 
",,,1,107
TRow.java,25,351,0.07122507122507123," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field colVals is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COL_VALS
        1:
            return COL_VALS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.colVals != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COL_VALS:
            return isSetColVals();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetColVals()) {
        List<TColumnValue> __this__colVals = new ArrayList<TColumnValue>();
        for (TColumnValue other_element : other.colVals) {
            __this__colVals.add(new TColumnValue(other_element));
        }
        this.colVals = __this__colVals;
    }
} 
",,,1,107
TGetTablesReq.java,29,739,0.03924221921515562," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field catalogName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field schemaName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field tableTypes is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        case // CATALOG_NAME
        2:
            return CATALOG_NAME;
        case // SCHEMA_NAME
        3:
            return SCHEMA_NAME;
        case // TABLE_NAME
        4:
            return TABLE_NAME;
        case // TABLE_TYPES
        5:
            return TABLE_TYPES;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    return this.catalogName != null;
} 
{
    return this.schemaName != null;
} 
{
    return this.tableName != null;
} 
{
    return this.tableTypes != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
        case CATALOG_NAME:
            return isSetCatalogName();
        case SCHEMA_NAME:
            return isSetSchemaName();
        case TABLE_NAME:
            return isSetTableName();
        case TABLE_TYPES:
            return isSetTableTypes();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
    if (other.isSetCatalogName()) {
        this.catalogName = other.catalogName;
    }
    if (other.isSetSchemaName()) {
        this.schemaName = other.schemaName;
    }
    if (other.isSetTableName()) {
        this.tableName = other.tableName;
    }
    if (other.isSetTableTypes()) {
        List<String> __this__tableTypes = new ArrayList<String>();
        for (String other_element : other.tableTypes) {
            __this__tableTypes.add(other_element);
        }
        this.tableTypes = __this__tableTypes;
    }
} 
",,,1,107
TCLIServiceConstants.java,6,89,0.06741573033707865,,,,,,,,1,0
TExecuteStatementResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TCloseOperationResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TStringValue.java,25,306,0.08169934640522876," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.value != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValue()) {
        this.value = other.value;
    }
} 
",,,1,107
TTypeEntry.java,17,530,0.03207547169811321," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
","{
    switch(fieldId) {
        case // PRIMITIVE_ENTRY
        1:
            return PRIMITIVE_ENTRY;
        case // ARRAY_ENTRY
        2:
            return ARRAY_ENTRY;
        case // MAP_ENTRY
        3:
            return MAP_ENTRY;
        case // STRUCT_ENTRY
        4:
            return STRUCT_ENTRY;
        case // UNION_ENTRY
        5:
            return UNION_ENTRY;
        case // USER_DEFINED_TYPE_ENTRY
        6:
            return USER_DEFINED_TYPE_ENTRY;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
",,,,,1,107
TGetTypeInfoReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
TRenewDelegationTokenResp.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
} 
",,,1,107
TColumnDesc.java,29,582,0.04982817869415808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field columnName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field typeDesc is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field position is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field comment is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // COLUMN_NAME
        1:
            return COLUMN_NAME;
        case // TYPE_DESC
        2:
            return TYPE_DESC;
        case // POSITION
        3:
            return POSITION;
        case // COMMENT
        4:
            return COMMENT;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.columnName != null;
} 
{
    return this.typeDesc != null;
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __POSITION_ISSET_ID);
} 
{
    return this.comment != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case COLUMN_NAME:
            return isSetColumnName();
        case TYPE_DESC:
            return isSetTypeDesc();
        case POSITION:
            return isSetPosition();
        case COMMENT:
            return isSetComment();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    if (other.isSetColumnName()) {
        this.columnName = other.columnName;
    }
    if (other.isSetTypeDesc()) {
        this.typeDesc = new TTypeDesc(other.typeDesc);
    }
    this.position = other.position;
    if (other.isSetComment()) {
        this.comment = other.comment;
    }
} 
",,,1,107
TByteValue.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TI32Column.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Integer> __this__values = new ArrayList<Integer>();
        for (Integer other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TGetTypeInfoResp.java,26,410,0.06341463414634146," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field status is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // STATUS
        1:
            return STATUS;
        case // OPERATION_HANDLE
        2:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.status != null;
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case STATUS:
            return isSetStatus();
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetStatus()) {
        this.status = new TStatus(other.status);
    }
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
THandleIdentifier.java,26,404,0.06435643564356436," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field guid is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field secret is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // GUID
        1:
            return GUID;
        case // SECRET
        2:
            return SECRET;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.guid != null;
} 
{
    return this.secret != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case GUID:
            return isSetGuid();
        case SECRET:
            return isSetSecret();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetGuid()) {
        this.guid = org.apache.thrift.TBaseHelper.copyBinary(other.guid);
        ;
    }
    if (other.isSetSecret()) {
        this.secret = org.apache.thrift.TBaseHelper.copyBinary(other.secret);
        ;
    }
} 
",,,1,107
TGetOperationStatusReq.java,27,401,0.06733167082294264," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TCLIService.java,804,14414,0.055779103649229916," The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. | The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field req is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field success is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // REQ
        1:
            return REQ;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.req != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case REQ:
            return isSetReq();
    }
    throw new IllegalStateException();
} 
{
    switch(fieldId) {
        case // SUCCESS
        0:
            return SUCCESS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.success != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SUCCESS:
            return isSetSuccess();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetReq()) {
        this.req = new TOpenSessionReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TOpenSessionResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCloseSessionReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCloseSessionResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetInfoReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetInfoResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TExecuteStatementReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TExecuteStatementResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTypeInfoReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTypeInfoResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetCatalogsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetCatalogsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetSchemasReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetSchemasResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTablesReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTablesResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetTableTypesReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetTableTypesResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetColumnsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetColumnsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetFunctionsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetFunctionsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetOperationStatusReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetOperationStatusResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCancelOperationReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCancelOperationResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCloseOperationReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCloseOperationResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetResultSetMetadataReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetResultSetMetadataResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TFetchResultsReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TFetchResultsResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TGetDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TGetDelegationTokenResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TCancelDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TCancelDelegationTokenResp(other.success);
    }
} 
{
    if (other.isSetReq()) {
        this.req = new TRenewDelegationTokenReq(other.req);
    }
} 
{
    if (other.isSetSuccess()) {
        this.success = new TRenewDelegationTokenResp(other.success);
    }
} 
",,,1,4103
TI16Value.java,26,302,0.08609271523178808," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field value is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUE
        1:
            return VALUE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUE_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUE:
            return isSetValue();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.value = other.value;
} 
",,,1,107
TGetResultSetMetadataReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field operationHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // OPERATION_HANDLE
        1:
            return OPERATION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.operationHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case OPERATION_HANDLE:
            return isSetOperationHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetOperationHandle()) {
        this.operationHandle = new TOperationHandle(other.operationHandle);
    }
} 
",,,1,107
TMapTypeEntry.java,27,381,0.07086614173228346," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field keyTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field valueTypePtr is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // KEY_TYPE_PTR
        1:
            return KEY_TYPE_PTR;
        case // VALUE_TYPE_PTR
        2:
            return VALUE_TYPE_PTR;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __KEYTYPEPTR_ISSET_ID);
} 
{
    return EncodingUtils.testBit(__isset_bitfield, __VALUETYPEPTR_ISSET_ID);
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case KEY_TYPE_PTR:
            return isSetKeyTypePtr();
        case VALUE_TYPE_PTR:
            return isSetValueTypePtr();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    __isset_bitfield = other.__isset_bitfield;
    this.keyTypePtr = other.keyTypePtr;
    this.valueTypePtr = other.valueTypePtr;
} 
",,,1,107
TUserDefinedTypeEntry.java,25,300,0.08333333333333333," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field typeClassName is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // TYPE_CLASS_NAME
        1:
            return TYPE_CLASS_NAME;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.typeClassName != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case TYPE_CLASS_NAME:
            return isSetTypeClassName();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetTypeClassName()) {
        this.typeClassName = other.typeClassName;
    }
} 
",,,1,107
TBoolColumn.java,26,443,0.05869074492099323," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field values is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field nulls is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // VALUES
        1:
            return VALUES;
        case // NULLS
        2:
            return NULLS;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.values != null;
} 
{
    return this.nulls != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case VALUES:
            return isSetValues();
        case NULLS:
            return isSetNulls();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetValues()) {
        List<Boolean> __this__values = new ArrayList<Boolean>();
        for (Boolean other_element : other.values) {
            __this__values.add(other_element);
        }
        this.values = __this__values;
    }
    if (other.isSetNulls()) {
        this.nulls = org.apache.thrift.TBaseHelper.copyBinary(other.nulls);
        ;
    }
} 
",,,1,107
TGetTableTypesReq.java,25,305,0.08196721311475409," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ","/**
 * Find the _Fields constant that matches fieldId, or null if its not found.
 */
 
/**
 * Find the _Fields constant that matches fieldId, throwing an exception
 * if it is not found.
 */
 
/**
 * Find the _Fields constant that matches name, or null if its not found.
 */
 
/**
 * Returns true if field sessionHandle is set (has been assigned a value) and false otherwise
 */
 
/**
 * Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise
 */
 
","{
    switch(fieldId) {
        case // SESSION_HANDLE
        1:
            return SESSION_HANDLE;
        default:
            return null;
    }
} 
{
    _Fields fields = findByThriftId(fieldId);
    if (fields == null)
        throw new IllegalArgumentException(""Field "" + fieldId + "" doesn't exist!"");
    return fields;
} 
{
    return byName.get(name);
} 
{
    return this.sessionHandle != null;
} 
{
    if (field == null) {
        throw new IllegalArgumentException();
    }
    switch(field) {
        case SESSION_HANDLE:
            return isSetSessionHandle();
    }
    throw new IllegalStateException();
} 
","/**
 * Performs a deep copy on <i>other</i>.
 */
 
","{
    if (other.isSetSessionHandle()) {
        this.sessionHandle = new TSessionHandle(other.sessionHandle);
    }
} 
",,,1,107
ServiceUtils.java,29,39,0.7435897435897436,,,,,,,,1,0
HttpAuthUtils.java,56,117,0.47863247863247865,"
 * Utility functions for HTTP mode authentication.
 |
   * We'll create an instance of this class within a doAs block so that the client's TGT credentials
   * can be read from the Subject
   ","/**
 * @return Stringified Base64 encoded kerberosAuthHeader on success
 * @throws Exception
 */
 
/**
 * Creates and returns a HS2 cookie token.
 * @param clientUserName Client User name.
 * @return An unsigned cookie token generated from input parameters.
 * The final cookie generated is of the following format :
 * {@code cu=<username>&rn=<randomNumber>&s=<cookieSignature>}
 */
 
/**
 * Parses a cookie token to retrieve client user name.
 * @param tokenStr Token String.
 * @return A valid user name if input is of valid format, else returns null.
 */
 
/**
 * Splits the cookie token into attributes pairs.
 * @param str input token.
 * @return a map with the attribute pairs of the token if the input is valid.
 * Else, returns null.
 */
 
","{
    String serverPrincipal = ShimLoader.getHadoopThriftAuthBridge().getServerPrincipal(principal, host);
    if (assumeSubject) {
        // With this option, we're assuming that the external application,
        // using the JDBC driver has done a JAAS kerberos login already
        AccessControlContext context = AccessController.getContext();
        Subject subject = Subject.getSubject(context);
        if (subject == null) {
            throw new Exception(""The Subject is not set"");
        }
        return Subject.doAs(subject, new HttpKerberosClientAction(serverPrincipal, serverHttpUrl));
    } else {
        // JAAS login from ticket cache to setup the client UserGroupInformation
        UserGroupInformation clientUGI = ShimLoader.getHadoopThriftAuthBridge().getCurrentUGIWithConf(""kerberos"");
        return clientUGI.doAs(new HttpKerberosClientAction(serverPrincipal, serverHttpUrl));
    }
} 
{
    StringBuffer sb = new StringBuffer();
    sb.append(COOKIE_CLIENT_USER_NAME).append(COOKIE_KEY_VALUE_SEPARATOR).append(clientUserName).append(COOKIE_ATTR_SEPARATOR);
    sb.append(COOKIE_CLIENT_RAND_NUMBER).append(COOKIE_KEY_VALUE_SEPARATOR).append((new Random(System.currentTimeMillis())).nextLong());
    return sb.toString();
} 
{
    Map<String, String> map = splitCookieToken(tokenStr);
    if (!map.keySet().equals(COOKIE_ATTRIBUTES)) {
        LOG.error(""Invalid token with missing attributes "" + tokenStr);
        return null;
    }
    return map.get(COOKIE_CLIENT_USER_NAME);
} 
{
    Map<String, String> map = new HashMap<String, String>();
    StringTokenizer st = new StringTokenizer(tokenStr, COOKIE_ATTR_SEPARATOR);
    while (st.hasMoreTokens()) {
        String part = st.nextToken();
        int separator = part.indexOf(COOKIE_KEY_VALUE_SEPARATOR);
        if (separator == -1) {
            LOG.error(""Invalid token string "" + tokenStr);
            return null;
        }
        String key = part.substring(0, separator);
        String value = part.substring(separator + 1);
        map.put(key, value);
    }
    return map;
} 
",,,,,4,185
HiveAuthFactory.java,37,364,0.10164835164835165,"
 * This class helps in some aspects of authentication. It creates the proper Thrift classes for the
 * given configuration as well as helps with authenticating requests.
 ","/**
 * Returns the thrift processor factory for HiveServer2 running in binary mode
 * @param service
 * @return
 * @throws LoginException
 */
 
","{
    if (authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.getAuthName())) {
        return KerberosSaslHelper.getKerberosProcessorFactory(saslServer, service);
    } else {
        return PlainSaslHelper.getPlainProcessorFactory(service);
    }
} 
",,,,,2,167
KerberosSaslHelper.java,17,80,0.2125,,,,,,,,1,0
TSetIpAddressProcessor.java,25,76,0.32894736842105265,"
 * This class is responsible for setting the ipAddress for operations executed via HiveServer2.
 *
 * - IP address is only set for operations that calls listeners with hookContext
 * - IP address is only set if the underlying transport mechanism is socket
 *
 * @see org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext
 ",,,,,,,6,311
PlainSaslHelper.java,18,114,0.15789473684210525,,,,,,,,1,0
HiveServer2.java,57,208,0.27403846153846156,"
 * HiveServer2.
 *
 |
   * ServerOptionsProcessor.
   * Process arguments given to HiveServer2 (-hiveconf property=value)
   * Set properties in System properties
   * Create an appropriate response object,
   * which has executor to execute the appropriate command based on the parsed options.
   |
   * The response sent back from {@link ServerOptionsProcessor#parse(String[])}
   |
   * The executor interface for running the appropriate HiveServer2 command based on parsed options
   |
   * HelpOptionExecutor: executes the --help option by printing out the usage
   |
   * StartOptionExecutor: starts HiveServer2.
   * This is the default executor, when no option is specified.
   ",,,,,,,17,657
ThreadWithGarbageCleanup.java,29,38,0.7631578947368421,"
 * A HiveServer2 thread used to construct new server threads.
 * In particular, this thread ensures an orderly cleanup,
 * when killed by its corresponding ExecutorService.
 ","/**
 * Add any Thread specific garbage cleanup code here.
 * Currently, it shuts down the RawStore object for this thread if it is not null.
 */
 
/**
 * Cache the ThreadLocal RawStore object. Called from the corresponding thread.
 */
 
","{
    cleanRawStore();
    super.finalize();
} 
{
    Long threadId = this.getId();
    RawStore threadLocalRawStore = HiveMetaStore.HMSHandler.getRawStore();
    if (threadLocalRawStore != null && !threadRawStoreMap.containsKey(threadId)) {
        LOG.debug(""Adding RawStore: "" + threadLocalRawStore + "", for the thread: "" + this.getName() + "" to threadRawStoreMap for future cleanup."");
        threadRawStoreMap.put(threadId, threadLocalRawStore);
    }
} 
",,,,,3,168
MetadataOperation.java,44,75,0.5866666666666667,"
 * MetadataOperation.
 *
 ","/**
 * Convert wildchars and escape sequence from JDBC format to datanucleous/regex
 */
 
/**
 * Convert wildchars and escape sequence of schema pattern from JDBC format to datanucleous/regex
 * The schema pattern treats empty string also as wildchar
 */
 
/**
 * Convert a pattern containing JDBC catalog search wildcards into
 * Java regex patterns.
 *
 * @param pattern input which may contain '%' or '_' wildcard characters, or
 * these characters escaped using {@link #getSearchStringEscape()}.
 * @return replace %/_ with regex search characters, also handle escaped
 * characters.
 *
 * The datanucleus module expects the wildchar as '*'. The columns search on the
 * other hand is done locally inside the hive code and that requires the regex wildchar
 * format '.*'  This is driven by the datanucleusFormat flag.
 */
 
","{
    if (pattern == null) {
        return convertPattern(""%"", true);
    } else {
        return convertPattern(pattern, datanucleusFormat);
    }
} 
{
    if ((pattern == null) || pattern.isEmpty()) {
        return convertPattern(""%"", true);
    } else {
        return convertPattern(pattern, true);
    }
} 
{
    String wStr;
    if (datanucleusFormat) {
        wStr = ""*"";
    } else {
        wStr = "".*"";
    }
    return pattern.replaceAll(""([^\\\\])%"", ""$1"" + wStr).replaceAll(""\\\\%"", ""%"").replaceAll(""^%"", wStr).replaceAll(""([^\\\\])_"", ""$1."").replaceAll(""\\\\_"", ""_"").replaceAll(""^_"", ""."");
} 
",,,,,2,22
GetFunctionsOperation.java,29,106,0.27358490566037735,"
 * GetFunctionsOperation.
 *
 ",,,,,,,2,26
OperationManager.java,30,238,0.12605042016806722,"
 * OperationManager.
 *
 ",,,,,,,2,21
ClassicTableTypeMapping.java,24,61,0.39344262295081966,"
 * ClassicTableTypeMapping.
 * Classic table type mapping :
 *  Managed Table to Table
 *  External Table to Table
 *  Virtual View to View
 ",,,,,,,5,131
HiveTableTypeMapping.java,22,22,1.0,"
 * HiveTableTypeMapping.
 * Default table type mapping
 *
 ",,,,,,,3,53
Operation.java,86,333,0.25825825825825827,,"/**
 * Invoked before runInternal().
 * Set up some preconditions, or configurations.
 */
 
/**
 * Invoked after runInternal(), even if an exception is thrown in runInternal().
 * Clean up resources, which was set up in beforeRun().
 */
 
/**
 * Implemented by subclass of Operation class to execute specific behaviors.
 * @throws HiveSQLException
 */
 
/**
 * Verify if the given fetch orientation is part of the default orientation types.
 * @param orientation
 * @throws HiveSQLException
 */
 
/**
 * Verify if the given fetch orientation is part of the supported orientation types.
 * @param orientation
 * @param supportedOrientations
 * @throws HiveSQLException
 */
 
","{
    createOperationLog();
} 
{
    unregisterOperationLog();
} 
runInternal 
{
    validateFetchOrientation(orientation, DEFAULT_FETCH_ORIENTATION_SET);
} 
{
    if (!supportedOrientations.contains(orientation)) {
        throw new HiveSQLException(""The fetch type "" + orientation.toString() + "" is not supported for this resultset"", ""HY106"");
    }
} 
",,,,,1,0
GetSchemasOperation.java,27,59,0.4576271186440678,"
 * GetSchemasOperation.
 *
 ",,,,,,,2,24
ExecuteStatementOperation.java,17,57,0.2982456140350877,,,,,,,,1,0
SQLOperation.java,86,333,0.25825825825825827,"
 * SQLOperation.
 *
 ","/**
 * Compile the query and extract metadata
 * @param sqlOperationConf
 * @throws HiveSQLException
 */
 
/**
 * Returns the current UGI on the stack
 * @param opConfig
 * @return UserGroupInformation
 * @throws HiveSQLException
 */
 
/**
 * Returns the ThreadLocal Hive for the current thread
 * @return Hive
 * @throws HiveSQLException
 */
 
/**
 * If there are query specific settings to overlay, then create a copy of config
 * There are two cases we need to clone the session config that's being passed to hive driver
 * 1. Async query -
 *    If the client changes a config setting, that shouldn't reflect in the execution already underway
 * 2. confOverlay -
 *    The query specific settings should only be applied to the query config and not session
 * @return new configuration
 * @throws HiveSQLException
 */
 
","{
    setState(OperationState.RUNNING);
    try {
        driver = new Driver(sqlOperationConf, getParentSession().getUserName());
        // set the operation handle information in Driver, so that thrift API users
        // can use the operation handle they receive, to lookup query information in
        // Yarn ATS
        String guid64 = Base64.encodeBase64URLSafeString(getHandle().getHandleIdentifier().toTHandleIdentifier().getGuid()).trim();
        driver.setOperationId(guid64);
        // In Hive server mode, we are not able to retry in the FetchTask
        // case, when calling fetch queries since execute() has returned.
        // For now, we disable the test attempts.
        driver.setTryCount(Integer.MAX_VALUE);
        String subStatement = new VariableSubstitution().substitute(sqlOperationConf, statement);
        response = driver.compileAndRespond(subStatement);
        if (0 != response.getResponseCode()) {
            throw toSQLException(""Error while compiling statement"", response);
        }
        mResultSchema = driver.getSchema();
        // hasResultSet should be true only if the query has a FetchTask
        // ""explain"" is an exception for now
        if (driver.getPlan().getFetchTask() != null) {
            // Schema has to be set
            if (mResultSchema == null || !mResultSchema.isSetFieldSchemas()) {
                throw new HiveSQLException(""Error compiling query: Schema and FieldSchema "" + ""should be set when query plan has a FetchTask"");
            }
            resultSchema = new TableSchema(mResultSchema);
            setHasResultSet(true);
        } else {
            setHasResultSet(false);
        }
        // Set hasResultSet true if the plan has ExplainTask
        // TODO explain should use a FetchTask for reading
        for (Task<? extends Serializable> task : driver.getPlan().getRootTasks()) {
            if (task.getClass() == ExplainTask.class) {
                resultSchema = new TableSchema(mResultSchema);
                setHasResultSet(true);
                break;
            }
        }
    } catch (HiveSQLException e) {
        setState(OperationState.ERROR);
        throw e;
    } catch (Exception e) {
        setState(OperationState.ERROR);
        throw new HiveSQLException(""Error running query: "" + e.toString(), e);
    }
} 
{
    try {
        return Utils.getUGI();
    } catch (Exception e) {
        throw new HiveSQLException(""Unable to get current user"", e);
    }
} 
{
    try {
        return Hive.get();
    } catch (HiveException e) {
        throw new HiveSQLException(""Failed to get ThreadLocal Hive object"", e);
    }
} 
{
    HiveConf sqlOperationConf = getParentSession().getHiveConf();
    if (!getConfOverlay().isEmpty() || shouldRunAsync()) {
        // clone the parent session config for this query
        sqlOperationConf = new HiveConf(sqlOperationConf);
        // apply overlay query specific settings, if any
        for (Map.Entry<String, String> confEntry : getConfOverlay().entrySet()) {
            try {
                sqlOperationConf.verifyAndSet(confEntry.getKey(), confEntry.getValue());
            } catch (IllegalArgumentException e) {
                throw new HiveSQLException(""Error applying statement specific settings"", e);
            }
        }
    }
    return sqlOperationConf;
} 
",,,,,2,17
HiveCommandOperation.java,42,152,0.27631578947368424,"
 * Executes a HiveCommand
 ","/**
 * Reads the temporary results for non-Hive (non-Driver) commands to the
 * resulting List of strings.
 * @param nLines number of lines read at once. If it is <= 0, then read all lines.
 */
 
","{
    if (resultReader == null) {
        SessionState sessionState = getParentSession().getSessionState();
        File tmp = sessionState.getTmpOutputFile();
        try {
            resultReader = new BufferedReader(new FileReader(tmp));
        } catch (FileNotFoundException e) {
            LOG.error(""File "" + tmp + "" not found. "", e);
            throw new HiveSQLException(e);
        }
    }
    List<String> results = new ArrayList<String>();
    for (int i = 0; i < nLines || nLines <= 0; ++i) {
        try {
            String line = resultReader.readLine();
            if (line == null) {
                // reached the end of the result file
                break;
            } else {
                results.add(line);
            }
        } catch (IOException e) {
            LOG.error(""Reading temp results encountered an exception: "", e);
            throw new HiveSQLException(e);
        }
    }
    return results;
} 
",,,"/**
 * For processors other than Hive queries (Driver), they output to session.out (a temp file)
 * first and the fetchOne/fetchN/fetchAll functions get the output from pipeIn.
 */
 
","Field resultReader
",1,25
GetTypeInfoOperation.java,27,105,0.2571428571428571,"
 * GetTypeInfoOperation.
 *
 ",,,,,,,2,25
GetColumnsOperation.java,27,202,0.13366336633663367,"
 * GetColumnsOperation.
 *
 ",,,,,,,2,24
TableTypeMapping.java,24,61,0.39344262295081966,,"/**
 * Map client's table type name to hive's table type
 * @param clientTypeName
 * @return
 */
 
/**
 * Map hive's table type name to client's table type
 * @param hiveTypeName
 * @return
 */
 
/**
 * Get all the table types of this mapping
 * @return
 */
 
","mapToHiveType 
mapToClientType 
getTableTypeNames 
",,,,,1,0
GetCatalogsOperation.java,27,45,0.6,"
 * GetCatalogsOperation.
 *
 ",,,,,,,2,25
GetTablesOperation.java,27,104,0.25961538461538464,"
 * GetTablesOperation.
 *
 ",,,,,,,2,23
GetTableTypesOperation.java,27,56,0.48214285714285715,"
 * GetTableTypesOperation.
 *
 ",,,,,,,2,27
OperationType.java,13,46,0.2826086956521739,"
 * OperationType.
 *
 ",,,,,,,2,18
ICLIService.java,17,65,0.26153846153846155,,,,,,,,1,0
FetchOrientation.java,13,37,0.35135135135135137,"
 * FetchOrientation.
 *
 ",,,,,,,2,21
Type.java,85,232,0.36637931034482757,"
 * Type.
 *
 ","/**
 * Radix for this type (typically either 2 or 10)
 * Null is returned for data types where this is not applicable.
 */
 
/**
 * Maximum precision for numeric types.
 * Returns null for non-numeric types.
 * @return
 */
 
/**
 * Prefix used to quote a literal of this type (may be null)
 */
 
/**
 * Suffix used to quote a literal of this type (may be null)
 * @return
 */
 
/**
 * Can you use NULL for this type?
 * @return
 * DatabaseMetaData.typeNoNulls - does not allow NULL values
 * DatabaseMetaData.typeNullable - allows NULL values
 * DatabaseMetaData.typeNullableUnknown - nullability unknown
 */
 
/**
 * Is the type case sensitive?
 * @return
 */
 
/**
 * Parameters used in creating the type (may be null)
 * @return
 */
 
/**
 * Can you use WHERE based on this type?
 * @return
 * DatabaseMetaData.typePredNone - No support
 * DatabaseMetaData.typePredChar - Only support with WHERE .. LIKE
 * DatabaseMetaData.typePredBasic - Supported except for WHERE .. LIKE
 * DatabaseMetaData.typeSearchable - Supported for all WHERE ..
 */
 
/**
 * Is this type unsigned?
 * @return
 */
 
/**
 * Can this type represent money?
 * @return
 */
 
/**
 * Can this type be used for an auto-increment value?
 * @return
 */
 
/**
 * Localized version of type name (may be null).
 * @return
 */
 
/**
 * Minimum scale supported for this type
 * @return
 */
 
/**
 * Maximum scale supported for this type
 * @return
 */
 
","{
    if (this.isNumericType()) {
        return 10;
    }
    return null;
} 
{
    switch(this) {
        case TINYINT_TYPE:
            return 3;
        case SMALLINT_TYPE:
            return 5;
        case INT_TYPE:
            return 10;
        case BIGINT_TYPE:
            return 19;
        case FLOAT_TYPE:
            return 7;
        case DOUBLE_TYPE:
            return 15;
        case DECIMAL_TYPE:
            return HiveDecimal.MAX_PRECISION;
        default:
            return null;
    }
} 
{
    return null;
} 
{
    return null;
} 
{
    // All Hive types are nullable
    return DatabaseMetaData.typeNullable;
} 
{
    switch(this) {
        case STRING_TYPE:
            return true;
        default:
            return false;
    }
} 
{
    return null;
} 
{
    if (isPrimitiveType()) {
        return DatabaseMetaData.typeSearchable;
    }
    return DatabaseMetaData.typePredNone;
} 
{
    if (isNumericType()) {
        return false;
    }
    return true;
} 
{
    return false;
} 
{
    return false;
} 
{
    return null;
} 
{
    return 0;
} 
{
    return 0;
} 
",,,,,2,9
SessionHandle.java,25,305,0.08196721311475409,"
 * SessionHandle.
 *
 ",,,,,,,2,18
ColumnValue.java,17,586,0.02901023890784983,"
 * Protocols before HIVE_CLI_SERVICE_PROTOCOL_V6 (used by RowBasedSet)
 *
 ",,,,,,,2,71
CLIService.java,804,14414,0.055779103649229916,"
 * CLIService.
 *
 ","/**
 * @deprecated  Use {@link #openSession(TProtocolVersion, String, String, String, Map)}
 */
 
/**
 * @deprecated  Use {@link #openSessionWithImpersonation(TProtocolVersion, String, String, String, Map, String)}
 */
 
","{
    SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, false, null);
    LOG.debug(sessionHandle + "": openSession()"");
    return sessionHandle;
} 
{
    SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, true, delegationToken);
    LOG.debug(sessionHandle + "": openSessionWithImpersonation()"");
    return sessionHandle;
} 
",,,,,2,15
ThriftCLIServiceClient.java,78,382,0.20418848167539266,"
 * ThriftCLIServiceClient.
 *
 ",,,,,,,2,27
ThriftBinaryCLIService.java,21,77,0.2727272727272727,,,,,,,,1,0
ThriftHttpServlet.java,126,393,0.32061068702290074,"
 *
 * ThriftHttpServlet
 *
 ","/**
 * Retrieves the client name from cookieString. If the cookie does not
 * correspond to a valid client, the function returns null.
 * @param cookies HTTP Request cookies.
 * @return Client Username if cookieString has a HS2 Generated cookie that is currently valid.
 * Else, returns null.
 */
 
/**
 * Convert cookie array to human readable cookie string
 * @param cookies Cookie Array
 * @return String containing all the cookies separated by a newline character.
 * Each cookie is of the format [key]=[value]
 */
 
/**
 * Validate the request cookie. This function iterates over the request cookie headers
 * and finds a cookie that represents a valid client/server session. If it finds one, it
 * returns the client name associated with the session. Else, it returns null.
 * @param request The HTTP Servlet Request send by the client
 * @return Client Username if the request has valid HS2 cookie, else returns null
 * @throws UnsupportedEncodingException
 */
 
/**
 * Generate a server side cookie given the cookie value as the input.
 * @param str Input string token.
 * @return The generated cookie.
 * @throws UnsupportedEncodingException
 */
 
/**
 * Generate httponly cookie from HS2 cookie
 * @param cookie HS2 generated cookie
 * @return The httponly cookie
 */
 
/**
 * Do the LDAP/PAM authentication
 * @param request
 * @param authType
 * @throws HttpAuthenticationException
 */
 
/**
 * Do the GSS-API kerberos authentication.
 * We already have a logged in subject in the form of serviceUGI,
 * which GSS-API will extract information from.
 * In case of a SPNego request we use the httpUGI,
 * for the authenticating service tickets.
 * @param request
 * @return
 * @throws HttpAuthenticationException
 */
 
/**
 * Returns the base64 encoded auth header payload
 * @param request
 * @param authType
 * @return
 * @throws HttpAuthenticationException
 */
 
","{
    // Current Cookie Name, Current Cookie Value
    String currName, currValue;
    // Following is the main loop which iterates through all the cookies send by the client.
    // The HS2 generated cookies are of the format hive.server2.auth=<value>
    // A cookie which is identified as a hiveserver2 generated cookie is validated
    // by calling signer.verifyAndExtract(). If the validation passes, send the
    // username for which the cookie is validated to the caller. If no client side
    // cookie passes the validation, return null to the caller.
    for (Cookie currCookie : cookies) {
        // Get the cookie name
        currName = currCookie.getName();
        if (!currName.equals(AUTH_COOKIE)) {
            // Not a HS2 generated cookie, continue.
            continue;
        }
        // If we reached here, we have match for HS2 generated cookie
        currValue = currCookie.getValue();
        // Validate the value.
        currValue = signer.verifyAndExtract(currValue);
        // Retrieve the user name, do the final validation step.
        if (currValue != null) {
            String userName = HttpAuthUtils.getUserNameFromCookieToken(currValue);
            if (userName == null) {
                LOG.warn(""Invalid cookie token "" + currValue);
                continue;
            }
            // We have found a valid cookie in the client request.
            if (LOG.isDebugEnabled()) {
                LOG.debug(""Validated the cookie for user "" + userName);
            }
            return userName;
        }
    }
    // No valid HS2 generated cookies found, return null
    return null;
} 
{
    String cookieStr = """";
    for (Cookie c : cookies) {
        cookieStr += c.getName() + ""="" + c.getValue() + "" ;\n"";
    }
    return cookieStr;
} 
{
    // Find all the valid cookies associated with the request.
    Cookie[] cookies = request.getCookies();
    if (cookies == null) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(""No valid cookies associated with the request "" + request);
        }
        return null;
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Received cookies: "" + toCookieStr(cookies));
    }
    return getClientNameFromCookie(cookies);
} 
{
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Cookie name = "" + AUTH_COOKIE + "" value = "" + str);
    }
    Cookie cookie = new Cookie(AUTH_COOKIE, str);
    cookie.setMaxAge(cookieMaxAge);
    if (cookieDomain != null) {
        cookie.setDomain(cookieDomain);
    }
    if (cookiePath != null) {
        cookie.setPath(cookiePath);
    }
    cookie.setSecure(isCookieSecure);
    return cookie;
} 
{
    NewCookie newCookie = new NewCookie(cookie.getName(), cookie.getValue(), cookie.getPath(), cookie.getDomain(), cookie.getVersion(), cookie.getComment(), cookie.getMaxAge(), cookie.getSecure());
    return newCookie + ""; HttpOnly"";
} 
{
    String userName = getUsername(request, authType);
    // No-op when authType is NOSASL
    if (!authType.equalsIgnoreCase(HiveAuthFactory.AuthTypes.NOSASL.toString())) {
        try {
            AuthMethods authMethod = AuthMethods.getValidAuthMethod(authType);
            PasswdAuthenticationProvider provider = AuthenticationProviderFactory.getAuthenticationProvider(authMethod);
            provider.Authenticate(userName, getPassword(request, authType));
        } catch (Exception e) {
            throw new HttpAuthenticationException(e);
        }
    }
    return userName;
} 
{
    // Try authenticating with the http/_HOST principal
    if (httpUGI != null) {
        try {
            return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));
        } catch (Exception e) {
            LOG.info(""Failed to authenticate with http/_HOST kerberos principal, "" + ""trying with hive/_HOST kerberos principal"");
        }
    }
    // Now try with hive/_HOST principal
    try {
        return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));
    } catch (Exception e) {
        LOG.error(""Failed to authenticate with hive/_HOST kerberos principal"");
        throw new HttpAuthenticationException(e);
    }
} 
{
    String authHeader = request.getHeader(HttpAuthUtils.AUTHORIZATION);
    // Each http request must have an Authorization header
    if (authHeader == null || authHeader.isEmpty()) {
        throw new HttpAuthenticationException(""Authorization header received "" + ""from the client is empty."");
    }
    String authHeaderBase64String;
    int beginIndex;
    if (isKerberosAuthMode(authType)) {
        beginIndex = (HttpAuthUtils.NEGOTIATE + "" "").length();
    } else {
        beginIndex = (HttpAuthUtils.BASIC + "" "").length();
    }
    authHeaderBase64String = authHeader.substring(beginIndex);
    // Authorization header must have a payload
    if (authHeaderBase64String == null || authHeaderBase64String.isEmpty()) {
        throw new HttpAuthenticationException(""Authorization header received "" + ""from the client does not contain any data."");
    }
    return authHeaderBase64String;
} 
",,,,,3,22
ThriftCLIService.java,66,598,0.11036789297658862,"
 * ThriftCLIService.
 *
 ","/**
 * Returns the effective username.
 * 1. If hive.server2.allow.user.substitution = false: the username of the connecting user
 * 2. If hive.server2.allow.user.substitution = true: the username of the end user,
 * that the connecting user is trying to proxy for.
 * This includes a check whether the connecting user is allowed to proxy for the end user.
 * @param req
 * @return
 * @throws HiveSQLException
 */
 
/**
 * Create a session handle
 * @param req
 * @param res
 * @return
 * @throws HiveSQLException
 * @throws LoginException
 * @throws IOException
 */
 
/**
 * If the proxy user name is provided then check privileges to substitute the user.
 * @param realUser
 * @param sessionConf
 * @param ipAddress
 * @return
 * @throws HiveSQLException
 */
 
","{
    String userName = null;
    // Kerberos
    if (isKerberosAuthMode()) {
        userName = hiveAuthFactory.getRemoteUser();
    }
    // Except kerberos, NOSASL
    if (userName == null) {
        userName = TSetIpAddressProcessor.getUserName();
    }
    // Http transport mode.
    // We set the thread local username, in ThriftHttpServlet.
    if (cliService.getHiveConf().getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase(""http"")) {
        userName = SessionManager.getUserName();
    }
    if (userName == null) {
        userName = req.getUsername();
    }
    userName = getShortName(userName);
    String effectiveClientUser = getProxyUser(userName, req.getConfiguration(), getIpAddress());
    LOG.debug(""Client's username: "" + effectiveClientUser);
    return effectiveClientUser;
} 
{
    String userName = getUserName(req);
    String ipAddress = getIpAddress();
    TProtocolVersion protocol = getMinVersion(CLIService.SERVER_VERSION, req.getClient_protocol());
    res.setServerProtocolVersion(protocol);
    SessionHandle sessionHandle;
    if (cliService.getHiveConf().getBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS) && (userName != null)) {
        String delegationTokenStr = getDelegationToken(userName);
        sessionHandle = cliService.openSessionWithImpersonation(protocol, userName, req.getPassword(), ipAddress, req.getConfiguration(), delegationTokenStr);
    } else {
        sessionHandle = cliService.openSession(protocol, userName, req.getPassword(), ipAddress, req.getConfiguration());
    }
    return sessionHandle;
} 
{
    String proxyUser = null;
    // Http transport mode.
    // We set the thread local proxy username, in ThriftHttpServlet.
    if (cliService.getHiveConf().getVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE).equalsIgnoreCase(""http"")) {
        proxyUser = SessionManager.getProxyUserName();
        LOG.debug(""Proxy user from query string: "" + proxyUser);
    }
    if (proxyUser == null && sessionConf != null && sessionConf.containsKey(HiveAuthFactory.HS2_PROXY_USER)) {
        String proxyUserFromThriftBody = sessionConf.get(HiveAuthFactory.HS2_PROXY_USER);
        LOG.debug(""Proxy user from thrift body: "" + proxyUserFromThriftBody);
        proxyUser = proxyUserFromThriftBody;
    }
    if (proxyUser == null) {
        return realUser;
    }
    // check whether substitution is allowed
    if (!hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ALLOW_USER_SUBSTITUTION)) {
        throw new HiveSQLException(""Proxy user substitution is not allowed"");
    }
    // If there's no authentication, then directly substitute the user
    if (HiveAuthFactory.AuthTypes.NONE.toString().equalsIgnoreCase(hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION))) {
        return proxyUser;
    }
    // Verify proxy user privilege of the realUser for the proxyUser
    HiveAuthFactory.verifyProxyAccess(realUser, proxyUser, ipAddress, hiveConf);
    LOG.debug(""Verified proxy user: "" + proxyUser);
    return proxyUser;
} 
",,,,,2,21
ThriftHttpCLIService.java,42,125,0.336,,"/**
 * Configure Jetty to serve http requests. Example of a client connection URL:
 * http://localhost:10000/servlets/thrifths2/ A gateway may cause actual target URL to differ,
 * e.g. http://gateway:port/hive2/servlets/thrifths2/
 */
 
/**
 * The config parameter can be like ""path"", ""/path"", ""/path/"", ""path/*"", ""/path1/path2/*"" and so on.
 * httpPath should end up as ""/*"", ""/path/*"" or ""/path1/../pathN/*""
 * @param httpPath
 * @return
 */
 
","{
    try {
        // Server thread pool
        // Start with minWorkerThreads, expand till maxWorkerThreads and reject subsequent requests
        String threadPoolName = ""HiveServer2-HttpHandler-Pool"";
        ThreadPoolExecutor executorService = new ThreadPoolExecutor(minWorkerThreads, maxWorkerThreads, workerKeepAliveTime, TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), new ThreadFactoryWithGarbageCleanup(threadPoolName));
        ExecutorThreadPool threadPool = new ExecutorThreadPool(executorService);
        // HTTP Server
        httpServer = new org.eclipse.jetty.server.Server(threadPool);
        // Connector configs
        ConnectionFactory[] connectionFactories;
        boolean useSsl = hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_USE_SSL);
        String schemeName = useSsl ? ""https"" : ""http"";
        // Change connector if SSL is used
        if (useSsl) {
            String keyStorePath = hiveConf.getVar(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH).trim();
            String keyStorePassword = ShimLoader.getHadoopShims().getPassword(hiveConf, HiveConf.ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PASSWORD.varname);
            if (keyStorePath.isEmpty()) {
                throw new IllegalArgumentException(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PATH.varname + "" Not configured for SSL connection"");
            }
            SslContextFactory sslContextFactory = new SslContextFactory.Server();
            String[] excludedProtocols = hiveConf.getVar(ConfVars.HIVE_SSL_PROTOCOL_BLACKLIST).split("","");
            LOG.info(""HTTP Server SSL: adding excluded protocols: "" + Arrays.toString(excludedProtocols));
            sslContextFactory.addExcludeProtocols(excludedProtocols);
            LOG.info(""HTTP Server SSL: SslContextFactory.getExcludeProtocols = "" + Arrays.toString(sslContextFactory.getExcludeProtocols()));
            sslContextFactory.setKeyStorePath(keyStorePath);
            sslContextFactory.setKeyStorePassword(keyStorePassword);
            connectionFactories = AbstractConnectionFactory.getFactories(sslContextFactory, new HttpConnectionFactory());
        } else {
            connectionFactories = new ConnectionFactory[] { new HttpConnectionFactory() };
        }
        ServerConnector connector = new ServerConnector(httpServer, null, // Call this full constructor to set this, which forces daemon threads:
        new ScheduledExecutorScheduler(""HiveServer2-HttpHandler-JettyScheduler"", true), null, -1, -1, connectionFactories);
        connector.setPort(portNum);
        // Linux:yes, Windows:no
        connector.setReuseAddress(!Shell.WINDOWS);
        int maxIdleTime = (int) hiveConf.getTimeVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_MAX_IDLE_TIME, TimeUnit.MILLISECONDS);
        connector.setIdleTimeout(maxIdleTime);
        httpServer.addConnector(connector);
        // Thrift configs
        hiveAuthFactory = new HiveAuthFactory(hiveConf);
        TProcessor processor = new TCLIService.Processor<Iface>(this);
        TProtocolFactory protocolFactory = new TBinaryProtocol.Factory();
        // Set during the init phase of HiveServer2 if auth mode is kerberos
        // UGI for the hive/_HOST (kerberos) principal
        UserGroupInformation serviceUGI = cliService.getServiceUGI();
        // UGI for the http/_HOST (SPNego) principal
        UserGroupInformation httpUGI = cliService.getHttpUGI();
        String authType = hiveConf.getVar(ConfVars.HIVE_SERVER2_AUTHENTICATION);
        TServlet thriftHttpServlet = new ThriftHttpServlet(processor, protocolFactory, authType, serviceUGI, httpUGI);
        // Context handler
        final ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);
        context.setContextPath(""/"");
        String httpPath = getHttpPath(hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_PATH));
        httpServer.setHandler(context);
        context.addServlet(new ServletHolder(thriftHttpServlet), httpPath);
        // TODO: check defaults: maxTimeout, keepalive, maxBodySize, bodyRecieveDuration, etc.
        // Finally, start the server
        httpServer.start();
        String msg = ""Started "" + ThriftHttpCLIService.class.getSimpleName() + "" in "" + schemeName + "" mode on port "" + connector.getLocalPort() + "" path="" + httpPath + "" with "" + minWorkerThreads + ""..."" + maxWorkerThreads + "" worker threads"";
        LOG.info(msg);
        httpServer.join();
    } catch (Throwable t) {
        LOG.fatal(""Error starting HiveServer2: could not start "" + ThriftHttpCLIService.class.getSimpleName(), t);
        System.exit(-1);
    }
} 
{
    if (httpPath == null || httpPath.equals("""")) {
        httpPath = ""/*"";
    } else {
        if (!httpPath.startsWith(""/"")) {
            httpPath = ""/"" + httpPath;
        }
        if (httpPath.endsWith(""/"")) {
            httpPath = httpPath + ""*"";
        }
        if (!httpPath.endsWith(""/*"")) {
            httpPath = httpPath + ""/*"";
        }
    }
    return httpPath;
} 
",,,,,1,0
HandleIdentifier.java,26,404,0.06435643564356436,"
 * HandleIdentifier.
 *
 ",,,,,,,2,21
Column.java,17,642,0.0264797507788162,"
 * Column.
 ",,,,,,,1,10
GetInfoValue.java,17,513,0.03313840155945419,"
 * GetInfoValue.
 *
 ",,,,,,,2,17
TableSchema.java,25,351,0.07122507122507123,"
 * TableSchema.
 *
 ",,,,,,,2,16
GetInfoType.java,13,160,0.08125,"
 * GetInfoType.
 *
 ",,,,,,,2,16
TypeDescriptor.java,39,104,0.375,"
 * TypeDescriptor.
 *
 ","/**
 * The column size for this type.
 * For numeric data this is the maximum precision.
 * For character data this is the length in characters.
 * For datetime types this is the length in characters of the String representation
 * (assuming the maximum allowed precision of the fractional seconds component).
 * For binary data this is the length in bytes.
 * Null is returned for data types where the column size is not applicable.
 */
 
/**
 * Maximum precision for numeric types.
 * Returns null for non-numeric types.
 * @return
 */
 
/**
 * The number of fractional digits for this type.
 * Null is returned for data types where this is not applicable.
 */
 
","{
    if (type.isNumericType()) {
        return getPrecision();
    }
    switch(type) {
        case STRING_TYPE:
        case BINARY_TYPE:
            return Integer.MAX_VALUE;
        case CHAR_TYPE:
        case VARCHAR_TYPE:
            return typeQualifiers.getCharacterMaximumLength();
        case DATE_TYPE:
            return 10;
        case TIMESTAMP_TYPE:
            return 29;
        default:
            return null;
    }
} 
{
    if (this.type == Type.DECIMAL_TYPE) {
        return typeQualifiers.getPrecision();
    }
    return this.type.getMaxPrecision();
} 
{
    switch(this.type) {
        case BOOLEAN_TYPE:
        case TINYINT_TYPE:
        case SMALLINT_TYPE:
        case INT_TYPE:
        case BIGINT_TYPE:
            return 0;
        case FLOAT_TYPE:
            return 7;
        case DOUBLE_TYPE:
            return 15;
        case DECIMAL_TYPE:
            return typeQualifiers.getScale();
        case TIMESTAMP_TYPE:
            return 9;
        default:
            return null;
    }
} 
",,,,,2,19
RowBasedSet.java,20,97,0.20618556701030927,"
 * RowBasedSet
 ",,,,,,,1,14
OperationState.java,23,80,0.2875,"
 * OperationState.
 *
 ",,,,,,,2,19
PatternOrIdentifier.java,21,19,1.105263157894737,"
 * PatternOrIdentifier.
 *
 ",,,,,,,2,24
ColumnDescriptor.java,24,59,0.4067796610169492,"
 * ColumnDescriptor.
 *
 ",,,,,,,2,21
RowSet.java,30,781,0.03841229193341869,,,,,,,,1,0
OperationHandle.java,41,575,0.07130434782608695,,,,,,,,1,0
TypeQualifiers.java,25,359,0.06963788300835655,"
 * This class holds type qualifier information for a primitive type,
 * such as char/varchar length or decimal precision/scale.
 ",,,,,,,2,125
Handle.java,41,575,0.07130434782608695,,,,,,,,1,0
EmbeddedCLIServiceClient.java,71,108,0.6574074074074074,"
 * EmbeddedCLIServiceClient.
 *
 ",,,,,,,2,29
HiveSQLException.java,82,143,0.5734265734265734,"
 * HiveSQLException.
 *
 ","/**
 * Converts current object to a {@link TStatus} object
 * @return a {@link TStatus} object
 */
 
/**
 * Converts the specified {@link Exception} object into a {@link TStatus} object
 * @param e a {@link Exception} object
 * @return a {@link TStatus} object
 */
 
/**
 * Converts a {@link Throwable} object into a flattened list of texts including its stack trace
 * and the stack traces of the nested causes.
 * @param ex  a {@link Throwable} object
 * @return    a flattened list of texts including the {@link Throwable} object's stack trace
 *            and the stack traces of the nested causes.
 */
 
/**
 * Converts a flattened list of texts including the stack trace and the stack
 * traces of the nested causes into a {@link Throwable} object.
 * @param details a flattened list of texts including the stack trace and the stack
 *                traces of the nested causes
 * @return        a {@link Throwable} object
 */
 
","{
    // TODO: convert sqlState, etc.
    TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
    tStatus.setSqlState(getSQLState());
    tStatus.setErrorCode(getErrorCode());
    tStatus.setErrorMessage(getMessage());
    tStatus.setInfoMessages(toString(this));
    return tStatus;
} 
{
    if (e instanceof HiveSQLException) {
        return ((HiveSQLException) e).toTStatus();
    }
    TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
    tStatus.setErrorMessage(e.getMessage());
    tStatus.setInfoMessages(toString(e));
    return tStatus;
} 
{
    return toString(ex, null);
} 
{
    return toStackTrace(details, null, 0);
} 
","/**
 */
 
/**
 * @param reason
 */
 
/**
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 */
 
/**
 * @param reason
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 * @param vendorCode
 */
 
/**
 * @param reason
 * @param sqlState
 * @param cause
 */
 
/**
 * @param reason
 * @param sqlState
 * @param vendorCode
 * @param cause
 */
 
","{
    super();
} 
{
    super(reason);
} 
{
    super(cause);
} 
{
    super(reason, sqlState);
} 
{
    super(reason, cause);
} 
{
    super(reason, sqlState, vendorCode);
} 
{
    super(reason, sqlState, cause);
} 
{
    super(reason, sqlState, vendorCode, cause);
} 
","/**
 */
 
","Field serialVersionUID
",2,21
RowSetFactory.java,17,19,0.8947368421052632,,,,,,,,1,0
ColumnBasedSet.java,22,150,0.14666666666666667,"
 * ColumnBasedSet.
 ",,,,,,,1,18
HiveSessionImpl.java,57,759,0.07509881422924901,"
 * HiveSession
 *
 | Copy from org.apache.hadoop.hive.ql.processors.SetProcessor, only change:|
   * It is used for processing hiverc file from HiveServer2 side.
   ","/**
 * 1. We'll remove the ThreadLocal SessionState as this thread might now serve
 * other requests.
 * 2. We'll cache the ThreadLocal RawStore object for this background thread for an orderly cleanup
 * when this thread is garbage collected later.
 * @see org.apache.hive.service.server.ThreadWithGarbageCleanup#finalize()
 */
 
","{
    SessionState.detachSession();
    if (ThreadWithGarbageCleanup.currentThread() instanceof ThreadWithGarbageCleanup) {
        ThreadWithGarbageCleanup currentThread = (ThreadWithGarbageCleanup) ThreadWithGarbageCleanup.currentThread();
        currentThread.cacheThreadLocalRawStore();
    }
    if (userAccess) {
        lastAccessTime = System.currentTimeMillis();
    }
    if (opHandleSet.isEmpty()) {
        lastIdleTime = System.currentTimeMillis();
    } else {
        lastIdleTime = 0;
    }
} 
",,,,,4,158
HiveSessionImplwithUGI.java,38,123,0.3089430894308943,"
 *
 * HiveSessionImplwithUGI.
 * HiveSession with connecting user's UGI and delegation token if required
 ","/**
 * Close the file systems for the session and remove it from the FileSystem cache.
 * Cancel the session's delegation token and close the metastore connection
 */
 
/**
 * Enable delegation token for the session
 * save the token string and set the token.signature in hive conf. The metastore client uses
 * this token.signature to determine where to use kerberos or delegation token
 * @throws HiveException
 * @throws IOException
 */
 
","{
    try {
        acquire(true);
        cancelDelegationToken();
    } finally {
        try {
            super.close();
        } finally {
            try {
                FileSystem.closeAllForUGI(sessionUgi);
            } catch (IOException ioe) {
                throw new HiveSQLException(""Could not clean up file-system handles for UGI: "" + sessionUgi, ioe);
            }
        }
    }
} 
{
    this.delegationTokenStr = delegationTokenStr;
    if (delegationTokenStr != null) {
        getHiveConf().set(""hive.metastore.token.signature"", HS2TOKEN);
        try {
            Utils.setTokenStr(sessionUgi, delegationTokenStr, HS2TOKEN);
        } catch (IOException e) {
            throw new HiveSQLException(""Couldn't setup delegation token in the ugi"", e);
        }
    }
} 
",,,,,3,100
HiveSessionBase.java,43,26,1.6538461538461537,"
 * Methods that don't need to be executed under a doAs
 * context are here. Rest of them in HiveSession interface
 ","/**
 * Set the session manager for the session
 * @param sessionManager
 */
 
/**
 * Get the session manager for the session
 */
 
/**
 * Set operation manager for the session
 * @param operationManager
 */
 
/**
 * Check whether operation logging is enabled and session dir is created successfully
 */
 
/**
 * Get the session dir, which is the parent dir of operation logs
 * @return a file representing the parent directory of operation logs
 */
 
/**
 * Set the session dir, which is the parent dir of operation logs
 * @param operationLogRootDir the parent dir of the session dir
 */
 
","setSessionManager 
getSessionManager 
setOperationManager 
isOperationLogEnabled 
getOperationLogSessionDir 
setOperationLogSessionDir 
",,,,,2,111
SessionManager.java,48,272,0.17647058823529413,"
 * SessionManager.
 *
 ","/**
 * Opens a new session and creates a session handle.
 * The username passed to this method is the effective username.
 * If withImpersonation is true (==doAs true) we wrap all the calls in HiveSession
 * within a UGI.doAs, where UGI corresponds to the effective user.
 *
 * Please see {@code org.apache.hive.service.cli.thrift.ThriftCLIService.getUserName()} for
 * more details.
 *
 * @param protocol
 * @param username
 * @param password
 * @param ipAddress
 * @param sessionConf
 * @param withImpersonation
 * @param delegationToken
 * @return
 * @throws HiveSQLException
 */
 
","{
    HiveSession session;
    // If doAs is set to true for HiveServer2, we will create a proxy object for the session impl.
    // Within the proxy object, we wrap the method call in a UserGroupInformation#doAs
    if (withImpersonation) {
        HiveSessionImplwithUGI sessionWithUGI = new HiveSessionImplwithUGI(protocol, username, password, hiveConf, ipAddress, delegationToken);
        session = HiveSessionProxy.getProxy(sessionWithUGI, sessionWithUGI.getSessionUgi());
        sessionWithUGI.setProxySession(session);
    } else {
        session = new HiveSessionImpl(protocol, username, password, hiveConf, ipAddress);
    }
    session.setSessionManager(this);
    session.setOperationManager(operationManager);
    try {
        session.open(sessionConf);
    } catch (Exception e) {
        try {
            session.close();
        } catch (Throwable t) {
            LOG.warn(""Error closing session"", t);
        }
        session = null;
        throw new HiveSQLException(""Failed to open new session: "" + e, e);
    }
    if (isOperationLogEnabled) {
        session.setOperationLogSessionDir(operationLogRootDir);
    }
    handleToSession.put(session.getSessionHandle(), session);
    return session.getSessionHandle();
} 
",,,,,2,19
HiveSession.java,124,49,2.5306122448979593,,"/**
 * getInfo operation handler
 * @param getInfoType
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @return
 * @throws HiveSQLException
 */
 
/**
 * execute operation handler
 * @param statement
 * @param confOverlay
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTypeInfo operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getCatalogs operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getSchemas operation handler
 * @param catalogName
 * @param schemaName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTables operation handler
 * @param catalogName
 * @param schemaName
 * @param tableName
 * @param tableTypes
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getTableTypes operation handler
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getColumns operation handler
 * @param catalogName
 * @param schemaName
 * @param tableName
 * @param columnName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * getFunctions operation handler
 * @param catalogName
 * @param schemaName
 * @param functionName
 * @return
 * @throws HiveSQLException
 */
 
/**
 * close the session
 * @throws HiveSQLException
 */
 
","getInfo 
executeStatement 
executeStatementAsync 
getTypeInfo 
getCatalogs 
getSchemas 
getTables 
getTableTypes 
getColumns 
getFunctions 
close 
",,,,,1,0
ServiceOperations.java,77,51,1.5098039215686274,"
 * ServiceOperations.
 *
 ","/**
 * Verify that a service is in a given state.
 * @param state the actual state a service is in
 * @param expectedState the desired state
 * @throws IllegalStateException if the service state is different from
 * the desired state
 */
 
/**
 * Initialize then start a service.
 *
 * The service state is checked <i>before</i> the operation begins.
 * This process is <i>not</i> thread safe.
 * @param service a service that must be in the state
 *   {@link Service.STATE#NOTINITED}
 * @param configuration the configuration to initialize the service with
 * @throws RuntimeException on a state change failure
 * @throws IllegalStateException if the service is in the wrong state
 */
 
/**
 * Stop a service.
 *
 * Do nothing if the service is null or not in a state in which it can be/needs to be stopped.
 *
 * The service state is checked <i>before</i> the operation begins.
 * This process is <i>not</i> thread safe.
 * @param service a service or null
 */
 
/**
 * Stop a service; if it is null do nothing. Exceptions are caught and
 * logged at warn level. (but not Throwables). This operation is intended to
 * be used in cleanup operations
 *
 * @param service a service; may be null
 * @return any exception that was caught; null if none was.
 */
 
","{
    if (state != expectedState) {
        throw new IllegalStateException(""For this operation, the "" + ""current service state must be "" + expectedState + "" instead of "" + state);
    }
} 
{
    init(service, configuration);
    start(service);
} 
{
    if (service != null) {
        Service.STATE state = service.getServiceState();
        if (state == Service.STATE.STARTED) {
            service.stop();
        }
    }
} 
{
    try {
        stop(service);
    } catch (Exception e) {
        LOG.warn(""When stopping the service "" + service.getName() + "" : "" + e, e);
        return e;
    }
    return null;
} 
",,,,,2,22
CompositeService.java,32,80,0.4,"
 * CompositeService.
 *
 |
   * JVM Shutdown hook for CompositeService which will stop the given
   * CompositeService gracefully in case of JVM shutdown.
   ",,,,,,,5,149
AbstractService.java,86,76,1.131578947368421,"
 * AbstractService.
 *
 ","/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * {@inheritDoc}
 *
 * @throws IllegalStateException
 *           if the current service state does not permit
 *           this action
 */
 
/**
 * Verify that a service is in a given state.
 *
 * @param currentState
 *          the desired state
 * @throws IllegalStateException
 *           if the service state is different from
 *           the desired state
 */
 
/**
 * Change to a new state and notify all listeners.
 * This is a private method that is only invoked from synchronized methods,
 * which avoid having to clone the listener list. It does imply that
 * the state change listener methods should be short lived, as they
 * will delay the state transition.
 *
 * @param newState
 *          new service state
 */
 
","{
    ensureCurrentState(STATE.NOTINITED);
    this.hiveConf = hiveConf;
    changeState(STATE.INITED);
    LOG.info(""Service:"" + getName() + "" is inited."");
} 
{
    startTime = System.currentTimeMillis();
    ensureCurrentState(STATE.INITED);
    changeState(STATE.STARTED);
    LOG.info(""Service:"" + getName() + "" is started."");
} 
{
    if (state == STATE.STOPPED || state == STATE.INITED || state == STATE.NOTINITED) {
        // already stopped, or else it was never
        // started (eg another service failing canceled startup)
        return;
    }
    ensureCurrentState(STATE.STARTED);
    changeState(STATE.STOPPED);
    LOG.info(""Service:"" + getName() + "" is stopped."");
} 
{
    ServiceOperations.ensureCurrentState(state, currentState);
} 
{
    state = newState;
    // notify listeners
    for (ServiceStateChangeListener l : listeners) {
        l.stateChanged(this);
    }
} 
","/**
 * Construct the service.
 *
 * @param name
 *          service name
 */
 
","{
    this.name = name;
} 
","/**
 * Service state: initially {@link STATE#NOTINITED}.
 */
 
/**
 * Service name.
 */
 
/**
 * Service start time. Will be zero until the service is started.
 */
 
/**
 * The configuration. Will be null until the service is initialized.
 */
 
/**
 * List of state change listeners; it is final to ensure
 * that it will never be null.
 */
 
","Field state
Field name
Field startTime
Field hiveConf
Field listeners
",2,20
CookieSigner.java,41,57,0.7192982456140351,"
 * The cookie signer generates a signature based on SHA digest
 * and appends it to the cookie value generated at the
 * server side. It uses SHA digest algorithm to sign and verify signatures.
 ","/**
 * Sign the cookie given the string token as input.
 * @param str Input token
 * @return Signed token that can be used to create a cookie
 */
 
/**
 * Verify a signed string and extracts the original string.
 * @param signedStr The already signed string
 * @return Raw Value of the string without the signature
 */
 
/**
 * Get the signature of the input string based on SHA digest algorithm.
 * @param str Input token
 * @return Signed String
 */
 
","{
    if (str == null || str.isEmpty()) {
        throw new IllegalArgumentException(""NULL or empty string to sign"");
    }
    String signature = getSignature(str);
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Signature generated for "" + str + "" is "" + signature);
    }
    return str + SIGNATURE + signature;
} 
{
    int index = signedStr.lastIndexOf(SIGNATURE);
    if (index == -1) {
        throw new IllegalArgumentException(""Invalid input sign: "" + signedStr);
    }
    String originalSignature = signedStr.substring(index + SIGNATURE.length());
    String rawValue = signedStr.substring(0, index);
    String currentSignature = getSignature(rawValue);
    if (LOG.isDebugEnabled()) {
        LOG.debug(""Signature generated for "" + rawValue + "" inside verify is "" + currentSignature);
    }
    if (!originalSignature.equals(currentSignature)) {
        throw new IllegalArgumentException(""Invalid sign, original = "" + originalSignature + "" current = "" + currentSignature);
    }
    return rawValue;
} 
{
    try {
        MessageDigest md = MessageDigest.getInstance(SHA_STRING);
        md.update(str.getBytes());
        md.update(secretBytes);
        byte[] digest = md.digest();
        return new Base64(0).encodeToString(digest);
    } catch (NoSuchAlgorithmException ex) {
        throw new RuntimeException(""Invalid SHA digest String: "" + SHA_STRING + "" "" + ex.getMessage(), ex);
    }
} 
","/**
 * Constructor
 * @param secret Secret Bytes
 */
 
","{
    if (secret == null) {
        throw new IllegalArgumentException("" NULL Secret Bytes"");
    }
    this.secretBytes = secret.clone();
} 
",,,3,189
ServiceException.java,21,12,1.75,"
 * ServiceException.
 *
 ",,,,,,,2,21
LdapAuthenticationProviderImpl.java,22,50,0.44,,,,,,,,1,0
AuthenticationProviderFactory.java,20,42,0.47619047619047616,"
 * This class helps select a {@link PasswdAuthenticationProvider} for a given {@code AuthMethod}.
 ",,,,,,,1,97
PlainSaslServer.java,30,125,0.24,"
 * Sun JDK only provides a PLAIN client and no server. This class implements the Plain SASL server
 * conforming to RFC #4616 (http://www.ietf.org/rfc/rfc4616.txt).
 ",,,,,,,2,162
PamAuthenticationProviderImpl.java,17,26,0.6538461538461539,,,,,,,,1,0
CustomAuthenticationProviderImpl.java,23,20,1.15,"
 * This authentication provider implements the {@code CUSTOM} authentication. It allows a {@link
 * PasswdAuthenticationProvider} to be specified at configuration time which may additionally
 * implement {@link org.apache.hadoop.conf.Configurable Configurable} to grab Hive's {@link
 * org.apache.hadoop.conf.Configuration Configuration}.
 ",,,,,,,4,332
SaslQOP.java,23,33,0.696969696969697,"
 * Possible values of SASL quality-of-protection value.
 ",,,,,,,1,55
AnonymousAuthenticationProviderImpl.java,21,7,3.0,"
 * This authentication provider allows any combination of username and password.
 ",,,,,,,1,80
PasswdAuthenticationProvider.java,31,5,6.2,,"/**
 * The Authenticate method is called by the HiveServer2 authentication layer
 * to authenticate users for their requests.
 * If a user is to be granted, return nothing/throw nothing.
 * When a user is to be disallowed, throw an appropriate {@link AuthenticationException}.
 *
 * For an example implementation, see {@link LdapAuthenticationProviderImpl}.
 *
 * @param user     The username received over the connection request
 * @param password The password received over the connection request
 *
 * @throws AuthenticationException When a user is found to be
 *                                 invalid by the implementation
 */
 
","Authenticate 
",,,,,1,0
TSubjectAssumingTransport.java,24,39,0.6153846153846154,"
 * This is used on the client side, where the API explicitly opens a transport to
 * the server using the Subject.doAs().
 ",,,,,,,2,119
HttpAuthenticationException.java,23,13,1.7692307692307692,,,,"/**
 * @param cause original exception
 */
 
/**
 * @param msg exception message
 */
 
/**
 * @param msg   exception message
 * @param cause original exception
 */
 
","{
    super(cause);
} 
{
    super(msg);
} 
{
    super(msg, cause);
} 
",,,1,0
ThreadFactoryWithGarbageCleanup.java,33,21,1.5714285714285714,"
 * A ThreadFactory for constructing new HiveServer2 threads that lets you plug
 * in custom cleanup code to be called before this thread is GC-ed.
 * Currently cleans up the following:
 * 1. ThreadLocal RawStore object:
 * In case of an embedded metastore, HiveServer2 threads (foreground and background)
 * end up caching a ThreadLocal RawStore object. The ThreadLocal RawStore object has
 * an instance of PersistenceManagerFactory and PersistenceManager.
 * The PersistenceManagerFactory keeps a cache of PersistenceManager objects,
 * which are only removed when PersistenceManager#close method is called.
 * HiveServer2 uses ExecutorService for managing thread pools for foreground and background threads.
 * ExecutorService unfortunately does not provide any hooks to be called,
 * when a thread from the pool is terminated.
 * As a solution, we're using this ThreadFactory to keep a cache of RawStore objects per thread.
 * And we are doing clean shutdown in the finalizer for each thread.
 ",,,,,,,14,970
BreakableService.java,30,74,0.40540540540540543,"
 * This is a service that can be configured to break on any of the lifecycle
 * events, so test the failure handling of other parts of the service
 * infrastructure.
 *
 * It retains a counter to the number of times each entry point is called -
 * these counters are incremented before the exceptions are raised and
 * before the superclass state methods are invoked.
 *
 |
   * The exception explicitly raised on a failure
   ",,,,,,,10,408
CLIServiceClient.java,78,382,0.20418848167539266,"
 * CLIServiceClient.
 *
 ",,,,,,,2,21
OperationStatus.java,21,15,1.4,"
 * OperationStatus
 *
 ",,,,,,,2,19
TableTypeMappingFactory.java,17,16,1.0625,,,,,,,,1,0
LogDivertAppender.java,52,132,0.3939393939393939,"
 * An Appender to divert logs from individual threads to the LogObject they belong to.
 |
   * A log filter that filters messages coming from the logger with the given names.
   * It be used as a white list filter or a black list filter.
   * We apply black list filter on the Loggers used by the log diversion stuff, so that
   * they don't generate more logs for themselves when they process logs.
   * White list filter is used for less verbose log collection
   ","/**
 * Overrides WriterAppender.subAppend(), which does the real logging. No need
 * to worry about concurrency since log4j calls this synchronously.
 */
 
","{
    super.subAppend(event);
    // That should've gone into our writer. Notify the LogContext.
    String logOutput = writer.toString();
    writer.reset();
    OperationLog log = operationManager.getOperationLogByThread();
    if (log == null) {
        LOG.debug("" ---+++=== Dropped log event from thread "" + event.getThreadName());
        return;
    }
    log.writeOperationLog(logOutput);
} 
",,,"/**
 * This is where the log message will go to
 */
 
","Field writer
",7,453
CLIServiceUtils.java,29,39,0.7435897435897436,"
 * CLIServiceUtils.
 *
 ","/**
 * Convert a SQL search pattern into an equivalent Java Regex.
 *
 * @param pattern input which may contain '%' or '_' wildcard characters, or
 * these characters escaped using {@code getSearchStringEscape()}.
 * @return replace %/_ with regex search characters, also handle escaped
 * characters.
 */
 
","{
    if (pattern == null) {
        return "".*"";
    } else {
        StringBuilder result = new StringBuilder(pattern.length());
        boolean escaped = false;
        for (int i = 0, len = pattern.length(); i < len; i++) {
            char c = pattern.charAt(i);
            if (escaped) {
                if (c != SEARCH_STRING_ESCAPE) {
                    escaped = false;
                }
                result.append(c);
            } else {
                if (c == SEARCH_STRING_ESCAPE) {
                    escaped = true;
                    continue;
                } else if (c == '%') {
                    result.append("".*"");
                } else if (c == '_') {
                    result.append('.');
                } else {
                    result.append(Character.toLowerCase(c));
                }
            }
        }
        return result.toString();
    }
} 
",,,,,2,20
FetchType.java,21,20,1.05,"
 * FetchType indicates the type of fetchResults request.
 * It maps the TFetchType, which is generated from Thrift interface.
 ",,,,,,,2,123
HiveSessionProxy.java,21,61,0.3442622950819672,,,,,,,,1,0
Service.java,804,14414,0.055779103649229916,"
 * Service.
 *
 |
   * Service states
   ","/**
 * Initialize the service.
 *
 * The transition must be from {@link STATE#NOTINITED} to {@link STATE#INITED} unless the
 * operation failed and an exception was raised.
 *
 * @param conf
 *          the configuration of the service
 */
 
/**
 * Start the service.
 *
 * The transition should be from {@link STATE#INITED} to {@link STATE#STARTED} unless the
 * operation failed and an exception was raised.
 */
 
/**
 * Stop the service.
 *
 * This operation must be designed to complete regardless of the initial state
 * of the service, including the state of all its internal fields.
 */
 
/**
 * Register an instance of the service state change events.
 *
 * @param listener
 *          a new listener
 */
 
/**
 * Unregister a previously instance of the service state change events.
 *
 * @param listener
 *          the listener to unregister.
 */
 
/**
 * Get the name of this service.
 *
 * @return the service name
 */
 
/**
 * Get the configuration of this service.
 * This is normally not a clone and may be manipulated, though there are no
 * guarantees as to what the consequences of such actions may be
 *
 * @return the current configuration, unless a specific implementation chooses
 *         otherwise.
 */
 
/**
 * Get the current service state
 *
 * @return the state of the service
 */
 
/**
 * Get the service start time
 *
 * @return the start time of the service. This will be zero if the service
 *         has not yet been started.
 */
 
","init 
start 
stop 
register 
unregister 
getName 
getHiveConf 
getServiceState 
getStartTime 
",,,,,4,34
ServiceStateChangeListener.java,38,4,9.5,"
 * ServiceStateChangeListener.
 *
 ","/**
 * Callback to notify of a state change. The service will already
 * have changed state before this callback is invoked.
 *
 * This operation is invoked on the thread that initiated the state change,
 * while the service itself in a synchronized section.
 * <ol>
 *   <li>Any long-lived operation here will prevent the service state
 *   change from completing in a timely manner.</li>
 *   <li>If another thread is somehow invoked from the listener, and
 *   that thread invokes the methods of the service (including
 *   subclass-specific methods), there is a risk of a deadlock.</li>
 * </ol>
 *
 * @param service the service that has changed.
 */
 
","stateChanged 
",,,,,2,31
FilterService.java,21,45,0.4666666666666667,"
 * FilterService.
 *
 ",,,,,,,2,18
